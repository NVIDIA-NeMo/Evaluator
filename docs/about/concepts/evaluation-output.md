(evaluation-output)=

# Evaluation Output

This page describes the structure and content of evaluation output files generated by NVIDIA NeMo Evaluator. The evaluation output provides comprehensive information about the evaluation run, including configuration details, results, and metadata.

## Input Configuration

The input configuration comes from the command described in the [Launcher Quickstart Guide](../../get-started/quickstart/launcher.md#quick-start), namely
```{literalinclude} ../../get-started/_snippets/launcher_full_example.sh
:language: bash
:start-after: "# [snippet-start]"
:end-before: "# [snippet-end]"
```

After running it, you can copy the artifacts folder using

```bash
nemo-evaluator-launcher info <invocation_id> --copy-artifacts <DIR>
```
and find this file under the the `./artifacts` subfolder.


:::{note}
There are several ways to retrieve the artifacts. Above is the way that works across
executors, including e.g. `slurm`, by downloading them.
:::

For the reference purposes, we cite here the launcher config that is used in the command:

```{literalinclude} ../../../packages/nemo-evaluator-launcher/examples/local_llama_3_1_8b_instruct_limit_samples.yaml
:language: yaml
:start-after: "[docs-start-snippet]"
```

## Output Structure

After running an evaluation, NeMo Evaluator creates a structured output directory containing various artifacts. All artifacts are stored in the directory specified by the `output_dir` parameter:

```
<output_dir>/
│   ├── run_config.yml               # Task-specific configuration used during execution
│   ├── eval_factory_metrics.json    # Evaluation metrics and performance statistics
│   ├── results.yml                  # Detailed results in YAML format
│   ├── report.html                  # Human-readable HTML report
│   ├── report.json                  # JSON format report
│   └── <Task specific arifacts>/    # Task-specific artifacts
```

These files are standardized and always follow the same structure regardless of the underlying evaluation harness:

```{list-table}
:header-rows: 1
:widths: 20 30 30 50

* - File Name
  - Description
  - Content
  - Usage

* - `results.yml`
  - Evaluation results in YAML format
  - * Final evaluation scores and metrics  
    * Evaluation configuration used  
  - The main results file for programmatic analysis and integration with downstream tools.

* - `run_config.yml`
  - Complete evaluation configuration (all parameters, overrides, and settings) used for the run.
  - * Task and model settings  
    * Endpoint configuration  
    * Interceptor config  
    * Evaluation-specific overrides
  - Enables full reproducibility of evaluations and configuration auditing.

* - `eval_factory_metrics.json`
  - Detailed metrics and performance statistics for the evaluation execution.
  - * Request/response timings  
    * Token usage  
    * Error rates  
    * Resource utilization
  - Performance analysis and failure pattern identification.

* - `report.html` and `report.json`
  - Example request-response pairs collected during benchmark execution.
  - * Human-readable HTML report
    * Machine-readable JSON version with the same content
  - For sharing, quick review, analysis, and debugging.

* - Task-specific artifacts
  - Artifacts procuded by the underlying benchmark (e.g., caches, raw outputs, error logs).
  - * Cached queries & responses  
    * Source/context data  
    * Special task outputs or logs
  - Advanced troubleshooting, debugging, or domain-specific analysis.

```

### Results file

The primary evaluation output is stored in a `results.yaml`.
It is standardized accross all evaluation benchmarks and follows the [API dataclasses](https://github.com/NVIDIA-NeMo/Evaluator/blob/main/packages/nemo-evaluator/src/nemo_evaluator/api/api_dataclasses.py) specification.


Below we give the output for the
command from the [Launcher Quickstart Section](../../get-started/quickstart/launcher.md#quick-start) for the `GPQA-Diamond` task.

```{literalinclude} ./_snippets/results.yaml
:language: yaml
```
:::{note}
It is instructive to compare the configuration file cited above and the resulting one.
:::


The evaluation output contains the following general sections:

| Section | Description |
|---------|-------------|
| `command` | The exact command executed to run the evaluation |
| `config` | Evaluation configuration including parameters and settings |
| `results` | Evaluation metrics and scores organized by groups and tasks |
| `target` | Model and API endpoint configuration details |
| `git_hash` | Git commit hash (if available) |


The evaluation metrics are available under the `results` key and are stored in a following structure:

```yaml
      metrics:
        metric_name:
          scores:
            score_name:
              stats:            # optional set of statistics, e.g.:
                count: 10       # number of values used for computing the score
                min: 0          # minimum of all values used for computing the score
                max: 1          # maximum of all values used for computing the score
                stderr: 0.13    # standard error
              value: 0.42   # score value
```

In the example output above, the metric used is the micro-average across the samples (thus `micro` key in the structure) and the standard deviation (`stddev`) and standard error (`stderr`) statistics are reported.
The types of metrics available in the results differ for different evaluation harness and task, but they are always presented using the same structure as shown above.


## Exporting the Results

Once the evaluation has finished and the `results.yaml` file was produced, the scores can be exported.
In this example we show how the local export works. For information on other exporters, see {ref}`exporters-overview`.


The results can be exported using the following command:

```bash
nemo-evaluator-launcher export <invocation_id> --dest local --format json
```

This command extracts the scores from the `results.yaml` and creates a `processed_results.json` file with the following content:

```{literalinclude} ./_snippets/processed_results.json
:language: json
```

The `nemo-evaluator-launcher export` can accept multiple invocation IDs and gather results accross different invocations, regardless if they have been run locally or using remote executors (see {ref}`executors-overview`), e.g.:

```bash
nemo-evaluator-launcher export <local-job-id> <slurm-job-id> --dest local --format json --output_dir combined-results
```

will create the `combined-results/processed_results.json` with the same stracuture as in the example above.