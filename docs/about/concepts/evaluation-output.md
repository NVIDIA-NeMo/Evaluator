(evaluation-output)=

# Evaluation Output

This page describes the structure and content of evaluation output files generated by NVIDIA NeMo Evaluator. The evaluation output provides comprehensive information about the evaluation run, including configuration details, results, and metadata.

## Input Configuration

The input configuration comes from the command described in the [Launcher Quickstart Guide](../../get-started/quickstart/launcher.md#quick-start), namely
```{literalinclude} ../../get-started/_snippets/launcher_full_example.sh
:language: bash
:start-after: "# [snippet-start]"
:end-before: "# [snippet-end]"
```

After running it, you can copy the artifacts folder using

```bash
nemo-evaluator-launcher debug <invocation_id> --copy-artifacts <DIR>
```
and find this file under the the `./artifacts` subfolder.


:::{note}
There are several ways to retrieve the artifacts. Above is the way that works across
executors, including e.g. `slurm`, by downloading them.
:::

For the reference purposes, we cite here the launcher config that is used in the command:

```{literalinclude} ../../../packages/nemo-evaluator-launcher/examples/local_llama_3_1_8b_instruct_limit_samples.yaml
:language: yaml
:start-after: "[docs-start-snippet]"
```

## Output Structure

The evaluation output is stored in a `results.yaml`. Below we give the output for the
command from the [Launcher Quickstart Section](../../get-started/quickstart/launcher.md#quick-start) for the `GPQA-Diamond` task.

```{literalinclude} ./_snippets/results.yaml
:language: yaml
```
:::{note}
It is instructive to compare the configuration file cited above and the resulting one.
:::


The evaluation output contains the following general sections:

| Section | Description |
|---------|-------------|
| `command` | The exact command executed to run the evaluation |
| `config` | Evaluation configuration including parameters and settings |
| `results` | Evaluation metrics and scores organized by groups and tasks |
| `target` | Model and API endpoint configuration details |
| `git_hash` | Git commit hash (if available) |


## Key Metrics

| Metric | Description |
|--------|-------------|
| Score Value | Primary performance metric |
| Standard Deviation | Measure of score variability |
| Standard Error | Statistical error measure |
In the example output above, the metric used is the micro-average across the samples (thus `micro` key in the structure).
The types of metrics available in the results differ for different evaluation harness and task, but they are always presented using the same structure as shown above.

## Additional Output Files

| File Type | Description |
|-----------|-------------|
| HTML Report | Human-readable evaluation report |
| JSON Report | Machine-readable evaluation data |
| Cache Files | Cached requests and responses |
| Log Files | Detailed execution logs |


## Exporting the Results

Once the evaluation has finished and the `results.yaml` file was produced, the scores can be exported.
In this example we show how the local export works. For information on other exporters, see {ref}`exporters-overview`.


The results can be exported using the following command:

```bash
nemo-evaluator-launcher export <invocation_id> --dest local --format json
```

This command extracts the scores from the `results.yaml` and creates a `processed_results.json` file with the following content:

```{literalinclude} ./_snippets/processed_results.json
:language: json
```

The `nemo-evaluator-launcher export` can accept multiple invocation IDs and gather results accross different invocations, regardless if they have been run locally or using remote executors (see {ref}`executors-overview`), e.g.:

```bash
nemo-evaluator-launcher export <local-job-id> <slurm-job-id> --dest local --format json --output_dir combined-results
```

will create the `combined-results/processed_results.json` with the same stracuture as in the example above.