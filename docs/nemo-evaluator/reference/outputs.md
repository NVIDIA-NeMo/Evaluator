(evaluation-outputs)=
# Evaluation Outputs

This document describes the output artifacts generated by NeMo Evaluator during evaluation runs. These artifacts provide comprehensive information about evaluation results, configuration, metrics, and execution details.

## Output Directory Structure

After running an evaluation, NeMo Evaluator creates a structured output directory containing various artifacts. All artifacts are stored in the directory specified by the `--output_dir` parameter:

```
<output_dir>/
│   ├── run_config.yml               # Task-specific configuration used during execution
│   ├── eval_factory_metrics.json    # Evaluation metrics and performance statistics
│   ├── results.yml                  # Detailed results in YAML format
│   ├── report.html                  # Human-readable HTML report
│   ├── report.json                  # JSON format report
│   └── <Task specific arifacts>/    # Task-specific artifacts
```

## Core Artifacts

### `results.yml`

**Description**: **Standardized** evaluation results in YAML format. Follows the [API dataclasses](https://github.com/NVIDIA-NeMo/Evaluator/blob/main/packages/nemo-evaluator/src/nemo_evaluator/api/api_dataclasses.py) specification.

**Contents**:
- Final evaluation scores and metrics
- Evaluation configuration used

**Usage**: This is the primary results file for programmatic analysis and integration with downstream tools.

See [Example Structure](#results-yml-example) below.

### `run_config.yml`

**Description**: Complete configuration used during the evaluation run, including all parameters, overrides, and settings.

**Contents**:
- Task configuration and parameters
- Model endpoint settings
- {ref}`configuring-interceptors`
- Evaluation-specific overrides

**Usage**: Enables exact reproduction of evaluation runs and configuration auditing.

**Reproducible Evaluations**: Use `run_config.yml` to relaunch evaluations with identical parameters:
```bash
eval-factory run_eval --run_config <path/to/run_config.yml> --output_dir <new_output_dir>
```
See {ref}`Run Configuration <cli-reference:run-configuration>` for details.

### `eval_factory_metrics.json`

**Description**: Detailed metrics and performance statistics collected during evaluation execution.

**Contents**:
- Request/response timing statistics
- Token usage statistics
- Error rates
- Resource utilization metrics

**Usage**: Performance analysis.

See [Example Structure](#eval-factory-metrics-json-example) below.

## Report Artifacts

### `report.html`

**Description**: Human-readable HTML report with visualizations, summaries, and detailed analysis of evaluation results.

**Usage**: Useful for sharing, analysis and debugging - shows example request/response pairs sent to endpoints and raw command reproducibility.

### `report.json`

**Description**: Machine-readable JSON version of the HTML report for programmatic processing.

**Usage**: Integration with automated analysis tools.

## Task-Specific Artifacts

in the results dir you will also find original benchmark artifacts.

## Advanced Features

### Adapter-Based Artifact Collection

Many adapters are capable of collecting, saving, and logging artifacts during runtime, providing enhanced observability and debugging capabilities. Examples include:

- **{ref}`Request Logging <configuring-interceptors:request-logging-interceptor>`**: Adapters can log all incoming requests with full context and metadata
- **{ref}`Response Logging <configuring-interceptors:response-logging-interceptor>`**: Complete response data including tokens, timing, and model-specific information

For detailed configuration options, see {ref}`configuring-interceptors`.

These artifacts can be programmatically accessed for:

- **Results Analysis**: Parse `results.yml` for scores and metrics
- **Configuration Management**: Use `run_config.yml` for reproducible runs
- **Performance Monitoring**: Analyze `eval_factory_metrics.json` 

## Example Structures

### `results.yml` Example {#results-yml-example}

This example shows results from an [ifeval](https://arxiv.org/abs/2311.07911) evaluation task:

```yaml
command: 'OPENAI_API_KEY=$API_KEY lm-eval --tasks ifeval --model local-chat-completions
  --model_args "base_url=https://integrate.api.nvidia.com/v1/chat/completions,model=meta/llama-3.1-8b-instruct,tokenized_requests=False,,tokenizer_backend=None,num_concurrent=4,timeout=3600,max_retries=5,stream=False"
  --log_samples --output_path /results --use_cache /results/lm_cache  --fewshot_as_multiturn
  --apply_chat_template   --gen_kwargs="temperature=1e-07,top_p=0.9999999" '
config:
  output_dir: /results
  params:
    extra: {}
    limit_samples: null
    max_new_tokens: null
    max_retries: null
    parallelism: 4
    request_timeout: 3600
    task: null
    temperature: null
    top_p: null
  supported_endpoint_types: null
  type: ifeval
git_hash: 001b448ca8a445a7688a02ec1a7aa7424613e7b7
results:
  groups:
    ifeval:
      metrics:
        inst_level_loose_acc:
          scores:
            inst_level_loose_acc:
              stats: {}
              value: 0.8525179856115108
        inst_level_strict_acc:
          scores:
            inst_level_strict_acc:
              stats: {}
              value: 0.8105515587529976
        prompt_level_loose_acc:
          scores:
            prompt_level_loose_acc:
              stats:
                stderr: 0.01760595448210665
              value: 0.7874306839186691
        prompt_level_strict_acc:
          scores:
            prompt_level_strict_acc:
              stats:
                stderr: 0.018976469193346637
              value: 0.7356746765249538
  tasks:
    ifeval:
      metrics:
        inst_level_loose_acc:
          scores:
            inst_level_loose_acc:
              stats: {}
              value: 0.8525179856115108
        inst_level_strict_acc:
          scores:
            inst_level_strict_acc:
              stats: {}
              value: 0.8105515587529976
        prompt_level_loose_acc:
          scores:
            prompt_level_loose_acc:
              stats:
                stderr: 0.01760595448210665
              value: 0.7874306839186691
        prompt_level_strict_acc:
          scores:
            prompt_level_strict_acc:
              stats:
                stderr: 0.018976469193346637
              value: 0.7356746765249538
target:
  api_endpoint:
    adapter_config:
      caching_dir: null
      discovery:
        dirs: []
        modules: []
      endpoint_type: chat
      generate_html_report: true
      html_report_size: 5
      interceptors:
      - config:
          cache_dir: /results/cache
          max_saved_requests: 5
          max_saved_responses: 5
          reuse_cached_responses: true
          save_requests: true
          save_responses: true
        enabled: true
        name: caching
      - config: {}
        enabled: true
        name: endpoint
      - config:
          cache_dir: /results/response_stats_cache
          logging_aggregated_stats_interval: 100
        enabled: true
        name: response_stats
      log_failed_requests: false
      post_eval_hooks:
      - config:
          html_report_size: 5
          report_types:
          - html
          - json
        enabled: true
        name: post_eval_report
      tracking_requests_stats: true
    api_key: API_KEY
    model_id: meta/llama-3.1-8b-instruct
    stream: null
    type: chat
    url: https://integrate.api.nvidia.com/v1/chat/completions
```

### `eval_factory_metrics.json` Example {#eval-factory-metrics-json-example}

This example shows metrics from an [ifeval](https://arxiv.org/abs/2311.07911) evaluation task:

```json
{
  "response_stats": {
    "description": "Response statistics saved during processing",
    "avg_prompt_tokens": 55.92,
    "avg_total_tokens": 391.02,
    "avg_completion_tokens": 335.04,
    "avg_latency_ms": 11459.88,
    "max_prompt_tokens": 370,
    "max_total_tokens": 1405,
    "max_completion_tokens": 1280,
    "max_latency_ms": 56991.39,
    "count": 541,
    "successful_count": 541,
    "tool_calls_count": 0,
    "function_calls_count": 0,
    "finish_reason": {
      "stop": 528,
      "length": 13
    },
    "stop_reason": {},
    "status_codes": {
      "200": 541
    },
    "inference_time": 1583.232442855835,
    "run_id": 0,
    "last_request_time": 1758703765.8184712,
    "inference_run_times": {
      "0": {
        "run_start": "2025-09-24T08:22:56.092597",
        "first_request_time": "2025-09-24T08:23:02.586028",
        "last_request_time": "2025-09-24T08:49:25.818471",
        "inference_time": 1583.232442855835,
        "time_to_first_request_seconds": 6.493
      }
    }
  },
  "evaluation": {
    "runtime_seconds": 1591.1531114578247,
    "start_time": "2025-09-24T08:22:55.%fZ",
    "end_time": "2025-09-24T08:49:27.%fZ",
    "token_usage": {
      "total_tokens": 211523,
      "prompt_tokens": 30255,
      "completion_tokens": 181268,
      "total_cached_requests": 541
    },
    "peak_memory_bytes": 59506688,
    "peak_tree_memory_bytes": 558637056
  }
}
```
