# Tasks Catalog

This page provides an overview of all available evaluation harnesses. Click on a harness card to view its tasks.

::::{grid} 1 2 2 2
:gutter: 1 1 1 2

:::{grid-item-card} bfcl
:link: ../_resources/harnesses_autogen/bfcl
:link-type: doc
The Berkeley Function Calling Leaderboard V3 (also called Berkeley Tool Calling Leaderboard V3) evaluates the LLM's ability to call functions (aka tools) accurately.

6 tasks
:::

:::{grid-item-card} bigcode-evaluation-harness
:link: ../_resources/harnesses_autogen/bigcode-evaluation-harness
:link-type: doc
A framework for the evaluation of autoregressive code generation language models.

25 tasks
:::

:::{grid-item-card} garak
:link: ../_resources/harnesses_autogen/garak
:link-type: doc
Garak is an LLM vulnerability scanner.

1 task
:::

:::{grid-item-card} helm
:link: ../_resources/harnesses_autogen/helm
:link-type: doc
A framework for evaluating large language models in medical applications across various healthcare tasks

15 tasks
:::

:::{grid-item-card} hle
:link: ../_resources/harnesses_autogen/hle
:link-type: doc
Humanity's Last Exam (HLE) is a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. Humanity's Last Exam consists of 3,000 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading.

1 task
:::

:::{grid-item-card} ifbench
:link: ../_resources/harnesses_autogen/ifbench
:link-type: doc
This repo contains IFBench, which is a new, challenging benchmark for precise instruction following.

1 task
:::

:::{grid-item-card} livecodebench
:link: ../_resources/harnesses_autogen/livecodebench
:link-type: doc
Holistic and Contamination Free Evaluation of Large Language Models for Code. Paper https://arxiv.org/pdf/2403.07974

14 tasks
:::

:::{grid-item-card} lm-evaluation-harness
:link: ../_resources/harnesses_autogen/lm-evaluation-harness
:link-type: doc
This project provides a unified framework to test generative language models on a large number of different evaluation tasks.

124 tasks
:::

:::{grid-item-card} mtbench
:link: ../_resources/harnesses_autogen/mtbench
:link-type: doc
MT-bench is designed to test multi-turn conversation and instruction-following ability, covering common use cases and focusing on challenging questions to differentiate models.

2 tasks
:::

:::{grid-item-card} profbench
:link: ../_resources/harnesses_autogen/profbench
:link-type: doc
Professional domain benchmark for evaluating LLMs on Physics PhD, Chemistry PhD, Finance MBA, and Consulting MBA tasks

2 tasks
:::

:::{grid-item-card} safety-eval
:link: ../_resources/harnesses_autogen/safety-eval
:link-type: doc
Home for Safety evaluations

3 tasks
:::

:::{grid-item-card} scicode
:link: ../_resources/harnesses_autogen/scicode
:link-type: doc
SciCode is a challenging benchmark designed to evaluate the capabilities of LLMs in generating code for solving realistic scientific research problems.

3 tasks
:::

:::{grid-item-card} simple-evals
:link: ../_resources/harnesses_autogen/simple-evals
:link-type: doc
simple-evals This repository contains a lightweight library for evaluating language models.

83 tasks
:::

:::{grid-item-card} tooltalk
:link: ../_resources/harnesses_autogen/tooltalk
:link-type: doc
ToolTalk is designed to evaluate tool-augmented LLMs as a chatbot. ToolTalk contains a handcrafted dataset of 28 easy conversations and 50 hard conversations.

1 task
:::

:::{grid-item-card} vlmevalkit
:link: ../_resources/harnesses_autogen/vlmevalkit
:link-type: doc
VLMEvalKit is an open-source evaluation toolkit of large vision-language models (LVLMs). It enables one-command evaluation of LVLMs on various benchmarks, without the heavy workload of data preparation under multiple repositories. In VLMEvalKit, we adopt generation-based evaluation for all LVLMs, and provide the evaluation results obtained with both exact matching and LLM-based answer extraction.

7 tasks
:::

::::

:::{toctree}
:caption: Harnesses
:hidden:

bfcl <../_resources/harnesses_autogen/bfcl>
bigcode-evaluation-harness <../_resources/harnesses_autogen/bigcode-evaluation-harness>
garak <../_resources/harnesses_autogen/garak>
helm <../_resources/harnesses_autogen/helm>
hle <../_resources/harnesses_autogen/hle>
ifbench <../_resources/harnesses_autogen/ifbench>
livecodebench <../_resources/harnesses_autogen/livecodebench>
lm-evaluation-harness <../_resources/harnesses_autogen/lm-evaluation-harness>
mtbench <../_resources/harnesses_autogen/mtbench>
profbench <../_resources/harnesses_autogen/profbench>
safety-eval <../_resources/harnesses_autogen/safety-eval>
scicode <../_resources/harnesses_autogen/scicode>
simple-evals <../_resources/harnesses_autogen/simple-evals>
tooltalk <../_resources/harnesses_autogen/tooltalk>
vlmevalkit <../_resources/harnesses_autogen/vlmevalkit>
:::
