# Tasks Catalog

This page provides an overview of all available evaluation harnesses. Click on a harness card to view its tasks.

::::{grid} 1 2 2 2
:gutter: 1 1 1 2

:::{grid-item-card} bfcl
:link: ../_resources/harnesses_autogen/bfcl
:link-type: doc
Single-turn, Live and Non-Live, AST and Exec evaluation

6 tasks
:::

:::{grid-item-card} bigcode-evaluation-harness
:link: ../_resources/harnesses_autogen/bigcode-evaluation-harness
:link-type: doc
HumanEval is used to measure functional correctness for synthesizing programs from docstrings....

25 tasks
:::

:::{grid-item-card} garak
:link: ../_resources/harnesses_autogen/garak
:link-type: doc
garak

1 task
:::

:::{grid-item-card} helm
:link: ../_resources/harnesses_autogen/helm
:link-type: doc
Extract and structure information from patient-doctor conversations

15 tasks
:::

:::{grid-item-card} hle
:link: ../_resources/harnesses_autogen/hle
:link-type: doc
hle

1 task
:::

:::{grid-item-card} ifbench
:link: ../_resources/harnesses_autogen/ifbench
:link-type: doc
ifbench

1 task
:::

:::{grid-item-card} livecodebench
:link: ../_resources/harnesses_autogen/livecodebench
:link-type: doc
AA code generation evaluating code comprehension ability....

14 tasks
:::

:::{grid-item-card} lm-evaluation-harness
:link: ../_resources/harnesses_autogen/lm-evaluation-harness
:link-type: doc
ADLR version of the AGIEval-EN-CoT benchmark used by NVIDIA Applied Deep Learning Research team (ADL...

124 tasks
:::

:::{grid-item-card} mtbench
:link: ../_resources/harnesses_autogen/mtbench
:link-type: doc
Standard MT-Bench

2 tasks
:::

:::{grid-item-card} profbench
:link: ../_resources/harnesses_autogen/profbench
:link-type: doc
Run LLM judge on provided ProfBench reports and score them

2 tasks
:::

:::{grid-item-card} safety-eval
:link: ../_resources/harnesses_autogen/safety-eval
:link-type: doc
Aegis V2 without evaluating reasoning traces.

3 tasks
:::

:::{grid-item-card} scicode
:link: ../_resources/harnesses_autogen/scicode
:link-type: doc
- SciCode is a challenging benchmark designed to evaluate the capabilities of LLMs in generating cod...

4 tasks
:::

:::{grid-item-card} simple-evals
:link: ../_resources/harnesses_autogen/simple-evals
:link-type: doc
AA AIME 2024 questions, math

86 tasks
:::

:::{grid-item-card} tooltalk
:link: ../_resources/harnesses_autogen/tooltalk
:link-type: doc
tooltalk

1 task
:::

:::{grid-item-card} vlmevalkit
:link: ../_resources/harnesses_autogen/vlmevalkit
:link-type: doc
A benchmark for evaluating diagram understanding capabilities of large vision-language models.

7 tasks
:::

::::

:::{toctree}
:caption: Harnesses
:hidden:

bfcl <../_resources/harnesses_autogen/bfcl>
bigcode-evaluation-harness <../_resources/harnesses_autogen/bigcode-evaluation-harness>
garak <../_resources/harnesses_autogen/garak>
helm <../_resources/harnesses_autogen/helm>
hle <../_resources/harnesses_autogen/hle>
ifbench <../_resources/harnesses_autogen/ifbench>
livecodebench <../_resources/harnesses_autogen/livecodebench>
lm-evaluation-harness <../_resources/harnesses_autogen/lm-evaluation-harness>
mtbench <../_resources/harnesses_autogen/mtbench>
profbench <../_resources/harnesses_autogen/profbench>
safety-eval <../_resources/harnesses_autogen/safety-eval>
scicode <../_resources/harnesses_autogen/scicode>
simple-evals <../_resources/harnesses_autogen/simple-evals>
tooltalk <../_resources/harnesses_autogen/tooltalk>
vlmevalkit <../_resources/harnesses_autogen/vlmevalkit>
:::
