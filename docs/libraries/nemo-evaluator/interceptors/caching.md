(interceptor-caching)=

# Caching

The caching interceptor stores and retrieves responses to improve performance, reduce API costs, and enable reproducible evaluations.

## Overview

The `CachingInterceptor` implements a sophisticated caching system that can store responses based on request content, enabling faster re-runs of evaluations and reducing costs when using paid APIs.

## Configuration

### Interceptor Configuration

The caching interceptor can be configured through the interceptors list in AdapterConfig:

```python
from nemo_evaluator.adapters.adapter_config import AdapterConfig, InterceptorConfig

adapter_config = AdapterConfig(
    interceptors=[
        InterceptorConfig(
            name="caching",
            enabled=True,
            config={
                "cache_dir": "./evaluation_cache",
                "reuse_cached_responses": True,
                "save_requests": True,
                "save_responses": True,
                "max_saved_requests": 1000,
                "max_saved_responses": 1000
            }
        )
    ]
)
```

### Legacy AdapterConfig Parameters

For backward compatibility, you can use legacy shortcuts that automatically configure the caching interceptor:

```python
from nemo_evaluator.adapters.adapter_config import AdapterConfig

adapter_config = AdapterConfig(
    use_caching=True,              # Enable caching
    caching_dir="./cache",         # Cache directory
    save_responses=True,           # Save responses
    save_requests=False            # Don't save requests
)
```

### CLI Configuration
```bash
--overrides 'target.api_endpoint.adapter_config.use_caching=True,target.api_endpoint.adapter_config.caching_dir=./cache,target.api_endpoint.adapter_config.reuse_cached_responses=True'
```

### YAML Configuration
```yaml
target:
  api_endpoint:
    adapter_config:
      interceptors:
        - name: "caching"
          enabled: true
          config:
            cache_dir: "./evaluation_cache"
            reuse_cached_responses: true
            save_requests: true
            save_responses: true
            max_saved_requests: 1000
            max_saved_responses: 1000
```

## Configuration Options

| Parameter | Description | Default | Type |
|-----------|-------------|---------|------|
| `cache_dir` | Directory to store cache files | `"/tmp"` | str |
| `reuse_cached_responses` | Use cached responses when available | `False` | bool |
| `save_requests` | Save requests to cache storage | `False` | bool |
| `save_responses` | Save responses to cache storage | `True` | bool |
| `max_saved_requests` | Maximum number of requests to save | `None` | int \| None |
| `max_saved_responses` | Maximum number of responses to cache | `None` | int \| None |

## Cache Key Generation

The cache key is generated by hashing the relevant request fields:

```python
# Example cache key generation
cache_key = hash({
    "messages": [{"role": "user", "content": "What is 2+2?"}],
    "temperature": 0.0,
    "max_new_tokens": 512,
    "model_id": "llama-3.1-8b"
})
```

## Cache Storage Format

### Cache Entry Structure
```json
{
    "cache_key": "abc123def456",
    "request": {
        "messages": [{"role": "user", "content": "What is 2+2?"}],
        "temperature": 0.0,
        "max_new_tokens": 512
    },
    "response": {
        "choices": [
            {
                "message": {
                    "role": "assistant", 
                    "content": "2+2 equals 4."
                }
            }
        ],
        "usage": {"prompt_tokens": 45, "completion_tokens": 8}
    },
    "timestamp": "2025-01-08T10:30:00Z",
    "model_id": "megatron_model",
    "status_code": 200
}
```

## Cache Behavior

### Cache Hit Process
1. **Request arrives** at the caching interceptor
2. **Generate cache key** from request parameters  
3. **Check cache** for existing response
4. **Return cached response** if found (sets `cache_hit=True`)
5. **Skip API call** and continue to next interceptor

### Cache Miss Process  
1. **Request continues** to endpoint interceptor
2. **Response received** from model API
3. **Store response** in cache with generated key
4. **Continue processing** with response interceptors

## Use Cases

- **Development Iterations**: Cache responses during prompt engineering and testing
- **Cost Optimization**: Reduce API costs for repeated evaluations
- **Reproducible Results**: Ensure identical responses for identical requests
- **Performance**: Speed up re-runs of large evaluations
- **Offline Analysis**: Enable offline analysis of cached evaluation data

## Best Practices

### Cache Management
```python
# Configure cache with interceptors
from nemo_evaluator.adapters.adapter_config import AdapterConfig, InterceptorConfig

adapter_config = AdapterConfig(
    interceptors=[
        InterceptorConfig(
            name="caching",
            enabled=True,
            config={
                "cache_dir": "./cache",
                "reuse_cached_responses": True,
                "save_responses": True,
                "max_saved_responses": 1000  # Limit cache size
            }
        )
    ]
)
```

### Selective Caching
```python
# Enable caching for reuse only, without saving all requests
adapter_config = AdapterConfig(
    interceptors=[
        InterceptorConfig(
            name="caching",
            enabled=True,
            config={
                "cache_dir": "./cache",
                "reuse_cached_responses": True,
                "save_requests": False,      # Don't save requests
                "save_responses": True       # Only save responses
            }
        )
    ]
)
```
