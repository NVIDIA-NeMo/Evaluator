# Reasoning model evaluation configuration (chat endpoint + reasoning)
# See docs: https://docs.nvidia.com/nemo/evaluator/latest/evaluation/run-evals/reasoning.html
evaluation:
  nemo_evaluator_config:
    config:
      params:
        request_timeout: 3600
        parallelism: 32 # Lower parallelism for longer generations
        temperature: 0.6 # Recommended for reasoning models
        top_p: 0.95
        max_new_tokens: 32768 # Extended for reasoning + final answer
    target:
      api_endpoint:
        adapter_config:
          process_reasoning_traces: true # Strip reasoning tokens and collect stats
          # Older models use the system prompt to control reasoning.
          # use_system_prompt: true
          # Set to match your model, e.g. "/think" for Llama or "detailed thinking on" for Nemotron
          # custom_system_prompt: ???
          # For newer models, use payload modifier instead
          # Set to match your model e.g. `"chat_template_kwargs": {"enable_thinking": true}` for Nemotron or
          #   `"extra_body": {"chat_template_kwargs": {"thinking": True}}` for DeepSeek
          # CONFIRM IT WITH THE MODEL CARD
          params_to_add: ???

