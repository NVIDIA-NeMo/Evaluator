defaults:
  - deployment: vllm

execution:
  env_vars:
    deployment:
      HF_TOKEN: $HF_TOKEN # Required for gated HuggingFace models

deployment:
  checkpoint_path: null # Set to path if using local checkpoint
  hf_model_handle: ??? # HuggingFace model handle (e.g., meta-llama/Llama-3.1-8B)
  served_model_name: ??? # Model name for API (e.g., meta-llama/Llama-3.1-8B)
  tensor_parallel_size: 1
  data_parallel_size: 1
  extra_args: "--max-model-len 32768"
