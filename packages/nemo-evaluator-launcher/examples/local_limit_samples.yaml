# Run this config with `nv-eval run --config-dir examples --config-name local_limit_samples`.

# This is a TEST CONFIGURATION that limits all benchmarks to use only 10 samples total.
# This allows you to test your setup end-to-end quickly without running full evaluations.
#
# ⚠️  WARNING: Results from this test run should NEVER be used to compare models or
#     report benchmark performance. This is solely for testing configuration and setup.
#     Always run full evaluations (without limit_samples) for actual benchmark results.

# specify default configs for execution and deployment
defaults:
  - execution: local
  - deployment: none
  - _self_


execution:
  output_dir: llama_3_1_8b_instruct_results


target:
  api_endpoint:
    model_id: meta/llama-3.1-8b-instruct
    url: https://integrate.api.nvidia.com/v1/chat/completions
    api_key_name: API_KEY  # API Key with access to build.nvidia.com


# specify the benchmarks to evaluate
evaluation:
  overrides:  # these overrides apply to all tasks; for task-specific overrides, use the `overrides` field
    config.params.request_timeout: 3600
    config.params.limit_samples: 10  # TEST ONLY: Limits all benchmarks to 10 samples total for quick testing
    target.api_endpoint.adapter_config.use_reasoning: false  # if true, strips reasoning tokens
    target.api_endpoint.adapter_config.use_system_prompt: true
    target.api_endpoint.adapter_config.custom_system_prompt: >-
      "Think step by step."
  tasks:
    - name: gpqa_diamond
      env_vars:
        HF_TOKEN: HF_TOKEN_FOR_GPQA_DIAMOND  # Click request access for GPQA-Diamond: https://huggingface.co/datasets/Idavidrein/gpqa
