metadata:
  mapping_toml_checksum: sha256:9934747991c97fc43d93012bb8d42a25011f3cd1dbe1464acf063954948f7ee3
  num_tasks: 425
  num_harnesses: 23
harnesses:
- name: lm-evaluation-harness
  description: This project provides a unified framework to test generative language
    models on a large number of different evaluation tasks.
  full_name: Language Model Evaluation Harness
  url: https://github.com/EleutherAI/lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:26.01
  container_digest: sha256:f5e5b59b2893e48ce113c4e163b0f9baadadf80824384bcfc84591e0664aba26
  arch: multiarch
- name: mtbench
  description: MT-bench is designed to test multi-turn conversation and instruction-following
    ability, covering common use cases and focusing on challenging questions to differentiate
    models.
  full_name: MT-Bench
  url: https://github.com/lm-sys/FastChat
  container: nvcr.io/nvidia/eval-factory/mtbench:26.01
  container_digest: sha256:69c930de81fdc8d3a55824fc7ebee9632c858ba56234f43ad9d1590e7fc861b1
  arch: multiarch
- name: ifbench
  description: IFBench is a new, challenging benchmark for precise instruction following.
  full_name: Generalizing Verifiable Instruction Following
  url: https://github.com/allenai/IFBench
  container: nvcr.io/nvidia/eval-factory/ifbench:26.01
  container_digest: sha256:e99059d2e334ef97826629a004c888f7daed1adb9d724ca73274e1b93c743ac1
  arch: multiarch
- name: simple_evals
  description: simple-evals - a lightweight library for evaluating language models.
  full_name: simple-evals
  url: https://github.com/openai/simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:26.01
  container_digest: sha256:5e59e6e34ac55bb7d2b7a17466c039a5a663a9961bce70751e5a4f3f09026158
  arch: multiarch
- name: bigcode-evaluation-harness
  description: A framework for the evaluation of autoregressive code generation language
    models.
  full_name: Code Generation LM Evaluation Harness
  url: https://github.com/bigcode-project/bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:26.01
  container_digest: sha256:4efb7706a525e248347773d7368c8305ca311cbed2fdafc837a9315164170acd
  arch: multiarch
- name: livecodebench
  description: Holistic and Contamination Free Evaluation of Large Language Models
    for Code.
  full_name: LiveCodeBench
  url: https://github.com/LiveCodeBench/LiveCodeBench
  container: nvcr.io/nvidia/eval-factory/livecodebench:26.01
  container_digest: sha256:76b4ce10b3e0f839bb5f86d11319d62dfc94fc49ac72c2cb126c27c021f7f69e
  arch: multiarch
- name: scicode
  description: SciCode is a challenging benchmark designed to evaluate the capabilities
    of LLMs in generating code for solving realistic scientific research problems.
  full_name: SciCode Benchmark
  url: https://github.com/scicode-bench/SciCode
  container: nvcr.io/nvidia/eval-factory/scicode:26.01
  container_digest: sha256:f5c12499db7d8b415321c4242e5625ed69affdc1632056326790e5d55a4656e0
  arch: multiarch
- name: hle
  description: Humanity's Last Exam (HLE) is a multi-modal benchmark at the frontier
    of human knowledge, designed to be the final closed-ended academic benchmark of
    its kind with broad subject coverage. Humanity's Last Exam consists of 3,000 questions
    across dozens of subjects, including mathematics, humanities, and the natural
    sciences. HLE is developed globally by subject-matter experts and consists of
    multiple-choice and short-answer questions suitable for automated grading.
  full_name: Humanity's Last Exam
  url: https://github.com/centerforaisafety/hle
  container: nvcr.io/nvidia/eval-factory/hle:26.01
  container_digest: sha256:59fa69e20bbaaa251effa5f9d440d60920bc601cfb26f9e03866f1b6aff6dc33
  arch: multiarch
- name: bfcl
  description: The Berkeley Function Calling Leaderboard V3 (also called Berkeley
    Tool Calling Leaderboard V3) evaluates the LLM's ability to call functions (aka
    tools) accurately.
  full_name: Berkeley Function Calling Leaderboard
  url: https://github.com/ShishirPatil/gorilla/tree/main/berkeley-function-call-leaderboard
  container: nvcr.io/nvidia/eval-factory/bfcl:26.01
  container_digest: sha256:5016e1f2b9984f5d348ac3806974d7b5d6ff6f550605f3220a3f08318e0c60c9
  arch: multiarch
- name: profbench
  description: Professional domain benchmark for evaluating LLMs on Physics PhD, Chemistry
    PhD, Finance MBA, and Consulting MBA tasks
  full_name: ProfBench
  url: https://github.com/nvidia/profbench
  container: nvcr.io/nvidia/eval-factory/profbench:26.01
  container_digest: sha256:7b2766affe4c2070ec803a893f7bf1ff2fc735df562aa520ec910c9ef58d3598
  arch: multiarch
- name: vlmevalkit
  description: VLMEvalKit is an open-source evaluation toolkit of large vision-language
    models (LVLMs). It enables one-command evaluation of LVLMs on various benchmarks,
    without the heavy workload of data preparation under multiple repositories. In
    VLMEvalKit, we adopt generation-based evaluation for all LVLMs, and provide the
    evaluation results obtained with both exact matching and LLM-based answer extraction.
  full_name: VLMEvalKit
  url: https://github.com/open-compass/VLMEvalKit
  container: nvcr.io/nvidia/eval-factory/vlmevalkit:26.01
  container_digest: sha256:24c650c547cfd666bcc5ec822c996eb90e89e4964a1d4ec29e4d01d8bd3a22dc
  arch: amd
- name: garak
  description: Garak is an LLM vulnerability scanner.
  full_name: Garak
  url: https://github.com/NVIDIA/garak/tree/main
  container: nvcr.io/nvidia/eval-factory/garak:26.01
  container_digest: sha256:72514ac2c35f76fdb139b02f1c1d4159103969946a121592e50b129087dd455e
  arch: multiarch
- name: nemo_skills
  description: NeMo Skills - a project to improve skills of LLMs
  full_name: NeMo Skills
  url: https://github.com/NVIDIA/NeMo-Skills
  container: nvcr.io/nvidia/eval-factory/nemo-skills:26.01
  container_digest: sha256:43e2c4d6e197744f7fd0a874d06c5600a8b46b54e16d333c0ebf057b6d54635a
  arch: multiarch
- name: safety_eval
  description: Harness for Safety evaluations
  full_name: Safety Harness
  url: null
  container: nvcr.io/nvidia/eval-factory/safety-harness:26.01
  container_digest: sha256:86df570bd581059d1a5133dcc055ea1adb3e2308ac36414e1377331f8eabba76
  arch: multiarch
- name: helm
  description: A framework for evaluating large language models in medical applications
    across various healthcare tasks
  full_name: Holistic Evaluation of Language Models for Medical Applications
  url: https://github.com/stanford-crfm/helm
  container: nvcr.io/nvidia/eval-factory/helm:26.01
  container_digest: sha256:58be32aed07b94d104b9b72130bf94ee03dc16b16ded14416e21c97b62970589
  arch: amd
- name: tooltalk
  description: ToolTalk is designed to evaluate tool-augmented LLMs as a chatbot.
    ToolTalk contains a handcrafted dataset of 28 easy conversations and 50 hard conversations.
  full_name: ToolTalk
  url: https://github.com/microsoft/ToolTalk
  container: nvcr.io/nvidia/eval-factory/tooltalk:26.01
  container_digest: sha256:2c032e8274fd3a825b3c2774d33d0caddfa198fe24980dd99b8e3ae77c8aadee
  arch: multiarch
- name: genai_perf_eval
  description: GenAI Perf is a tool to evaluate the performance of LLM endpoints,
    based on GenAI Perf.
  full_name: GenAI Perf
  url: https://github.com/triton-inference-server
  container: nvcr.io/nvidia/eval-factory/genai-perf:26.01
  container_digest: sha256:ab3f8b34a6cb63f7e48e8847fb069be71a3b73eb4f924bcf274cb02c6cc975b6
  arch: amd
- name: mmath
  description: MMATH is a new benchmark specifically designed for multilingual complex
    reasoning. It comprises 374 carefully selected math problems from high-quality
    sources, including AIME, CNMO, and MATH-500, and covers ten typologically and
    geographically diverse languages. Each problem is translated and validated through
    a rigorous pipeline that combines frontier LLMs with human verification, ensuring
    semantic consistency.
  full_name: 'MMATH: A Multilingual Benchmark for Mathematical Reasoning'
  url: https://github.com/RUCAIBox/MMATH/
  container: nvcr.io/nvidia/eval-factory/mmath:26.01
  container_digest: sha256:da033bf95efd05af58d2ab06feb2344dbca60678f3075a4bf7f53899901c5efc
  arch: multiarch
- name: tau2_bench
  description: Evaluating Conversational Agents in a Dual-Control Environment
  full_name: Tau2-Bench
  url: https://github.com/sierra-research/tau2-bench.git
  container: nvcr.io/nvidia/eval-factory/tau2-bench:26.01
  container_digest: sha256:24aae1ed0eb955810a597382b1cbbfd8da64f9f74e1e64a4afd6a271d1b98be3
  arch: multiarch
- name: ruler
  description: RULER generates synthetic examples to evaluate long-context language
    models with configurable sequence length and task complexity.
  full_name: RULER
  url: https://github.com/NVIDIA/RULER
  container: nvcr.io/nvidia/eval-factory/long-context-eval:26.01
  container_digest: sha256:461a74e48403c66058797cbfb6f42b1cc769b33f92dbd0503706586b2eb84689
  arch: multiarch
- name: codec
  description: Contamination detection framework for evaluating language models
  full_name: contamination-detection-via-context
  url: null
  container: nvcr.io/nvidia/eval-factory/contamination-detection:26.01
  container_digest: sha256:e16f56f78f4b36b3b1b6ce6da3d6bef7937ea578b1b0ba4595a1c71f927550e2
  arch: amd
- name: mteb
  description: The Massive Text Embedding Benchmark (MTEB) is a comprehensive benchmark
    designed to evaluate the performance of text embedding models across a wide range
    of tasks and datasets. It includes 58 datasets covering 8 tasks and 112 languages.
  full_name: Massive Text Embedding Benchmark
  url: https://github.com/embeddings-benchmark/mteb
  container: nvcr.io/nvidia/eval-factory/mteb:26.01
  container_digest: sha256:fb0ea5360bec880d4ecbfc63015d775dc3d22601e5ab17d760a992402646cbbb
  arch: multiarch
- name: AA-LCR
  description: A challenging benchmark measuring language models' ability to extract,
    reason about, and synthesize information from long-form documents ranging from
    10k to 100k tokens (measured using the cl100k_base tokenizer).
  full_name: Artificial Analysis Long Context Reasoning Benchmark
  url: https://huggingface.co/datasets/ArtificialAnalysis/AA-LCR
  container: nvcr.io/nvidia/eval-factory/aa-lcr:26.01
  container_digest: sha256:67dd35302ed15610afc9471a2ff4f515d95a235753f1b259db60748249366939
  arch: multiarch
tasks:
- name: mmlu
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark covers
    57 subjects across various fields, testing both world knowledge and problem-solving
    abilities. - This variant uses text generation.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_str
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
          args: --trust_remote_code
      supported_endpoint_types:
      - completions
      type: mmlu
    target:
      api_endpoint:
        stream: false
- name: mmlu_instruct
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark covers
    57 subjects across various fields, testing both world knowledge and problem-solving
    abilities. - This variant uses the chat endpoint, defaults to zero-shot evaluation
    and instructs the model to produce a single letter response.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_str
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
          args: --trust_remote_code --add_instruction
      supported_endpoint_types:
      - chat
      type: mmlu_instruct
    target:
      api_endpoint:
        stream: false
- name: mmlu_instruct_completions
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark covers
    57 subjects across various fields, testing both world knowledge and problem-solving
    abilities. - This variant uses the completions endpoint, defaults to zero-shot
    evaluation and instructs the model to produce a single letter response.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_str
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
          args: --trust_remote_code --add_instruction
      supported_endpoint_types:
      - completions
      type: mmlu_instruct_completions
    target:
      api_endpoint:
        stream: false
- name: mmlu_cot_0_shot_chat
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark covers
    57 subjects across various fields, testing both world knowledge and problem-solving
    abilities. - This variant defaults to chain-of-thought zero-shot evaluation.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_cot_0_shot_chat
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          args: --trust_remote_code
      supported_endpoint_types:
      - chat
      type: mmlu_cot_0_shot_chat
    target:
      api_endpoint:
        stream: false
- name: ifeval
  description: IFEval is a dataset designed to test a model's ability to follow explicit
    instructions, such as "include keyword x" or "use format y." The focus is on the
    model's adherence to formatting instructions rather than the content generated,
    allowing for the use of strict and rigorous metrics.
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: ifeval
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: ifeval
    target:
      api_endpoint:
        stream: false
- name: mmlu_pro
  description: MMLU-Pro is a refined version of the MMLU dataset with 10 choices instead
    of 4 (completions endpoint).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_pro
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: mmlu_pro
    target:
      api_endpoint:
        stream: false
- name: mmlu_pro_instruct
  description: '- MMLU-Pro is a refined version of the MMLU dataset with 10 choices
    instead of 4. - This variant applies a chat template and defaults to zero-shot
    evaluation.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: mmlu_pro
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
      supported_endpoint_types:
      - chat
      type: mmlu_pro_instruct
    target:
      api_endpoint:
        stream: false
- name: mmlu_redux
  description: MMLU-Redux is a subset of 3,000 manually re-annotated questions across
    30 MMLU subjects.
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_redux
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: mmlu_redux
    target:
      api_endpoint:
        stream: false
- name: mmlu_redux_instruct
  description: '- MMLU-Redux is a subset of 3,000 manually re-annotated questions
    across 30 MMLU subjects. - This variant applies a chat template and defaults to
    zero-shot evaluation.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 8192
        max_retries: 5
        parallelism: 10
        task: mmlu_redux
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
          args: --add_instruction
      supported_endpoint_types:
      - chat
      type: mmlu_redux_instruct
    target:
      api_endpoint:
        stream: false
- name: m_mmlu_id_str_chat
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark translated
    to Indonesian with string-based evaluation (chat endpoint).'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: m_mmlu_id_str
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
          args: --trust_remote_code
      supported_endpoint_types:
      - chat
      type: m_mmlu_id_str_chat
    target:
      api_endpoint:
        stream: false
- name: m_mmlu_id_str_completions
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark translated
    to Indonesian with string-based evaluation (completions endpoint).'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: m_mmlu_id_str
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
          args: --trust_remote_code
      supported_endpoint_types:
      - completions
      type: m_mmlu_id_str_completions
    target:
      api_endpoint:
        stream: false
- name: gsm8k
  description: The GSM8K benchmark evaluates the arithmetic reasoning of large language
    models using 1,319 grade school math word problems.
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: gsm8k
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: gsm8k
    target:
      api_endpoint:
        stream: false
- name: gsm8k_cot_instruct
  description: '- The GSM8K benchmark evaluates the arithmetic reasoning of large
    language models using 1,319 grade school math word problems. - This variant defaults
    to chain-of-thought zero-shot evaluation with custom instructions.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: gsm8k_zeroshot_cot
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          args: --add_instruction
      supported_endpoint_types:
      - chat
      type: gsm8k_cot_instruct
    target:
      api_endpoint:
        stream: false
- name: gsm8k_cot_zeroshot
  description: '- The GSM8K benchmark evaluates the arithmetic reasoning of large
    language models using 1,319 grade school math word problems. - This variant defaults
    to chain-of-thought zero-shot evaluation.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: gsm8k_cot_zeroshot
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: gsm8k_cot_zeroshot
    target:
      api_endpoint:
        stream: false
- name: gsm8k_cot_llama
  description: '- The GSM8K benchmark evaluates the arithmetic reasoning of large
    language models using 1,319 grade school math word problems. - This variant defaults
    to chain-of-thought evaluation - implementation taken from llama.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: gsm8k_cot_llama
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: gsm8k_cot_llama
    target:
      api_endpoint:
        stream: false
- name: gsm8k_cot_zeroshot_llama
  description: '- The GSM8K benchmark evaluates the arithmetic reasoning of large
    language models using 1,319 grade school math word problems. - This variant defaults
    to chain-of-thought zero-shot evaluation - implementation taken from llama.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: gsm8k_cot_llama
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
      supported_endpoint_types:
      - chat
      type: gsm8k_cot_zeroshot_llama
    target:
      api_endpoint:
        stream: false
- name: humaneval_instruct
  description: '- The HumanEval benchmark measures functional correctness for synthesizing
    programs from docstrings. - Implementation taken from llama.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: humaneval_instruct
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: humaneval_instruct
    target:
      api_endpoint:
        stream: false
- name: mbpp_plus_chat
  description: MBPP EvalPlus is an extension of the MBPP benchmark with 35x more test
    cases (chat endpoint).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mbpp_plus
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          args: --confirm_run_unsafe_code
      supported_endpoint_types:
      - chat
      type: mbpp_plus_chat
    target:
      api_endpoint:
        stream: false
- name: mbpp_plus_completions
  description: MBPP EvalPlus is an extension of the MBPP benchmark with 35x more test
    cases (completions endpoint).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mbpp_plus
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          args: --confirm_run_unsafe_code
      supported_endpoint_types:
      - completions
      type: mbpp_plus_completions
    target:
      api_endpoint:
        stream: false
- name: mgsm
  description: '- The Multilingual Grade School Math (MGSM) benchmark consists of
    250 grade-school math problems from the GSM8K dataset, translated into ten languages.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mgsm_direct
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: mgsm
    target:
      api_endpoint:
        stream: false
- name: mgsm_cot_chat
  description: '- The Multilingual Grade School Math (MGSM) benchmark consists of
    250 grade-school math problems from the GSM8K dataset, translated into ten languages.
    - This variant uses the chat endpoint and defaults to chain-of-thought evaluation.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: mgsm_cot_native
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
      supported_endpoint_types:
      - chat
      type: mgsm_cot_chat
    target:
      api_endpoint:
        stream: false
- name: mgsm_cot_completions
  description: '- The Multilingual Grade School Math (MGSM) benchmark consists of
    250 grade-school math problems from the GSM8K dataset, translated into ten languages.
    - This variant uses the completions endpoint and defaults to chain-of-thought
    evaluation.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: mgsm_cot_native
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
      supported_endpoint_types:
      - completions
      type: mgsm_cot_completions
    target:
      api_endpoint:
        stream: false
- name: wikilingua
  description: '- The WikiLingua benchmark is a large-scale, multilingual dataset
    designed for evaluating cross-lingual abstractive summarization systems.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: wikilingua
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          args: --trust_remote_code
      supported_endpoint_types:
      - chat
      type: wikilingua
    target:
      api_endpoint:
        stream: false
- name: winogrande
  description: WinoGrande is a collection of 44k problems formulated as a fill-in-a-blank
    task with binary options testing commonsense reasoning.
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: winogrande
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: winogrande
    target:
      api_endpoint:
        stream: false
- name: arc_challenge
  description: The ARC challenge dataset consists of 2,590 multiple-choice science
    exam questions.
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: arc_challenge
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: arc_challenge
    target:
      api_endpoint:
        stream: false
- name: arc_challenge_chat
  description: '- The ARC challenge dataset consists of 2,590 multiple-choice science
    exam questions. - This variant applies a chat template and defaults to zero-shot
    evaluation.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: arc_challenge_chat
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
      supported_endpoint_types:
      - chat
      type: arc_challenge_chat
    target:
      api_endpoint:
        stream: false
- name: hellaswag
  description: The HellaSwag benchmark tests a language model's commonsense reasoning
    by having it choose the most logical ending for a given story.
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: hellaswag
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 10
      supported_endpoint_types:
      - completions
      type: hellaswag
    target:
      api_endpoint:
        stream: false
- name: truthfulqa
  description: '- The TruthfulQA benchmark measures the truthfulness of language models
    in generating answers to questions. - It consists of 817 questions across 38 categories,
    such as health, law, finance, and politics, designed to test whether models can
    avoid generating false answers that mimic common human misconceptions.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: truthfulqa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: truthfulqa
    target:
      api_endpoint:
        stream: false
- name: bbh
  description: The BIG-Bench Hard (BBH) benchmark is a part of the BIG-Bench evaluation
    suite, focusing on 23 particularly difficult tasks that current language models
    struggle with.
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: leaderboard_bbh
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: bbh
    target:
      api_endpoint:
        stream: false
- name: bbh_instruct
  description: '- The BIG-Bench Hard (BBH) benchmark is a part of the BIG-Bench evaluation
    suite, focusing on 23 particularly difficult tasks that current language models
    struggle with. - This variant aaplies chat template and defaults to zero-shot
    evaluation.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: bbh_zeroshot
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: bbh_instruct
    target:
      api_endpoint:
        stream: false
- name: musr
  description: The MuSR (Multistep Soft Reasoning) benchmark evaluates the reasoning
    capabilities of large language models through complex, multistep tasks specified
    in natural language narratives.
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: leaderboard_musr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: musr
    target:
      api_endpoint:
        stream: false
- name: gpqa
  description: The GPQA (Graduate-Level Google-Proof Q&A) benchmark is a challenging
    dataset of 448 multiple-choice questions in biology, physics, and chemistry.
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: leaderboard_gpqa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: gpqa
    target:
      api_endpoint:
        stream: false
- name: gpqa_diamond_cot
  description: '- The GPQA (Graduate-Level Google-Proof Q&A) benchmark is a challenging
    dataset of 448 multiple-choice questions in biology, physics, and chemistry. -
    This variant uses the Diamond subset and defaults to zero-shot chain-of-thought
    evaluation.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: gpqa_diamond_cot_zeroshot
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: gpqa_diamond_cot
    target:
      api_endpoint:
        stream: false
- name: commonsense_qa
  description: '- CommonsenseQA is a multiple-choice question answering dataset that
    requires different types of commonsense knowledge to predict the correct answers.
    - It contains 12,102 questions with one correct answer and four distractor answers.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: commonsense_qa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 7
      supported_endpoint_types:
      - completions
      type: commonsense_qa
    target:
      api_endpoint:
        stream: false
- name: openbookqa
  description: '- OpenBookQA is a question-answering dataset modeled after open book
    exams for assessing human understanding of a subject. - Answering OpenBookQA questions
    requires additional broad common knowledge, not contained in the book. - The questions,
    by design, are answered incorrectly by both a retrieval-based algorithm and a
    word co-occurrence algorithm.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: openbookqa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: openbookqa
    target:
      api_endpoint:
        stream: false
- name: mmlu_logits
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark covers
    57 subjects across various fields, testing both world knowledge and problem-solving
    abilities. - This variant uses the logits of the model to evaluate the accuracy.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: mmlu_logits
    target:
      api_endpoint:
        stream: false
- name: piqa
  description: '- Physical Interaction: Question Answering (PIQA) is a physical commonsense
    reasoning benchmark designed to investigate the physical knowledge of large language
    models.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: piqa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: piqa
    target:
      api_endpoint:
        stream: false
- name: social_iqa
  description: '- Social IQa contains 38,000 multiple choice questions for probing
    emotional and social intelligence in a variety of everyday situations.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: social_iqa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          args: --trust_remote_code
      supported_endpoint_types:
      - completions
      type: social_iqa
    target:
      api_endpoint:
        stream: false
- name: adlr_agieval_en_cot
  description: Version of the AGIEval-EN-CoT benchmark used by NVIDIA Applied Deep
    Learning Research team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_agieval_en_cot
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: adlr_agieval_en_cot
    target:
      api_endpoint:
        stream: false
- name: adlr_math_500_4_shot_sampled
  description: MATH-500 Sampled version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_math_500_4_shot_sampled
        temperature: 0.7
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 4
      supported_endpoint_types:
      - completions
      type: adlr_math_500_4_shot_sampled
    target:
      api_endpoint:
        stream: false
- name: adlr_race
  description: RACE version used by NVIDIA Applied Deep Learning Research team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_race
        temperature: 1.0
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: adlr_race
    target:
      api_endpoint:
        stream: false
- name: adlr_truthfulqa_mc2
  description: TruthfulQA-MC2 version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_truthfulqa_mc2
        temperature: 1.0
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: adlr_truthfulqa_mc2
    target:
      api_endpoint:
        stream: false
- name: adlr_arc_challenge_llama_25_shot
  description: ARC-Challenge-Llama version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_arc_challenge_llama
        temperature: 1.0
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 25
      supported_endpoint_types:
      - completions
      type: adlr_arc_challenge_llama_25_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_gpqa_diamond_cot_5_shot
  description: Version of the GPQA-Diamond-CoT benchmark used by NVIDIA Applied Deep
    Learning Research team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_gpqa_diamond_cot_5_shot
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: adlr_gpqa_diamond_cot_5_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_mmlu
  description: MMLU version used by NVIDIA Applied Deep Learning Research team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_str
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
          args: --trust_remote_code
      supported_endpoint_types:
      - completions
      type: adlr_mmlu
    target:
      api_endpoint:
        stream: false
- name: adlr_mmlu_pro_5_shot_base
  description: MMLU-Pro 5-shot base version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_mmlu_pro_5_shot_base
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: adlr_mmlu_pro_5_shot_base
    target:
      api_endpoint:
        stream: false
- name: adlr_minerva_math_nemo_4_shot
  description: Minerva-Math version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_minerva_math_nemo
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 4
      supported_endpoint_types:
      - completions
      type: adlr_minerva_math_nemo_4_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_gsm8k_cot_8_shot
  description: GSM8K-CoT version used by NVIDIA Applied Deep Learning Research team
    (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_gsm8k_fewshot_cot
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 8
      supported_endpoint_types:
      - completions
      type: adlr_gsm8k_cot_8_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_humaneval_greedy
  description: HumanEval Greedy version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_humaneval_greedy
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: adlr_humaneval_greedy
    target:
      api_endpoint:
        stream: false
- name: adlr_humaneval_sampled
  description: HumanEval Sampled version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_humaneval_sampled
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: adlr_humaneval_sampled
    target:
      api_endpoint:
        stream: false
- name: adlr_mbpp_sanitized_3_shot_greedy
  description: MBPP Greedy version used by NVIDIA Applied Deep Learning Research team
    (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_mbpp_sanitized_3_shot_greedy
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 3
      supported_endpoint_types:
      - completions
      type: adlr_mbpp_sanitized_3_shot_greedy
    target:
      api_endpoint:
        stream: false
- name: adlr_mbpp_sanitized_3_shot_sampled
  description: MBPP Sampled version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_mbpp_sanitized_3shot_sampled
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 3
      supported_endpoint_types:
      - completions
      type: adlr_mbpp_sanitized_3_shot_sampled
    target:
      api_endpoint:
        stream: false
- name: adlr_global_mmlu_lite_5_shot
  description: Global-MMLU subset (8 languages - es, de, fr, zh, it, ja, pt, ko) used
    by NVIDIA Applied Deep Learning Research team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_global_mmlu
        temperature: 1.0
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: adlr_global_mmlu_lite_5_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_mgsm_native_cot_8_shot
  description: MGSM native CoT subset (6 languages - es, de, fr, zh, ja, ru) used
    by NVIDIA Applied Deep Learning Research team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_mgsm_native_cot_8_shot
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 8
      supported_endpoint_types:
      - completions
      type: adlr_mgsm_native_cot_8_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_commonsense_qa_7_shot
  description: CommonsenseQA version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: commonsense_qa
        temperature: 1.0
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 7
      supported_endpoint_types:
      - completions
      type: adlr_commonsense_qa_7_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_winogrande_5_shot
  description: Winogrande version used by NVIDIA Applied Deep Learning Research team
    (ADLR).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: winogrande
        temperature: 1.0
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: adlr_winogrande_5_shot
    target:
      api_endpoint:
        stream: false
- name: bbq_chat
  description: The BBQ (Bias Benchmark for QA) is a benchmark designed to measure
    social biases in question answering systems. It contains ambiguous questions spanning
    9 categories - disability, gender, nationality, physical appearance, race/ethnicity,
    religion, sexual orientation, socioeconomic status, and age (chat endpoint).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: bbq_generate
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: bbq_chat
    target:
      api_endpoint:
        stream: false
- name: bbq_completions
  description: The BBQ (Bias Benchmark for QA) is a benchmark designed to measure
    social biases in question answering systems. It contains ambiguous questions spanning
    9 categories - disability, gender, nationality, physical appearance, race/ethnicity,
    religion, sexual orientation, socioeconomic status, and age (completions endpoint).
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: bbq_generate
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: bbq_completions
    target:
      api_endpoint:
        stream: false
- name: arc_multilingual
  description: The multilingual versions of the ARC challenge dataset.
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: arc_multilingual
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: arc_multilingual
    target:
      api_endpoint:
        stream: false
- name: hellaswag_multilingual
  description: The multilingual versions of the HellaSwag benchmark.
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: hellaswag_multilingual
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 10
      supported_endpoint_types:
      - completions
      type: hellaswag_multilingual
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_chat
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    (chat endpoint)
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: mmlu_prox_chat
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_completions
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    (completions endpoint)
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: mmlu_prox_completions
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_fr_chat
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    - French dataset (chat endpoint)
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_fr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: mmlu_prox_fr_chat
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_fr_completions
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    - French dataset (completions endpoint)
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_fr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: mmlu_prox_fr_completions
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_de_chat
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    - German dataset (chat endpoint)
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_de
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: mmlu_prox_de_chat
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_de_completions
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    - German dataset (completions endpoint)
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_de
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: mmlu_prox_de_completions
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_it_chat
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    - Italian dataset (chat endpoint)
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_it
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: mmlu_prox_it_chat
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_it_completions
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    - Italian dataset (completions endpoint)
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_it
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: mmlu_prox_it_completions
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_ja_chat
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    - Japanese dataset (chat endpoint)
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_ja
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: mmlu_prox_ja_chat
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_ja_completions
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    - Japanese dataset (completions endpoint)
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_ja
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: mmlu_prox_ja_completions
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_es_chat
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    - Spanish dataset (chat endpoint)
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_es
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: mmlu_prox_es_chat
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_es_completions
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    - Spanish dataset (completions endpoint)
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_es
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: mmlu_prox_es_completions
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full
  description: Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English.
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_am
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the AM subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_am
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_am
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ar
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the AR subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ar
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ar
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_bn
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the BN subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_bn
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_bn
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_cs
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the CS subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_cs
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_cs
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_de
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the DE subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_de
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_de
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_el
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the EL subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_el
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_el
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_en
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the EN subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_en
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_en
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_es
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the ES subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_es
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_es
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_fa
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the FA subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_fa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_fa
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_fil
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the FIL subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_fil
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_fil
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_fr
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the FR subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_fr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_fr
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ha
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the HA subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ha
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ha
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_he
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the HE subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_he
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_he
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_hi
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the HI subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_hi
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_hi
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_id
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the ID subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_id
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_id
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ig
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the IG subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ig
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ig
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_it
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the IT subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_it
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_it
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ja
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the JA subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ja
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ja
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ko
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the KO subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ko
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ko
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ky
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the KY subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ky
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ky
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_lt
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the LT subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_lt
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_lt
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_mg
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the MG subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_mg
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_mg
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ms
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the MS subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ms
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ms
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ne
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the NE subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ne
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ne
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_nl
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the NL subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_nl
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_nl
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ny
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the NY subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ny
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ny
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_pl
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the PL subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_pl
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_pl
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_pt
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the PT subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_pt
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_pt
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ro
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the RO subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ro
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ro
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ru
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the RU subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ru
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ru
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_si
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the SI subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_si
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_si
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_sn
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the SN subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_sn
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_sn
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_so
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the SO subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_so
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_so
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_sr
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the SR subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_sr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_sr
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_sv
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the SV subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_sv
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_sv
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_sw
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the SW subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_sw
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_sw
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_te
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the TE subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_te
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_te
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_tr
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the TR subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_tr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_tr
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_uk
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the UK subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_uk
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_uk
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_vi
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the VI subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_vi
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_vi
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_yo
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the YO subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_yo
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_yo
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_zh
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the ZH subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_zh
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_zh
    target:
      api_endpoint:
        stream: false
- name: global_mmlu
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - It is designed for efficient evaluation
    of multilingual models in 15 languages (including English).'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_ar
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the AR subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_ar
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_ar
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_bn
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the BN subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_bn
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_bn
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_de
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the DE subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_de
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_de
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_en
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the EN subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_en
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_en
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_es
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the ES subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_es
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_es
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_fr
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the FR subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_fr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_fr
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_hi
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the HI subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_hi
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_hi
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_id
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the ID subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_id
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_id
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_it
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the IT subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_it
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_it
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_ja
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the JA subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_ja
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_ja
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_ko
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the KO subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_ko
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_ko
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_pt
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the PT subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_pt
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_pt
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_sw
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the SW subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_sw
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_sw
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_yo
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the YO subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_yo
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_yo
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_zh
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the ZH subset.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_zh
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_zh
    target:
      api_endpoint:
        stream: false
- name: agieval
  description: AGIEval - A Human-Centric Benchmark for Evaluating Foundation Models
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: agieval
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: agieval
    target:
      api_endpoint:
        stream: false
- name: wikitext
  description: '- The WikiText language modeling dataset is a collection of over 100
    million tokens extracted from verified Good and Featured articles on Wikipedia.
    - This task measures perplexity on the WikiText-2 dataset via rolling loglikelihoods.'
  harness: lm-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: wikitext
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          args: --trust_remote_code
      supported_endpoint_types:
      - completions
      type: wikitext
    target:
      api_endpoint:
        stream: false
- name: mtbench
  description: Standard MT-Bench
  harness: mtbench
  defaults:
    command: 'mtbench-evaluator {% if target.api_endpoint.model_id is not none %}
      --model {{target.api_endpoint.model_id}}{% endif %} {% if target.api_endpoint.url
      is not none %} --url {{target.api_endpoint.url}}{% endif %} {% if target.api_endpoint.api_key_name
      is not none %} --api_key {{target.api_endpoint.api_key_name}}{% endif %} {%
      if config.params.request_timeout is not none %} --timeout {{config.params.request_timeout}}{%
      endif %} {% if config.params.max_retries is not none %} --max_retries {{config.params.max_retries}}{%
      endif %} {% if config.params.parallelism is not none %} --parallelism {{config.params.parallelism}}{%
      endif %} {% if config.params.max_new_tokens is not none %} --max_tokens {{config.params.max_new_tokens}}{%
      endif %} --workdir {{config.output_dir}} {% if config.params.temperature is
      not none %} --temperature {{config.params.temperature}}{% endif %} {% if config.params.top_p
      is not none %} --top_p {{config.params.top_p}}{% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.limit_samples
      is not none %}--first_n {{config.params.limit_samples}}{% endif %} --generate
      --judge {% if config.params.extra.judge.url is not none %} --judge_url {{config.params.extra.judge.url}}{%
      endif %} {% if config.params.extra.judge.model_id is not none %} --judge_model
      {{config.params.extra.judge.model_id}}{% endif %} {% if config.params.extra.judge.api_key_name
      is not none %} --judge_api_key_name {{config.params.extra.judge.api_key_name}}{%
      endif %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %}     '
    framework_name: mtbench
    pkg_name: mtbench
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        request_timeout: 30
        extra:
          judge:
            url: null
            model_id: gpt-4
            api_key_name: null
            request_timeout: 60
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 2048
      supported_endpoint_types:
      - chat
      type: mtbench
    target:
      api_endpoint: {}
- name: mtbench-cor1
  description: Corrected MT-Bench
  harness: mtbench
  defaults:
    command: 'mtbench-evaluator {% if target.api_endpoint.model_id is not none %}
      --model {{target.api_endpoint.model_id}}{% endif %} {% if target.api_endpoint.url
      is not none %} --url {{target.api_endpoint.url}}{% endif %} {% if target.api_endpoint.api_key_name
      is not none %} --api_key {{target.api_endpoint.api_key_name}}{% endif %} {%
      if config.params.request_timeout is not none %} --timeout {{config.params.request_timeout}}{%
      endif %} {% if config.params.max_retries is not none %} --max_retries {{config.params.max_retries}}{%
      endif %} {% if config.params.parallelism is not none %} --parallelism {{config.params.parallelism}}{%
      endif %} {% if config.params.max_new_tokens is not none %} --max_tokens {{config.params.max_new_tokens}}{%
      endif %} --workdir {{config.output_dir}} {% if config.params.temperature is
      not none %} --temperature {{config.params.temperature}}{% endif %} {% if config.params.top_p
      is not none %} --top_p {{config.params.top_p}}{% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.limit_samples
      is not none %}--first_n {{config.params.limit_samples}}{% endif %} --generate
      --judge {% if config.params.extra.judge.url is not none %} --judge_url {{config.params.extra.judge.url}}{%
      endif %} {% if config.params.extra.judge.model_id is not none %} --judge_model
      {{config.params.extra.judge.model_id}}{% endif %} {% if config.params.extra.judge.api_key_name
      is not none %} --judge_api_key_name {{config.params.extra.judge.api_key_name}}{%
      endif %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %}     '
    framework_name: mtbench
    pkg_name: mtbench
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        request_timeout: 30
        extra:
          judge:
            url: null
            model_id: gpt-4
            api_key_name: null
            request_timeout: 60
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 2048
          args: --judge_reference_model gpt-4-0125-preview
      supported_endpoint_types:
      - chat
      type: mtbench-cor1
    target:
      api_endpoint: {}
- name: ifbench
  description: IFBench with vanilla settings
  harness: ifbench
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} ifbench --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --results-dir
      {{config.output_dir}} --inference-params max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}}
      --parallelism {{config.params.parallelism}} --retries {{config.params.max_retries}}
      {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}
      {% endif %}'
    framework_name: ifbench
    pkg_name: ifbench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 8
        temperature: 0.01
        top_p: 0.95
        extra: {}
      supported_endpoint_types:
      - chat
      type: ifbench
    target:
      api_endpoint:
        stream: false
- name: ifbench_aa_v2
  description: IFBench - params aligned with Artificial Analysis Index v2
  harness: ifbench
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} ifbench --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --results-dir
      {{config.output_dir}} --inference-params max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}}
      --parallelism {{config.params.parallelism}} --retries {{config.params.max_retries}}
      {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}
      {% endif %}'
    framework_name: ifbench
    pkg_name: ifbench
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 8
        temperature: 0.0
        top_p: 0.95
        extra: {}
      supported_endpoint_types:
      - chat
      type: ifbench_aa_v2
    target:
      api_endpoint:
        stream: false
- name: AIME_2025
  description: AIME 2025 questions, math
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: AIME_2025
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: AIME_2025
    target:
      api_endpoint: {}
- name: AIME_2024
  description: AIME 2024 questions, math
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: AIME_2024
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: AIME_2024
    target:
      api_endpoint: {}
- name: AA_AIME_2024
  description: AIME 2024 questions, math, using Artificial Analysis's setup.
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: AA_AIME_2024
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: AA_AIME_2024
    target:
      api_endpoint: {}
- name: AA_math_test_500
  description: Open Ai math test 500, using Artificial Analysis's setup.
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: AA_math_test_500
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 3
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: AA_math_test_500
    target:
      api_endpoint: {}
- name: math_test_500
  description: Open AI math test 500
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: math_test_500
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: math_test_500
    target:
      api_endpoint: {}
- name: mgsm
  description: MGSM is a benchmark of grade-school math problems. The same 250 problems
    from GSM8K are each translated via human annotators in 10 languages.
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mgsm
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mgsm
    target:
      api_endpoint: {}
- name: humaneval
  description: HumanEval evaluates the performance in Python code generation tasks.
    It used to measure functional correctness for synthesizing programs from docstrings.
    It consists of 164 original programming problems, assessing language comprehension,
    algorithms, and simple mathematics, with some comparable to simple software interview
    questions.
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: humaneval
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: humaneval
    target:
      api_endpoint: {}
- name: humanevalplus
  description: HumanEvalPlus is a dataset of 164 programming problems, assessing language
    comprehension, algorithms, and simple mathematics, with some comparable to simple
    software interview questions.
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: humanevalplus
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: humanevalplus
    target:
      api_endpoint: {}
- name: mmlu_pro
  description: MMLU-Pro dataset is a more robust and challenging massive multi-task
    understanding dataset tailored to more rigorously benchmark large language models'
    capabilities. This dataset contains 12K complex questions across various disciplines.
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_pro
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pro
    target:
      api_endpoint: {}
- name: mmlu_am
  description: Global-MMLU 0-shot CoT in Amharic (am)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_am
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_am
    target:
      api_endpoint: {}
- name: mmlu_ar
  description: Global-MMLU 0-shot CoT in Arabic (ar)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ar
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ar
    target:
      api_endpoint: {}
- name: mmlu_bn
  description: Global-MMLU 0-shot CoT in Bengali (bn)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_bn
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_bn
    target:
      api_endpoint: {}
- name: mmlu_cs
  description: Global-MMLU 0-shot CoT in Czech (cs)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_cs
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_cs
    target:
      api_endpoint: {}
- name: mmlu_de
  description: Global-MMLU 0-shot CoT in German (de)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_de
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_de
    target:
      api_endpoint: {}
- name: mmlu_el
  description: Global-MMLU 0-shot CoT in Greek (el)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_el
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_el
    target:
      api_endpoint: {}
- name: mmlu_en
  description: Global-MMLU 0-shot CoT in English (en)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_en
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_en
    target:
      api_endpoint: {}
- name: mmlu_es
  description: Global-MMLU 0-shot CoT in Spanish (es)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_es
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_es
    target:
      api_endpoint: {}
- name: mmlu_fa
  description: Global-MMLU 0-shot CoT in Persian (fa)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_fa
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_fa
    target:
      api_endpoint: {}
- name: mmlu_fil
  description: Global-MMLU 0-shot CoT in Filipino (fil)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_fil
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_fil
    target:
      api_endpoint: {}
- name: mmlu_fr
  description: Global-MMLU 0-shot CoT in French (fr)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_fr
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_fr
    target:
      api_endpoint: {}
- name: mmlu_ha
  description: Global-MMLU 0-shot CoT in Hausa (ha)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ha
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ha
    target:
      api_endpoint: {}
- name: mmlu_he
  description: Global-MMLU 0-shot CoT in Hebrew (he)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_he
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_he
    target:
      api_endpoint: {}
- name: mmlu_hi
  description: Global-MMLU 0-shot CoT in Hindi (hi)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_hi
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_hi
    target:
      api_endpoint: {}
- name: mmlu_id
  description: Global-MMLU 0-shot CoT in Indonesian (id)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_id
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_id
    target:
      api_endpoint: {}
- name: mmlu_ig
  description: Global-MMLU 0-shot CoT in Igbo (ig)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ig
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ig
    target:
      api_endpoint: {}
- name: mmlu_it
  description: Global-MMLU 0-shot CoT in Italian (it)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_it
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_it
    target:
      api_endpoint: {}
- name: mmlu_ja
  description: Global-MMLU 0-shot CoT in Japanese (ja)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ja
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ja
    target:
      api_endpoint: {}
- name: mmlu_ko
  description: Global-MMLU 0-shot CoT in Korean (ko)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ko
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ko
    target:
      api_endpoint: {}
- name: mmlu_ky
  description: Global-MMLU 0-shot CoT in Kyrgyz (ky)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ky
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ky
    target:
      api_endpoint: {}
- name: mmlu_lt
  description: Global-MMLU 0-shot CoT in Lithuanian (lt)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_lt
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_lt
    target:
      api_endpoint: {}
- name: mmlu_mg
  description: Global-MMLU 0-shot CoT in Malagasy (mg)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_mg
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_mg
    target:
      api_endpoint: {}
- name: mmlu_ms
  description: Global-MMLU 0-shot CoT in Malay (ms)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ms
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ms
    target:
      api_endpoint: {}
- name: mmlu_ne
  description: Global-MMLU 0-shot CoT in Nepali (ne)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ne
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ne
    target:
      api_endpoint: {}
- name: mmlu_nl
  description: Global-MMLU 0-shot CoT in Dutch (nl)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_nl
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_nl
    target:
      api_endpoint: {}
- name: mmlu_ny
  description: Global-MMLU 0-shot CoT in Nyanja (ny)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ny
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ny
    target:
      api_endpoint: {}
- name: mmlu_pl
  description: Global-MMLU 0-shot CoT in Polish (pl)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_pl
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pl
    target:
      api_endpoint: {}
- name: mmlu_pt
  description: Global-MMLU 0-shot CoT in Portuguese (pt)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_pt
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pt
    target:
      api_endpoint: {}
- name: mmlu_ro
  description: Global-MMLU 0-shot CoT in Romanian (ro)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ro
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ro
    target:
      api_endpoint: {}
- name: mmlu_ru
  description: Global-MMLU 0-shot CoT in Russian (ru)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ru
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ru
    target:
      api_endpoint: {}
- name: mmlu_si
  description: Global-MMLU 0-shot CoT in Sinhala (si)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_si
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_si
    target:
      api_endpoint: {}
- name: mmlu_sn
  description: Global-MMLU 0-shot CoT in Shona (sn)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_sn
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_sn
    target:
      api_endpoint: {}
- name: mmlu_so
  description: Global-MMLU 0-shot CoT in Somali (so)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_so
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_so
    target:
      api_endpoint: {}
- name: mmlu_sr
  description: Global-MMLU 0-shot CoT in Serbian (sr)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_sr
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_sr
    target:
      api_endpoint: {}
- name: mmlu_sv
  description: Global-MMLU 0-shot CoT in Swedish (sv)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_sv
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_sv
    target:
      api_endpoint: {}
- name: mmlu_sw
  description: Global-MMLU 0-shot CoT in Swahili (sw)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_sw
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_sw
    target:
      api_endpoint: {}
- name: mmlu_te
  description: Global-MMLU 0-shot CoT in Telugu (te)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_te
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_te
    target:
      api_endpoint: {}
- name: mmlu_tr
  description: Global-MMLU 0-shot CoT in Turkish (tr)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_tr
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_tr
    target:
      api_endpoint: {}
- name: mmlu_uk
  description: Global-MMLU 0-shot CoT in Ukrainian (uk)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_uk
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_uk
    target:
      api_endpoint: {}
- name: mmlu_vi
  description: Global-MMLU 0-shot CoT in Vietnamese (vi)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_vi
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_vi
    target:
      api_endpoint: {}
- name: mmlu_yo
  description: Global-MMLU 0-shot CoT in Yoruba (yo)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_yo
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_yo
    target:
      api_endpoint: {}
- name: mmlu_ar-lite
  description: Global-MMLU-Lite 0-shot CoT in Arabic (ar)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ar-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ar-lite
    target:
      api_endpoint: {}
- name: mmlu_bn-lite
  description: Global-MMLU-Lite 0-shot CoT in Bengali (bn)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_bn-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_bn-lite
    target:
      api_endpoint: {}
- name: mmlu_de-lite
  description: Global-MMLU-Lite 0-shot CoT in German (de)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_de-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_de-lite
    target:
      api_endpoint: {}
- name: mmlu_en-lite
  description: Global-MMLU-Lite 0-shot CoT in English (en)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_en-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_en-lite
    target:
      api_endpoint: {}
- name: mmlu_es-lite
  description: Global-MMLU-Lite 0-shot CoT in Spanish (es)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_es-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_es-lite
    target:
      api_endpoint: {}
- name: mmlu_fr-lite
  description: Global-MMLU-Lite 0-shot CoT in French (fr)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_fr-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_fr-lite
    target:
      api_endpoint: {}
- name: mmlu_hi-lite
  description: Global-MMLU-Lite 0-shot CoT in Hindi (hi)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_hi-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_hi-lite
    target:
      api_endpoint: {}
- name: mmlu_id-lite
  description: Global-MMLU-Lite 0-shot CoT in Indonesian (id)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_id-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_id-lite
    target:
      api_endpoint: {}
- name: mmlu_it-lite
  description: Global-MMLU-Lite 0-shot CoT in Italian (it)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_it-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_it-lite
    target:
      api_endpoint: {}
- name: mmlu_ja-lite
  description: Global-MMLU-Lite 0-shot CoT in Japanese (ja)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ja-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ja-lite
    target:
      api_endpoint: {}
- name: mmlu_ko-lite
  description: Global-MMLU-Lite 0-shot CoT in Korean (ko)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_ko-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ko-lite
    target:
      api_endpoint: {}
- name: mmlu_my-lite
  description: Global-MMLU-Lite 0-shot CoT in Malay (my)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_my-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_my-lite
    target:
      api_endpoint: {}
- name: mmlu_pt-lite
  description: Global-MMLU-Lite 0-shot CoT in Portuguese (pt)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_pt-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pt-lite
    target:
      api_endpoint: {}
- name: mmlu_sw-lite
  description: Global-MMLU-Lite 0-shot CoT in Swahili (sw)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_sw-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_sw-lite
    target:
      api_endpoint: {}
- name: mmlu_yo-lite
  description: Global-MMLU-Lite 0-shot CoT in Yoruba (yo)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_yo-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_yo-lite
    target:
      api_endpoint: {}
- name: mmlu_zh-lite
  description: Global-MMLU-Lite 0-shot CoT in Chinese (Simplified) (zh)
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_zh-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_zh-lite
    target:
      api_endpoint: {}
- name: mmlu
  description: MMLU 0-shot CoT
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu
    target:
      api_endpoint: {}
- name: gpqa_diamond
  description: gpqa_diamond 0-shot CoT
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: gpqa_diamond
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: gpqa_diamond
    target:
      api_endpoint: {}
- name: gpqa_extended
  description: gpqa_extended 0-shot CoT
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: gpqa_extended
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: gpqa_extended
    target:
      api_endpoint: {}
- name: gpqa_main
  description: gpqa_main 0-shot CoT
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: gpqa_main
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: gpqa_main
    target:
      api_endpoint: {}
- name: simpleqa
  description: A factuality benchmark called SimpleQA that measures the ability for
    language models to answer short, fact-seeking questions.
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: simpleqa
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: simpleqa
    target:
      api_endpoint: {}
- name: aime_2025_nemo
  description: AIME 2025 questions, math, using NeMo's alignment template
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: aime_2025_nemo
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: aime_2025_nemo
    target:
      api_endpoint: {}
- name: aime_2024_nemo
  description: AIME 2024 questions, math, using NeMo's alignment template
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: aime_2024_nemo
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: aime_2024_nemo
    target:
      api_endpoint: {}
- name: math_test_500_nemo
  description: math_test_500 questions, math, using NeMo's alignment template
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: math_test_500_nemo
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 3
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: math_test_500_nemo
    target:
      api_endpoint: {}
- name: gpqa_diamond_nemo
  description: gpqa_diamond questions, reasoning, using NeMo's alignment template
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: gpqa_diamond_nemo
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 5
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: gpqa_diamond_nemo
    target:
      api_endpoint: {}
- name: gpqa_diamond_aa_v2_llama_4
  description: gpqa_diamond questions with custom regex extraction patterns for Llama
    4
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: gpqa_diamond
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 5
          downsampling_ratio: null
          add_system_prompt: false
          custom_config:
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_colon_llama4
            - regex: (?i)(?:the )?best? answer is\s*[\*\_,{}\.]*([A-D])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_is_llama4
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: gpqa_diamond_aa_v2_llama_4
    target:
      api_endpoint: {}
- name: gpqa_diamond_aa_v2
  description: gpqa_diamond questions with custom regex extraction patterns for AA
    v2
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: gpqa_diamond
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 5
          downsampling_ratio: null
          add_system_prompt: false
          custom_config:
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: aa_v2_regex
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: gpqa_diamond_aa_v2
    target:
      api_endpoint: {}
- name: AIME_2025_aa_v2
  description: AIME 2025 questions, math - params aligned with Artificial Analysis
    Index v2
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: AIME_2025
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: AIME_2025_aa_v2
    target:
      api_endpoint: {}
- name: mgsm_aa_v2
  description: MGSM is a benchmark of grade-school math problems - params aligned
    with Artificial Analysis Index v2
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: mgsm
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mgsm_aa_v2
    target:
      api_endpoint: {}
- name: mmlu_pro_aa_v2
  description: MMLU-Pro - params aligned with Artificial Analysis Index v2
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: mmlu_pro
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pro_aa_v2
    target:
      api_endpoint: {}
- name: mmlu_llama_4
  description: MMLU questions with custom regex extraction patterns for Llama 4
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config:
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_colon_llama4
            - regex: (?i)(?:the )?best? answer is\s*[\*\_,{}\.]*([A-D])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_is_llama4
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_llama_4
    target:
      api_endpoint: {}
- name: mmlu_pro_llama_4
  description: MMLU-Pro questions with custom regex extraction patterns for Llama
    4
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: mmlu_pro
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config:
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_colon_llama4
            - regex: (?i)(?:the )?best? answer is\s*[\*\_,{}\.]*([A-D])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_is_llama4
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pro_llama_4
    target:
      api_endpoint: {}
- name: healthbench
  description: HealthBench is an open-source benchmark measuring the performance and
    safety of large language models in healthcare.
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: healthbench
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: healthbench
    target:
      api_endpoint: {}
- name: healthbench_consensus
  description: HealthBench is an open-source benchmark measuring the performance and
    safety of large language models in healthcare. The consensus subset measures 34
    particularly important aspects of model behavior and has been validated by the
    consensus of multiple physicians.
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: healthbench_consensus
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: healthbench_consensus
    target:
      api_endpoint: {}
- name: healthbench_hard
  description: HealthBench is an open-source benchmark measuring the performance and
    safety of large language models in healthcare. The hard subset consists of 1000
    examples chosen because they are difficult for current frontier models.
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: healthbench_hard
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: healthbench_hard
    target:
      api_endpoint: {}
- name: browsecomp
  description: BrowseComp is a benchmark for measuring the ability for agents to browse
    the web.
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 5
        parallelism: 10
        task: browsecomp
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: browsecomp
    target:
      api_endpoint: {}
- name: gpqa_diamond_aa_v3
  description: GPQA Diamond with AA v3 methodology - multi-stage regex extraction
    for robust answer parsing
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: gpqa_diamond
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 5
          downsampling_ratio: null
          add_system_prompt: false
          custom_config:
            prompt_template: 'Answer the following multiple choice question. The last
              line of your response should be in the following format: ''Answer: A/B/C/D''
              (e.g. ''Answer: A'').


              {Question}


              A) {A}

              B) {B}

              C) {C}

              D) {D}

              '
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: primary_answer_format
            - regex: \\boxed\{[^}]*([A-Z])[^}]*\}
              match_group: 1
              name: latex_boxed
            - regex: answer is ([a-zA-Z])
              match_group: 1
              name: natural_language
            - regex: answer is \(([a-zA-Z])\)
              match_group: 1
              name: with_parenthesis
            - regex: ([A-Z])\)\s*[^A-Z]*
              match_group: 1
              name: choice_format
            - regex: ([A-Z])\s+is\s+the\s+correct\s+answer
              match_group: 1
              name: explicit_statement
            - regex: ([A-Z])\s*$
              match_group: 1
              name: standalone_letter_end
            - regex: ([A-Z])\s*\.
              match_group: 1
              name: letter_with_period
            - regex: ([A-Z])\s*[^\w]
              match_group: 1
              name: letter_nonword
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: gpqa_diamond_aa_v3
    target:
      api_endpoint: {}
- name: mmlu_pro_aa_v3
  description: MMLU-Pro with AA v3 methodology - multi-stage regex extraction with
    A-J options
  harness: simple_evals
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} echo ''{{config.params.extra.custom_config | tojson}}'' > {{config.output_dir}}/temp_config.json
      && python3 -c ''import yaml, json; config_data = json.load(open("{{config.output_dir}}/temp_config.json"));
      yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
      default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: mmlu_pro
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 1
          downsampling_ratio: null
          add_system_prompt: false
          custom_config:
            prompt_template: 'Answer the following multiple choice question. The last
              line of your response should be in the following format: ''Answer: A/B/C/D/E/F/G/H/I/J''
              (e.g. ''Answer: A'').


              {Question}


              A) {A}

              B) {B}

              C) {C}

              D) {D}

              E) {E}

              F) {F}

              G) {G}

              H) {H}

              I) {I}

              J) {J}

              '
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: primary_answer_format
            - regex: \\boxed\{[^}]*([A-Z])[^}]*\}
              match_group: 1
              name: latex_boxed
            - regex: answer is ([a-zA-Z])
              match_group: 1
              name: natural_language
            - regex: answer is \(([a-zA-Z])\)
              match_group: 1
              name: with_parenthesis
            - regex: ([A-Z])\)\s*[^A-Z]*
              match_group: 1
              name: choice_format
            - regex: ([A-Z])\s+is\s+the\s+correct\s+answer
              match_group: 1
              name: explicit_statement
            - regex: ([A-Z])\s*$
              match_group: 1
              name: standalone_letter_end
            - regex: ([A-Z])\s*\.
              match_group: 1
              name: letter_with_period
            - regex: ([A-Z])\s*[^\w]
              match_group: 1
              name: letter_nonword
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pro_aa_v3
    target:
      api_endpoint: {}
- name: humaneval
  description: HumanEval is used to measure functional correctness for synthesizing
    programs from docstrings. It consists of 164 original programming problems, assessing
    language comprehension, algorithms, and simple mathematics, with some comparable
    to simple software interview questions.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: humaneval
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 20
      supported_endpoint_types:
      - completions
      type: humaneval
    target:
      api_endpoint: {}
- name: humaneval_instruct
  description: InstructHumanEval is a modified version of OpenAI HumanEval. For a
    given prompt, we extracted its signature, its docstring as well as its header
    to create a flexing setting which would allow to evaluation instruction-tuned
    LLM. The delimiters used in the instruction-tuning procedure can be use to build
    and instruction that would allow the model to elicit its best capabilities.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: instruct-humaneval-nocontext-py
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 20
      supported_endpoint_types:
      - chat
      type: humaneval_instruct
    target:
      api_endpoint: {}
- name: humanevalplus
  description: HumanEvalPlus is a modified version of HumanEval containing 80x more
    test cases.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: humanevalplus
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: humanevalplus
    target:
      api_endpoint: {}
- name: mbpp-chat
  description: MBPP consists of Python programming problems, designed to be solvable
    by entry level programmers, covering programming fundamentals, standard library
    functionality, and so on. Each problem consists of a task description, code solution
    and 3 automated test cases. This variant uses the chat endpoint.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 10
        task: mbpp
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 10
      supported_endpoint_types:
      - chat
      type: mbpp-chat
    target:
      api_endpoint: {}
- name: mbpp-completions
  description: MBPP consists of Python programming problems, designed to be solvable
    by entry level programmers, covering programming fundamentals, standard library
    functionality, and so on. Each problem consists of a task description, code solution
    and 3 automated test cases. This variant uses the completions endpoint.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 10
        task: mbpp
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 10
      supported_endpoint_types:
      - completions
      type: mbpp-completions
    target:
      api_endpoint: {}
- name: mbppplus-chat
  description: MBPP+ is a modified version of MBPP containing 35x more test cases.
    This variant uses the chat endpoint.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 10
        task: mbppplus
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - chat
      type: mbppplus-chat
    target:
      api_endpoint: {}
- name: mbppplus-completions
  description: MBPP+ is a modified version of MBPP containing 35x more test cases.
    This variant uses the completions endpoint.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 10
        task: mbppplus
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: mbppplus-completions
    target:
      api_endpoint: {}
- name: mbppplus_nemo
  description: MBPP+NeMo is a modified version of MBPP+ that uses the NeMo alignment
    prompt template.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 10
        task: mbppplus_nemo
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - chat
      type: mbppplus_nemo
    target:
      api_endpoint: {}
- name: multiple-py
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "py" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-py
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-py
    target:
      api_endpoint: {}
- name: multiple-sh
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "sh" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-sh
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-sh
    target:
      api_endpoint: {}
- name: multiple-cpp
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "cpp" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-cpp
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-cpp
    target:
      api_endpoint: {}
- name: multiple-cs
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "cs" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-cs
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-cs
    target:
      api_endpoint: {}
- name: multiple-d
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "d" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-d
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-d
    target:
      api_endpoint: {}
- name: multiple-go
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "go" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-go
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-go
    target:
      api_endpoint: {}
- name: multiple-java
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "java" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-java
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-java
    target:
      api_endpoint: {}
- name: multiple-js
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "js" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-js
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-js
    target:
      api_endpoint: {}
- name: multiple-jl
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "jl" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-jl
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-jl
    target:
      api_endpoint: {}
- name: multiple-lua
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "lua" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-lua
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-lua
    target:
      api_endpoint: {}
- name: multiple-pl
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "pl" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-pl
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-pl
    target:
      api_endpoint: {}
- name: multiple-php
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "php" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-php
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-php
    target:
      api_endpoint: {}
- name: multiple-r
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "r" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-r
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-r
    target:
      api_endpoint: {}
- name: multiple-rkt
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "rkt" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-rkt
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-rkt
    target:
      api_endpoint: {}
- name: multiple-rb
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "rb" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-rb
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-rb
    target:
      api_endpoint: {}
- name: multiple-rs
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "rs" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-rs
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-rs
    target:
      api_endpoint: {}
- name: multiple-scala
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "scala" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-scala
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-scala
    target:
      api_endpoint: {}
- name: multiple-swift
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "swift" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-swift
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-swift
    target:
      api_endpoint: {}
- name: multiple-ts
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "ts" subset.
  harness: bigcode-evaluation-harness
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key_name}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-ts
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-ts
    target:
      api_endpoint: {}
- name: codegeneration_release_latest
  description: Code generation latest version
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_latest
      supported_endpoint_types:
      - chat
      type: codegeneration_release_latest
    target:
      api_endpoint: {}
- name: codegeneration_release_v1
  description: The initial release of the dataset (v1) with problems released between
    May 2023 and Mar 2024 containing 400 problems.
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v1
      supported_endpoint_types:
      - chat
      type: codegeneration_release_v1
    target:
      api_endpoint: {}
- name: codegeneration_release_v2
  description: The updated release of the dataset (v2) with problems released between
    May 2023 and May 2024 containing 511 problems.
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v2
      supported_endpoint_types:
      - chat
      type: codegeneration_release_v2
    target:
      api_endpoint: {}
- name: codegeneration_release_v3
  description: The updated release of the dataset (v3) with problems released between
    May 2023 and Jul 2024 containing 612 problems.
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v3
      supported_endpoint_types:
      - chat
      type: codegeneration_release_v3
    target:
      api_endpoint: {}
- name: codegeneration_release_v4
  description: The updated release of the dataset (v4) with problems released between
    May 2023 and Sep 2024 containing 713 problems.
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v4
      supported_endpoint_types:
      - chat
      type: codegeneration_release_v4
    target:
      api_endpoint: {}
- name: codegeneration_release_v5
  description: The updated release of the dataset (v5) with problems released between
    May 2023 and Jan 2025 containing 880 problems.
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v5
      supported_endpoint_types:
      - chat
      type: codegeneration_release_v5
    target:
      api_endpoint: {}
- name: codegeneration_release_v6
  description: The updated release of the dataset (v6) with problems released between
    May 2023 and Apr 2025 containing 1055 problems.
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v6
      supported_endpoint_types:
      - chat
      type: codegeneration_release_v6
    target:
      api_endpoint: {}
- name: codegeneration_notfast
  description: Not fast version of code generation (v2).
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          args: --not_fast
      supported_endpoint_types:
      - chat
      type: codegeneration_notfast
    target:
      api_endpoint: {}
- name: testoutputprediction
  description: Solve the natural language task on a specified input, evaluating the
    ability to generate testing outputs. The model is given the natural language problem
    description and an input, and the output should be the output for the problem.
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: testoutputprediction
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_latest
      supported_endpoint_types:
      - chat
      type: testoutputprediction
    target:
      api_endpoint: {}
- name: codeexecution_v2
  description: "\u201CExecute\u201D a program on an input, evaluating code comprehension\
    \ ability. The model is given a program and an input, and the output should be\
    \ the result."
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codeexecution
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v2
      supported_endpoint_types:
      - chat
      type: codeexecution_v2
    target:
      api_endpoint: {}
- name: codeexecution_v2_cot
  description: "\u201CCoT. Execute\u201D a program on an input, evaluating code comprehension\
    \ ability. The model is given a program and an input, and the output should be\
    \ the result. Chain-of-Thought version of the task."
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codeexecution
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: true
          release_version: release_v2
      supported_endpoint_types:
      - chat
      type: codeexecution_v2_cot
    target:
      api_endpoint: {}
- name: livecodebench_0724_0125
  description: '- Code generation evaluating code comprehension ability. The model
    is given a program and an input, and the output should be the result. - The data
    period and sampling parameters used by Artificial Analaysis (https://artificialanalysis.ai/methodology/intelligence-benchmarking)'
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 3
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: 2024-07-01
          end_date: 2025-01-01
          cot_code_execution: false
          release_version: release_v5
      supported_endpoint_types:
      - chat
      type: livecodebench_0724_0125
    target:
      api_endpoint: {}
- name: livecodebench_aa_v2
  description: '- Code generation evaluating code comprehension ability. The model
    is given a program and an input, and the output should be the result. - The data
    period and sampling parameters used by Artificial Analaysis (https://artificialanalysis.ai/methodology/intelligence-benchmarking)'
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 3
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: 2024-07-01
          end_date: 2025-01-01
          cot_code_execution: false
          release_version: release_v5
      supported_endpoint_types:
      - chat
      type: livecodebench_aa_v2
    target:
      api_endpoint: {}
- name: livecodebench_0824_0225
  description:
  - Code generation evaluating code comprehension ability. The model is given a program
    and an input, and the output should be the result.
  - The data period and sampling parameters used by NeMo Alignment team.
  harness: livecodebench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 3
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: 2024-08-01
          end_date: 2025-02-01
          cot_code_execution: false
          release_version: release_v5
      supported_endpoint_types:
      - chat
      type: livecodebench_0824_0225
    target:
      api_endpoint: {}
- name: scicode
  description: '- SciCode is a challenging benchmark designed to evaluate the capabilities
    of LLMs in generating code for solving realistic scientific research problems.
    - Includes default system prompt ("You are a helpful assistant.").'
  harness: scicode
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} scicode_eval --model {{target.api_endpoint.model_id}} --url {{target.api_endpoint.url}}
      --output-dir {{config.output_dir}}/scicode_results --log-dir {{config.output_dir}}/logs
      {% if config.params.temperature is not none %}--temperature={{config.params.temperature}}{%
      endif %} {% if config.params.limit_samples is not none %}--limit-samples={{config.params.limit_samples}}{%
      endif %} --n-samples={{config.params.extra.n_samples}} --extra-params top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},max_tokens={{config.params.max_new_tokens}},max_retries={{config.params.max_retries}},include_system_prompt={{config.params.extra.include_system_prompt}}
      {% if config.params.extra.with_background %}--with-background {% endif %} {%
      if config.params.extra.include_dev %}--include-dev{% endif %} {% if config.params.extra.eval_threads
      is not none %}--eval-threads={{config.params.extra.eval_threads}}{% endif %}
      {% if config.params.extra.regex_path is not none %}--regex-path={{config.params.extra.regex_path}}{%
      endif %} {% if config.params.extra.prompt_template_type is not none %}--prompt-template-type={{config.params.extra.prompt_template_type}}{%
      endif %} --concurrent-requests={{config.params.parallelism}}'
    framework_name: scicode
    pkg_name: scicode
    config:
      params:
        max_new_tokens: 2048
        max_retries: 2
        parallelism: 1
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          with_background: false
          include_dev: false
          n_samples: 1
          eval_threads: null
          include_system_prompt: true
          regex_path: null
          prompt_template_type: null
      supported_endpoint_types:
      - chat
      type: scicode
    target:
      api_endpoint:
        stream: false
- name: scicode_background
  description: '- SciCode is a challenging benchmark designed to evaluate the capabilities
    of LLMs in generating code for solving realistic scientific research problems.
    - This variant includes scientist-annotated background in the prompts. - Includes
    default system prompt ("You are a helpful assistant.").'
  harness: scicode
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} scicode_eval --model {{target.api_endpoint.model_id}} --url {{target.api_endpoint.url}}
      --output-dir {{config.output_dir}}/scicode_results --log-dir {{config.output_dir}}/logs
      {% if config.params.temperature is not none %}--temperature={{config.params.temperature}}{%
      endif %} {% if config.params.limit_samples is not none %}--limit-samples={{config.params.limit_samples}}{%
      endif %} --n-samples={{config.params.extra.n_samples}} --extra-params top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},max_tokens={{config.params.max_new_tokens}},max_retries={{config.params.max_retries}},include_system_prompt={{config.params.extra.include_system_prompt}}
      {% if config.params.extra.with_background %}--with-background {% endif %} {%
      if config.params.extra.include_dev %}--include-dev{% endif %} {% if config.params.extra.eval_threads
      is not none %}--eval-threads={{config.params.extra.eval_threads}}{% endif %}
      {% if config.params.extra.regex_path is not none %}--regex-path={{config.params.extra.regex_path}}{%
      endif %} {% if config.params.extra.prompt_template_type is not none %}--prompt-template-type={{config.params.extra.prompt_template_type}}{%
      endif %} --concurrent-requests={{config.params.parallelism}}'
    framework_name: scicode
    pkg_name: scicode
    config:
      params:
        max_new_tokens: 2048
        max_retries: 2
        parallelism: 1
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          with_background: true
          include_dev: false
          n_samples: 1
          eval_threads: null
          include_system_prompt: true
          regex_path: null
          prompt_template_type: null
      supported_endpoint_types:
      - chat
      type: scicode_background
    target:
      api_endpoint:
        stream: false
- name: scicode_aa_v2
  description: '- SciCode is a challenging benchmark designed to evaluate the capabilities
    of LLMs in generating code for solving realistic scientific research problems.
    - This variant mimicks setup used by Artificial Analysis in their Intelligence
    Benchmark (v2). - It includes scientist-annotated background in the prompts and
    uses all available problems for evaluation (including "dev" set). - Does not include
    a default system prompt ("You are a helpful assistant.").'
  harness: scicode
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} scicode_eval --model {{target.api_endpoint.model_id}} --url {{target.api_endpoint.url}}
      --output-dir {{config.output_dir}}/scicode_results --log-dir {{config.output_dir}}/logs
      {% if config.params.temperature is not none %}--temperature={{config.params.temperature}}{%
      endif %} {% if config.params.limit_samples is not none %}--limit-samples={{config.params.limit_samples}}{%
      endif %} --n-samples={{config.params.extra.n_samples}} --extra-params top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},max_tokens={{config.params.max_new_tokens}},max_retries={{config.params.max_retries}},include_system_prompt={{config.params.extra.include_system_prompt}}
      {% if config.params.extra.with_background %}--with-background {% endif %} {%
      if config.params.extra.include_dev %}--include-dev{% endif %} {% if config.params.extra.eval_threads
      is not none %}--eval-threads={{config.params.extra.eval_threads}}{% endif %}
      {% if config.params.extra.regex_path is not none %}--regex-path={{config.params.extra.regex_path}}{%
      endif %} {% if config.params.extra.prompt_template_type is not none %}--prompt-template-type={{config.params.extra.prompt_template_type}}{%
      endif %} --concurrent-requests={{config.params.parallelism}}'
    framework_name: scicode
    pkg_name: scicode
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 1
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          with_background: true
          include_dev: true
          n_samples: 3
          eval_threads: null
          include_system_prompt: false
          regex_path: aa_regex.txt
          prompt_template_type: background_comment_template.txt
      supported_endpoint_types:
      - chat
      type: scicode_aa_v2
    target:
      api_endpoint:
        stream: false
- name: hle
  description: Text-only questions from Humanity's Last Exam
  harness: hle
  defaults:
    command: hle_eval --dataset=cais/hle --model_name={{target.api_endpoint.model_id}}
      --model_url={{target.api_endpoint.url}}  --temperature={{config.params.temperature}}
      --top_p={{config.params.top_p}} --timeout={{config.params.request_timeout}}  {%
      if config.params.limit_samples is not none %}--limit {{config.params.limit_samples}}{%
      endif %} --output_dir={{config.output_dir}}  {% if target.api_endpoint.api_key_name
      is not none %}--api_key_name={{target.api_endpoint.api_key_name}}{% endif %}
      --max_retries={{config.params.max_retries}} --num_workers={{config.params.parallelism}}  --max_new_tokens={{config.params.max_new_tokens}}
      --text_only --generate --judge
    framework_name: hle
    pkg_name: hle
    config:
      params:
        max_new_tokens: 8192
        max_retries: 30
        parallelism: 10
        temperature: 0.0
        request_timeout: 600
        top_p: 1.0
        extra: {}
      supported_endpoint_types:
      - chat
      type: hle
    target:
      api_endpoint: {}
- name: hle_aa_v2
  description: Text-only questions from Humanity's Last Exam and params aligned with
    Artificial Analysis Index v2
  harness: hle
  defaults:
    command: hle_eval --dataset=cais/hle --model_name={{target.api_endpoint.model_id}}
      --model_url={{target.api_endpoint.url}}  --temperature={{config.params.temperature}}
      --top_p={{config.params.top_p}} --timeout={{config.params.request_timeout}}  {%
      if config.params.limit_samples is not none %}--limit {{config.params.limit_samples}}{%
      endif %} --output_dir={{config.output_dir}}  {% if target.api_endpoint.api_key_name
      is not none %}--api_key_name={{target.api_endpoint.api_key_name}}{% endif %}
      --max_retries={{config.params.max_retries}} --num_workers={{config.params.parallelism}}  --max_new_tokens={{config.params.max_new_tokens}}
      --text_only --generate --judge
    framework_name: hle
    pkg_name: hle
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        temperature: 0.0
        request_timeout: 600
        top_p: 1.0
        extra: {}
      supported_endpoint_types:
      - chat
      type: hle_aa_v2
    target:
      api_endpoint: {}
- name: bfclv3
  description: BFCL v3 with Single-turn and Multi-turn, Live and Non-Live, AST and
    Exec evaluation. Not using native function calling.
  harness: bfcl
  defaults:
    command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
      \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
      \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n\
      \  --dataset_path {{config.params.extra.custom_dataset.path}} \\\n  --test_category\
      \ {{config.params.task}} \\\n  --processing_output_dir {{config.output_dir ~\
      \ \"/custom_dataset_processing\"}} \\\n  {% if config.params.extra.custom_dataset.data_template_path\
      \ %}--data_template_path {{config.params.extra.custom_dataset.data_template_path}}{%\
      \ endif %}) && \\\necho \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n\
      {% endif -%}\n{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%\
      \ endif %} bfcl generate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
      \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
      \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key_name\
      \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{% endif\
      \ %} bfcl evaluate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --score-dir {{config.output_dir}} --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
    framework_name: bfcl
    pkg_name: bfcl
    config:
      params:
        parallelism: 10
        task: all
        extra:
          native_calling: false
          custom_dataset:
            path: null
            format: null
            data_template_path: null
      supported_endpoint_types:
      - chat
      - vlm
      type: bfclv3
    target:
      api_endpoint: {}
- name: bfclv3_ast
  description: BFCL v3 with Single-turn and Multi-turn, Live and Non-Live, AST evaluation.
    Uses native function calling.
  harness: bfcl
  defaults:
    command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
      \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
      \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n\
      \  --dataset_path {{config.params.extra.custom_dataset.path}} \\\n  --test_category\
      \ {{config.params.task}} \\\n  --processing_output_dir {{config.output_dir ~\
      \ \"/custom_dataset_processing\"}} \\\n  {% if config.params.extra.custom_dataset.data_template_path\
      \ %}--data_template_path {{config.params.extra.custom_dataset.data_template_path}}{%\
      \ endif %}) && \\\necho \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n\
      {% endif -%}\n{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%\
      \ endif %} bfcl generate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
      \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
      \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key_name\
      \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{% endif\
      \ %} bfcl evaluate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --score-dir {{config.output_dir}} --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
    framework_name: bfcl
    pkg_name: bfcl
    config:
      params:
        parallelism: 10
        task: multi_turn,ast
        extra:
          native_calling: true
          custom_dataset:
            path: null
            format: null
            data_template_path: null
      supported_endpoint_types:
      - chat
      - vlm
      type: bfclv3_ast
    target:
      api_endpoint: {}
- name: bfclv3_ast_prompting
  description: BFCL v3 with Single-turn and Multi-turn, Live and Non-Live, AST evaluation.
    Not using native function calling.
  harness: bfcl
  defaults:
    command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
      \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
      \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n\
      \  --dataset_path {{config.params.extra.custom_dataset.path}} \\\n  --test_category\
      \ {{config.params.task}} \\\n  --processing_output_dir {{config.output_dir ~\
      \ \"/custom_dataset_processing\"}} \\\n  {% if config.params.extra.custom_dataset.data_template_path\
      \ %}--data_template_path {{config.params.extra.custom_dataset.data_template_path}}{%\
      \ endif %}) && \\\necho \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n\
      {% endif -%}\n{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%\
      \ endif %} bfcl generate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
      \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
      \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key_name\
      \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{% endif\
      \ %} bfcl evaluate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --score-dir {{config.output_dir}} --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
    framework_name: bfcl
    pkg_name: bfcl
    config:
      params:
        parallelism: 10
        task: multi_turn,ast
        extra:
          native_calling: false
          custom_dataset:
            path: null
            format: null
            data_template_path: null
      supported_endpoint_types:
      - chat
      - vlm
      type: bfclv3_ast_prompting
    target:
      api_endpoint: {}
- name: bfclv2
  description: BFCL v2 with Single-turn, Live and Non-Live, AST and Exec evaluation.
    Not using native function calling.
  harness: bfcl
  defaults:
    command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
      \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
      \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n\
      \  --dataset_path {{config.params.extra.custom_dataset.path}} \\\n  --test_category\
      \ {{config.params.task}} \\\n  --processing_output_dir {{config.output_dir ~\
      \ \"/custom_dataset_processing\"}} \\\n  {% if config.params.extra.custom_dataset.data_template_path\
      \ %}--data_template_path {{config.params.extra.custom_dataset.data_template_path}}{%\
      \ endif %}) && \\\necho \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n\
      {% endif -%}\n{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%\
      \ endif %} bfcl generate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
      \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
      \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key_name\
      \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{% endif\
      \ %} bfcl evaluate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --score-dir {{config.output_dir}} --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
    framework_name: bfcl
    pkg_name: bfcl
    config:
      params:
        parallelism: 10
        task: single_turn
        extra:
          native_calling: false
          custom_dataset:
            path: null
            format: null
            data_template_path: null
      supported_endpoint_types:
      - chat
      - vlm
      type: bfclv2
    target:
      api_endpoint: {}
- name: bfclv2_ast
  description: BFCL v2 with Single-turn, Live and Non-Live, AST evaluation only. Uses
    native function calling.
  harness: bfcl
  defaults:
    command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
      \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
      \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n\
      \  --dataset_path {{config.params.extra.custom_dataset.path}} \\\n  --test_category\
      \ {{config.params.task}} \\\n  --processing_output_dir {{config.output_dir ~\
      \ \"/custom_dataset_processing\"}} \\\n  {% if config.params.extra.custom_dataset.data_template_path\
      \ %}--data_template_path {{config.params.extra.custom_dataset.data_template_path}}{%\
      \ endif %}) && \\\necho \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n\
      {% endif -%}\n{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%\
      \ endif %} bfcl generate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
      \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
      \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key_name\
      \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{% endif\
      \ %} bfcl evaluate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --score-dir {{config.output_dir}} --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
    framework_name: bfcl
    pkg_name: bfcl
    config:
      params:
        parallelism: 10
        task: ast
        extra:
          native_calling: true
          custom_dataset:
            path: null
            format: null
            data_template_path: null
      supported_endpoint_types:
      - chat
      - vlm
      type: bfclv2_ast
    target:
      api_endpoint: {}
- name: bfclv2_ast_prompting
  description: BFCL v2 with Single-turn, Live and Non-Live, AST evaluation only. Not
    using native function calling.
  harness: bfcl
  defaults:
    command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
      \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
      \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n\
      \  --dataset_path {{config.params.extra.custom_dataset.path}} \\\n  --test_category\
      \ {{config.params.task}} \\\n  --processing_output_dir {{config.output_dir ~\
      \ \"/custom_dataset_processing\"}} \\\n  {% if config.params.extra.custom_dataset.data_template_path\
      \ %}--data_template_path {{config.params.extra.custom_dataset.data_template_path}}{%\
      \ endif %}) && \\\necho \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n\
      {% endif -%}\n{% if target.api_endpoint.api_key_name is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{%\
      \ endif %} bfcl generate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
      \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
      \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key_name\
      \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}{% endif\
      \ %} bfcl evaluate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --score-dir {{config.output_dir}} --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
    framework_name: bfcl
    pkg_name: bfcl
    config:
      params:
        parallelism: 10
        task: ast
        extra:
          native_calling: false
          custom_dataset:
            path: null
            format: null
            data_template_path: null
      supported_endpoint_types:
      - chat
      - vlm
      type: bfclv2_ast_prompting
    target:
      api_endpoint: {}
- name: report_generation
  description: Generate professional reports and evaluate them (full pipeline)
  harness: profbench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} {% if config.params.extra.run_generation %}\n  python -m\
      \ profbench.run_report_generation \\\n    --model {{target.api_endpoint.model_id}}\
      \ \\\n    --library {{config.params.extra.library}} \\\n    --timeout {{config.params.request_timeout}}\
      \ \\\n    --parallel {{config.params.parallelism}} \\\n    --retry-attempts\
      \ {{config.params.max_retries}} \\\n    --folder {{config.output_dir}}{% if\
      \ target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
      \ endif %}{% if config.params.extra.version is not none %} --version {{config.params.extra.version}}{%\
      \ endif %}{% if config.params.extra.web_search %} --web-search{% endif %}{%\
      \ if config.params.extra.reasoning %} --reasoning{% endif %}{% if config.params.extra.reasoning_effort\
      \ is not none %} --reasoning-effort {{config.params.extra.reasoning_effort}}{%\
      \ endif %}{% if config.params.limit_samples is not none %} --limit-samples {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
      \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
      \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
      \ endif %} && \n  GENERATION_OUTPUT=$(ls -t {{config.output_dir}}/*.jsonl |\
      \ head -1) && \n{% endif %} {% if config.params.extra.run_judge_generated %}\n\
      \  python -m profbench.run_best_llm_judge_on_generated_reports \\\n    --filename\
      \ $GENERATION_OUTPUT \\\n    --api-key $API_KEY \\\n    --model {{target.api_endpoint.model_id}}\
      \ \\\n    --library {{config.params.extra.library}} \\\n    --timeout {{config.params.request_timeout}}\
      \ \\\n    --parallel {{config.params.parallelism}} \\\n    --retry-attempts\
      \ {{config.params.max_retries}} \\\n    --output-folder {{config.output_dir}}/judgements{%\
      \ if target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
      \ endif %}{% if config.params.limit_samples is not none %} --limit-samples {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
      \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
      \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
      \ endif %} && \n  JUDGE_OUTPUT=$(ls -t {{config.output_dir}}/judgements/*.jsonl\
      \ | head -1) && \n  python -m profbench.score_report_generation $JUDGE_OUTPUT\n\
      {% endif %} {% if config.params.extra.run_judge_provided %}\n  python -m profbench.run_llm_judge_on_provided_reports\
      \ \\\n    --model {{target.api_endpoint.model_id}} \\\n    --library {{config.params.extra.library}}\
      \ \\\n    --timeout {{config.params.request_timeout}} \\\n    --parallel {{config.params.parallelism}}\
      \ \\\n    --retry-attempts {{config.params.max_retries}} \\\n    --folder {{config.output_dir}}{%\
      \ if target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
      \ endif %}{% if config.params.extra.reasoning %} --reasoning{% endif %}{% if\
      \ config.params.extra.reasoning_effort is not none %} --reasoning-effort {{config.params.extra.reasoning_effort}}{%\
      \ endif %}{% if config.params.extra.debug %} --debug{% endif %}{% if config.params.limit_samples\
      \ is not none %} --limit-samples {{config.params.limit_samples}}{% endif %}{%\
      \ if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
      \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
      \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
      \ endif %} && \n  JUDGE_OUTPUT=$(ls -t {{config.output_dir}}/*.jsonl | head\
      \ -1) && \n  python -m profbench.score_llm_judge $JUDGE_OUTPUT\n{% endif %}\n"
    framework_name: profbench
    pkg_name: profbench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        temperature: 0.0
        request_timeout: 600
        top_p: 1.0e-05
        extra:
          run_generation: true
          run_judge_generated: true
          run_judge_provided: false
          library: openai
          version: lite
          web_search: false
          reasoning: false
          reasoning_effort: null
          debug: false
      supported_endpoint_types:
      - chat
      type: report_generation
    target:
      api_endpoint: {}
- name: llm_judge
  description: Run LLM judge on provided ProfBench reports and score them
  harness: profbench
  defaults:
    command: "{% if target.api_endpoint.api_key_name is not none %}\n  export API_KEY=${{target.api_endpoint.api_key_name}}\
      \ && \n{% endif %} {% if config.params.extra.run_generation %}\n  python -m\
      \ profbench.run_report_generation \\\n    --model {{target.api_endpoint.model_id}}\
      \ \\\n    --library {{config.params.extra.library}} \\\n    --timeout {{config.params.request_timeout}}\
      \ \\\n    --parallel {{config.params.parallelism}} \\\n    --retry-attempts\
      \ {{config.params.max_retries}} \\\n    --folder {{config.output_dir}}{% if\
      \ target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
      \ endif %}{% if config.params.extra.version is not none %} --version {{config.params.extra.version}}{%\
      \ endif %}{% if config.params.extra.web_search %} --web-search{% endif %}{%\
      \ if config.params.extra.reasoning %} --reasoning{% endif %}{% if config.params.extra.reasoning_effort\
      \ is not none %} --reasoning-effort {{config.params.extra.reasoning_effort}}{%\
      \ endif %}{% if config.params.limit_samples is not none %} --limit-samples {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
      \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
      \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
      \ endif %} && \n  GENERATION_OUTPUT=$(ls -t {{config.output_dir}}/*.jsonl |\
      \ head -1) && \n{% endif %} {% if config.params.extra.run_judge_generated %}\n\
      \  python -m profbench.run_best_llm_judge_on_generated_reports \\\n    --filename\
      \ $GENERATION_OUTPUT \\\n    --api-key $API_KEY \\\n    --model {{target.api_endpoint.model_id}}\
      \ \\\n    --library {{config.params.extra.library}} \\\n    --timeout {{config.params.request_timeout}}\
      \ \\\n    --parallel {{config.params.parallelism}} \\\n    --retry-attempts\
      \ {{config.params.max_retries}} \\\n    --output-folder {{config.output_dir}}/judgements{%\
      \ if target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
      \ endif %}{% if config.params.limit_samples is not none %} --limit-samples {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
      \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
      \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
      \ endif %} && \n  JUDGE_OUTPUT=$(ls -t {{config.output_dir}}/judgements/*.jsonl\
      \ | head -1) && \n  python -m profbench.score_report_generation $JUDGE_OUTPUT\n\
      {% endif %} {% if config.params.extra.run_judge_provided %}\n  python -m profbench.run_llm_judge_on_provided_reports\
      \ \\\n    --model {{target.api_endpoint.model_id}} \\\n    --library {{config.params.extra.library}}\
      \ \\\n    --timeout {{config.params.request_timeout}} \\\n    --parallel {{config.params.parallelism}}\
      \ \\\n    --retry-attempts {{config.params.max_retries}} \\\n    --folder {{config.output_dir}}{%\
      \ if target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
      \ endif %}{% if config.params.extra.reasoning %} --reasoning{% endif %}{% if\
      \ config.params.extra.reasoning_effort is not none %} --reasoning-effort {{config.params.extra.reasoning_effort}}{%\
      \ endif %}{% if config.params.extra.debug %} --debug{% endif %}{% if config.params.limit_samples\
      \ is not none %} --limit-samples {{config.params.limit_samples}}{% endif %}{%\
      \ if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
      \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
      \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
      \ endif %} && \n  JUDGE_OUTPUT=$(ls -t {{config.output_dir}}/*.jsonl | head\
      \ -1) && \n  python -m profbench.score_llm_judge $JUDGE_OUTPUT\n{% endif %}\n"
    framework_name: profbench
    pkg_name: profbench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        temperature: 0.0
        request_timeout: 600
        top_p: 1.0e-05
        extra:
          run_generation: false
          run_judge_generated: false
          run_judge_provided: true
          library: openai
          version: lite
          web_search: false
          reasoning: false
          reasoning_effort: null
          debug: false
      supported_endpoint_types:
      - chat
      type: llm_judge
    target:
      api_endpoint: {}
- name: ai2d_judge
  description: A benchmark for evaluating diagram understanding capabilities of large
    vision-language models.
  harness: vlmevalkit
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key_name}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: AI2D_TEST
            class: ImageMCQDataset
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
      supported_endpoint_types:
      - vlm
      type: ai2d_judge
    target:
      api_endpoint: {}
- name: chartqa
  description: A Benchmark for Question Answering about Charts with Visual and Logical
    Reasoning
  harness: vlmevalkit
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key_name}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: ChartQA_TEST
            class: ImageVQADataset
      supported_endpoint_types:
      - vlm
      type: chartqa
    target:
      api_endpoint: {}
- name: mathvista-mini
  description: Evaluating Math Reasoning in Visual Contexts
  harness: vlmevalkit
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key_name}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: MathVista_MINI
            class: MathVista
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
      supported_endpoint_types:
      - vlm
      type: mathvista-mini
    target:
      api_endpoint: {}
- name: mmmu_judge
  description: A benchmark for evaluating multimodal models on massive multi-discipline
    tasks demanding college-level subject knowledge and deliberate reasoning.
  harness: vlmevalkit
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key_name}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: MMMU_DEV_VAL
            class: MMMUDataset
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
      supported_endpoint_types:
      - vlm
      type: mmmu_judge
    target:
      api_endpoint: {}
- name: ocrbench
  description: Comprehensive evaluation benchmark designed to assess the OCR capabilities
    of Large Multimodal Models
  harness: vlmevalkit
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key_name}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: OCRBench
            class: OCRBench
      supported_endpoint_types:
      - vlm
      type: ocrbench
    target:
      api_endpoint: {}
- name: ocr_reasoning
  description: Comprehensive benchmark of 1,069 human-annotated examples designed
    to evaluate multimodal large language models on text-rich image reasoning tasks
    by assessing both final answers and the reasoning process across six core abilities
    and 18 practical tasks.
  harness: vlmevalkit
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key_name}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: OCR_Reasoning
            class: OCR_Reasoning
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
      supported_endpoint_types:
      - vlm
      type: ocr_reasoning
    target:
      api_endpoint: {}
- name: slidevqa
  description: Evaluates ability to answer questions about slide decks by selecting
    relevant slides from multiple images
  harness: vlmevalkit
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key_name}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: SLIDEVQA
            class: SlideVQA
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
      supported_endpoint_types:
      - vlm
      type: slidevqa
    target:
      api_endpoint: {}
- name: garak
  description: Task for running the default set of Garak probes. This variant uses
    the chat endpoint.
  harness: garak
  defaults:
    command: "cat > garak_config.yaml << 'EOF'\n{% if config.params.extra.seed is\
      \ not none %}run:\n  seed: {{config.params.extra.seed}}{% endif %}\nplugins:\n\
      \  {% if config.params.extra.probes is not none %}probe_spec: {{config.params.extra.probes}}{%\
      \ endif %}\n  extended_detectors: true\n  target_type: {% if target.api_endpoint.type\
      \ == \"completions\" %}nim.NVOpenAICompletion{% elif target.api_endpoint.type\
      \ == \"chat\" %}nim.NVOpenAIChat{% endif %}\n  target_name: {{target.api_endpoint.model_id}}\n\
      \  generators:\n    nim:\n      uri: {{target.api_endpoint.url | replace('/chat/completions',\
      \ '') | replace('/completions', '')}}\n      {% if config.params.temperature\
      \ is not none %}temperature: {{config.params.temperature}}{% endif %}\n    \
      \  {% if config.params.top_p is not none %}top_p: {{config.params.top_p}}{%\
      \ endif %}\n      {% if config.params.max_new_tokens is not none %}max_tokens:\
      \ {{config.params.max_new_tokens}}{% endif %}\n    skip_seq_start: {{config.params.extra.skip_seq_start}}\n\
      \    skip_seq_end: {{config.params.extra.skip_seq_end}}\n    \nsystem:\n  parallel_attempts:\
      \ {{config.params.parallelism}}\n  lite: false\nEOF\n{% if target.api_endpoint.api_key_name\
      \ is not none %}\nexport NIM_API_KEY=${{target.api_endpoint.api_key_name}} &&\n\
      {% else %}\nexport NIM_API_KEY=dummy &&\n{% endif %}\nexport XDG_DATA_HOME={{config.output_dir}}\
      \ &&\ngarak --config garak_config.yaml --report_prefix=results\n"
    framework_name: garak
    pkg_name: garak
    config:
      params:
        max_new_tokens: 150
        parallelism: 32
        temperature: 0.1
        top_p: 0.7
        extra:
          probes: null
          seed: 42
          skip_seq_start: <think>
          skip_seq_end: </think>
      supported_endpoint_types:
      - chat
      type: garak
    target:
      api_endpoint:
        api_key_name: API_KEY
- name: garak-completions
  description: Task for running the default set of Garak probes. This variant uses
    the completions endpoint.
  harness: garak
  defaults:
    command: "cat > garak_config.yaml << 'EOF'\n{% if config.params.extra.seed is\
      \ not none %}run:\n  seed: {{config.params.extra.seed}}{% endif %}\nplugins:\n\
      \  {% if config.params.extra.probes is not none %}probe_spec: {{config.params.extra.probes}}{%\
      \ endif %}\n  extended_detectors: true\n  target_type: {% if target.api_endpoint.type\
      \ == \"completions\" %}nim.NVOpenAICompletion{% elif target.api_endpoint.type\
      \ == \"chat\" %}nim.NVOpenAIChat{% endif %}\n  target_name: {{target.api_endpoint.model_id}}\n\
      \  generators:\n    nim:\n      uri: {{target.api_endpoint.url | replace('/chat/completions',\
      \ '') | replace('/completions', '')}}\n      {% if config.params.temperature\
      \ is not none %}temperature: {{config.params.temperature}}{% endif %}\n    \
      \  {% if config.params.top_p is not none %}top_p: {{config.params.top_p}}{%\
      \ endif %}\n      {% if config.params.max_new_tokens is not none %}max_tokens:\
      \ {{config.params.max_new_tokens}}{% endif %}\n    skip_seq_start: {{config.params.extra.skip_seq_start}}\n\
      \    skip_seq_end: {{config.params.extra.skip_seq_end}}\n    \nsystem:\n  parallel_attempts:\
      \ {{config.params.parallelism}}\n  lite: false\nEOF\n{% if target.api_endpoint.api_key_name\
      \ is not none %}\nexport NIM_API_KEY=${{target.api_endpoint.api_key_name}} &&\n\
      {% else %}\nexport NIM_API_KEY=dummy &&\n{% endif %}\nexport XDG_DATA_HOME={{config.output_dir}}\
      \ &&\ngarak --config garak_config.yaml --report_prefix=results\n"
    framework_name: garak
    pkg_name: garak
    config:
      params:
        max_new_tokens: 150
        parallelism: 32
        temperature: 0.1
        top_p: 0.7
        extra:
          probes: null
          seed: 42
          skip_seq_start: <think>
          skip_seq_end: </think>
      supported_endpoint_types:
      - completions
      type: garak-completions
    target:
      api_endpoint:
        api_key_name: API_KEY
- name: ns_aime2024
  description: AIME2024
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: aime24
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: true
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: math_judge
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_aime2024
    target: {}
- name: ns_aime2025
  description: AIME2025
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: aime25
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: true
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: math_judge
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_aime2025
    target: {}
- name: ns_hmmt_feb2025
  description: HMMT February 2025 (MathArena/hmmt_feb_2025)
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: hmmt_feb25
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: true
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: math_judge
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_hmmt_feb2025
    target: {}
- name: ns_gpqa
  description: GPQA Diamond
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: gpqa
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_gpqa
    target: {}
- name: ns_bfcl_v3
  description: BFCLv3
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: bfcl_v3
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: ++use_client_parsing=False
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_bfcl_v3
    target: {}
- name: ns_bfcl_v4
  description: BFCLv4
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: bfcl_v4
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: ++use_client_parsing=False
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_bfcl_v4
    target: {}
- name: ns_livecodebench
  description: LiveCodeBench v6
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: livecodebench
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: test_v6_2408_2505
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_livecodebench
    target: {}
- name: ns_livecodebench_v5
  description: LiveCodeBench v5
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: livecodebench
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: test_v5_2407_2412
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_livecodebench_v5
    target: {}
- name: ns_livecodebench_aa
  description: LiveCodeBench with AA custom prompt format (315 problems from July
    2024 to Dec 2024, release_v5)
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: livecodebench
        extra:
          use_sandbox: false
          num_repeats: 3
          prompt_config: /nemo_run/code/eval_factory_prompts/livecodebench-aa.yaml
          args: null
          system_message: null
          dataset_split: test_v5_2407_2412
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_livecodebench_aa
    target: {}
- name: ns_hle
  description: HumanityLastExam
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: hle
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_hle
    target: {}
- name: ns_hle_aa
  description: HumanityLastExam aligned with AA
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: hle
        extra:
          use_sandbox: false
          num_repeats: 1
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: true
          judge:
            url: https://inference-api.nvidia.com/v1
            model_id: us/azure/openai/gpt-4.1
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_hle_aa
    target: {}
- name: ns_ruler
  description: RULER - Long Context Understanding
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: ruler.evaluation_128k
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: /workspace/ruler_data
            cluster: local
            setup: evaluation_128k
            max_seq_length: 131072
            tokenizer_path: null
            template_tokens: 50
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - completions
      type: ns_ruler
    target: {}
- name: ns_mmlu
  description: MMLU
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: mmlu
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_mmlu
    target: {}
- name: ns_mmlu_pro
  description: MMLU-PRO
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: mmlu-pro
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_mmlu_pro
    target: {}
- name: ns_scicode
  description: SciCode
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: scicode
        extra:
          use_sandbox: true
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_scicode
    target: {}
- name: ns_aa_lcr
  description: AA-LCR
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: aalcr
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: true
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: 0.0
            top_p: 1.0
            max_new_tokens: 4096
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_aa_lcr
    target: {}
- name: ns_ifbench
  description: IFBench - Instruction Following Benchmark
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: ifbench
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_ifbench
    target: {}
- name: ns_wmt24pp
  description: WMT24++
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: wmt24pp
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_wmt24pp
    target: {}
- name: ns_ifeval
  description: IFEval - Instruction-Following Evaluation for Large Language Models
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: ifeval
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_ifeval
    target: {}
- name: ns_mmlu_prox
  description: MMLU-ProX
  harness: nemo_skills
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} ++server.api_key_env_var={% if target.api_endpoint.api_key_name is
      not none %}{{target.api_endpoint.api_key_name}}{% else %}DUMMY_API_KEY{% endif
      %} {% if config.params.max_new_tokens is not none %}++inference.tokens_to_generate={{config.params.max_new_tokens}}{%
      endif %} {% if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}{% if config.params.extra.num_repeats
      is not none and config.params.extra.num_repeats > 1 %}++max_concurrent_requests={{[(config.params.parallelism
      / config.params.extra.num_repeats) | int, 1] | max}}{% else %}++max_concurrent_requests={{config.params.parallelism
      | int}}{% endif %}{% endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.prompt_config is not none %}++prompt_config={{config.params.extra.prompt_config}}{%
      endif %} {% if config.params.extra.ruler.tokenizer_path is not none %}++tokenizer={{config.params.extra.ruler.tokenizer_path}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={% if config.params.extra.judge.api_key
      is not none %}{{config.params.extra.judge.api_key}}{% else %}DUMMY_API_KEY{%
      endif %} {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %} {%- if config.params.extra.judge.parallelism is not none %} ++max_concurrent_requests={{config.params.extra.judge.parallelism}}{%
      endif %} {%- if config.params.extra.judge.args is not none %} {{config.params.extra.judge.args}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        parallelism: 16
        task: mmlu-prox
        extra:
          use_sandbox: false
          num_repeats: null
          prompt_config: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
            args: null
            parallelism: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      supported_endpoint_types:
      - chat
      type: ns_mmlu_prox
    target: {}
- name: aegis_v2
  description: Aegis V2 without evaluating reasoning traces. This version is used
    by the NeMo Safety Toolkit.
  harness: safety_eval
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} {% if config.params.extra.judge.api_key is not none %}export JUDGE_API_KEY=${{config.params.extra.judge.api_key}}
      && {% endif %} safety-eval  --model-name  {{target.api_endpoint.model_id}} --model-url
      {{target.api_endpoint.url}} --model-type {{target.api_endpoint.type}}  --judge-url  {{config.params.extra.judge.url}}   --results-dir
      {{config.output_dir}}   --eval {{config.params.task}}  --mut-inference-params
      max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},concurrency={{config.params.parallelism}},retries={{config.params.max_retries}}
      --judge-inference-params concurrency={{config.params.extra.judge.parallelism}},retries={{config.params.max_retries}}  {%
      if config.params.extra.dataset is defined and config.params.extra.dataset %}
      --dataset ''{{config.params.extra.dataset}}''{% endif %} {% if config.params.extra.policy
      is defined and config.params.extra.policy %} --policy ''{{config.params.extra.policy}}''{%
      endif %} {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}
      {% endif %} {% if config.params.extra.judge.model_id is not none %} --judge-model-name
      {{config.params.extra.judge.model_id}} {% endif %} {% if config.type == "aegis_v2_reasoning"
      %} {% if config.params.extra.evaluate_reasoning_traces  %} --evaluate-reasoning-traces
      {% endif %} {% endif %}'
    framework_name: safety_eval
    pkg_name: safety_eval
    config:
      params:
        max_new_tokens: 6144
        max_retries: 5
        parallelism: 8
        task: aegis_v2
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          judge:
            url: null
            model_id: null
            api_key: null
            parallelism: 32
            request_timeout: 60
            max_retries: 16
          evaluate_reasoning_traces: false
      supported_endpoint_types:
      - chat
      type: aegis_v2
    target:
      api_endpoint:
        stream: false
- name: aegis_v2_completions
  description: Aegis V2 without evaluating reasoning traces. This variant uses the
    completions endpoint.
  harness: safety_eval
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} {% if config.params.extra.judge.api_key is not none %}export JUDGE_API_KEY=${{config.params.extra.judge.api_key}}
      && {% endif %} safety-eval  --model-name  {{target.api_endpoint.model_id}} --model-url
      {{target.api_endpoint.url}} --model-type {{target.api_endpoint.type}}  --judge-url  {{config.params.extra.judge.url}}   --results-dir
      {{config.output_dir}}   --eval {{config.params.task}}  --mut-inference-params
      max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},concurrency={{config.params.parallelism}},retries={{config.params.max_retries}}
      --judge-inference-params concurrency={{config.params.extra.judge.parallelism}},retries={{config.params.max_retries}}  {%
      if config.params.extra.dataset is defined and config.params.extra.dataset %}
      --dataset ''{{config.params.extra.dataset}}''{% endif %} {% if config.params.extra.policy
      is defined and config.params.extra.policy %} --policy ''{{config.params.extra.policy}}''{%
      endif %} {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}
      {% endif %} {% if config.params.extra.judge.model_id is not none %} --judge-model-name
      {{config.params.extra.judge.model_id}} {% endif %} {% if config.type == "aegis_v2_reasoning"
      %} {% if config.params.extra.evaluate_reasoning_traces  %} --evaluate-reasoning-traces
      {% endif %} {% endif %}'
    framework_name: safety_eval
    pkg_name: safety_eval
    config:
      params:
        max_new_tokens: 6144
        max_retries: 5
        parallelism: 8
        task: aegis_v2
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          judge:
            url: null
            model_id: null
            api_key: null
            parallelism: 32
            request_timeout: 60
            max_retries: 16
          evaluate_reasoning_traces: false
      supported_endpoint_types:
      - completions
      type: aegis_v2_completions
    target:
      api_endpoint:
        stream: false
- name: aegis_v2_reasoning
  description: Aegis V2 with evaluating reasoning traces.
  harness: safety_eval
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} {% if config.params.extra.judge.api_key is not none %}export JUDGE_API_KEY=${{config.params.extra.judge.api_key}}
      && {% endif %} safety-eval  --model-name  {{target.api_endpoint.model_id}} --model-url
      {{target.api_endpoint.url}} --model-type {{target.api_endpoint.type}}  --judge-url  {{config.params.extra.judge.url}}   --results-dir
      {{config.output_dir}}   --eval {{config.params.task}}  --mut-inference-params
      max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},concurrency={{config.params.parallelism}},retries={{config.params.max_retries}}
      --judge-inference-params concurrency={{config.params.extra.judge.parallelism}},retries={{config.params.max_retries}}  {%
      if config.params.extra.dataset is defined and config.params.extra.dataset %}
      --dataset ''{{config.params.extra.dataset}}''{% endif %} {% if config.params.extra.policy
      is defined and config.params.extra.policy %} --policy ''{{config.params.extra.policy}}''{%
      endif %} {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}
      {% endif %} {% if config.params.extra.judge.model_id is not none %} --judge-model-name
      {{config.params.extra.judge.model_id}} {% endif %} {% if config.type == "aegis_v2_reasoning"
      %} {% if config.params.extra.evaluate_reasoning_traces  %} --evaluate-reasoning-traces
      {% endif %} {% endif %}'
    framework_name: safety_eval
    pkg_name: safety_eval
    config:
      params:
        max_new_tokens: 6144
        max_retries: 5
        parallelism: 8
        task: aegis_v2
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          judge:
            url: null
            model_id: null
            api_key: null
            parallelism: 32
            request_timeout: 60
            max_retries: 16
          evaluate_reasoning_traces: true
      supported_endpoint_types:
      - chat
      type: aegis_v2_reasoning
    target:
      api_endpoint:
        stream: false
- name: wildguard
  description: Wildguard
  harness: safety_eval
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} {% if config.params.extra.judge.api_key is not none %}export JUDGE_API_KEY=${{config.params.extra.judge.api_key}}
      && {% endif %} safety-eval  --model-name  {{target.api_endpoint.model_id}} --model-url
      {{target.api_endpoint.url}} --model-type {{target.api_endpoint.type}}  --judge-url  {{config.params.extra.judge.url}}   --results-dir
      {{config.output_dir}}   --eval {{config.params.task}}  --mut-inference-params
      max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},concurrency={{config.params.parallelism}},retries={{config.params.max_retries}}
      --judge-inference-params concurrency={{config.params.extra.judge.parallelism}},retries={{config.params.max_retries}}  {%
      if config.params.extra.dataset is defined and config.params.extra.dataset %}
      --dataset ''{{config.params.extra.dataset}}''{% endif %} {% if config.params.extra.policy
      is defined and config.params.extra.policy %} --policy ''{{config.params.extra.policy}}''{%
      endif %} {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}
      {% endif %} {% if config.params.extra.judge.model_id is not none %} --judge-model-name
      {{config.params.extra.judge.model_id}} {% endif %} {% if config.type == "aegis_v2_reasoning"
      %} {% if config.params.extra.evaluate_reasoning_traces  %} --evaluate-reasoning-traces
      {% endif %} {% endif %}'
    framework_name: safety_eval
    pkg_name: safety_eval
    config:
      params:
        max_new_tokens: 6144
        max_retries: 5
        parallelism: 8
        task: wildguard
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          judge:
            url: null
            model_id: null
            api_key: null
            parallelism: 32
            request_timeout: 60
            max_retries: 16
      supported_endpoint_types:
      - chat
      type: wildguard
    target:
      api_endpoint:
        stream: false
- name: wildguard_completions
  description: Wildguard. This variant uses the completions endpoint.
  harness: safety_eval
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} {% if config.params.extra.judge.api_key is not none %}export JUDGE_API_KEY=${{config.params.extra.judge.api_key}}
      && {% endif %} safety-eval  --model-name  {{target.api_endpoint.model_id}} --model-url
      {{target.api_endpoint.url}} --model-type {{target.api_endpoint.type}}  --judge-url  {{config.params.extra.judge.url}}   --results-dir
      {{config.output_dir}}   --eval {{config.params.task}}  --mut-inference-params
      max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},concurrency={{config.params.parallelism}},retries={{config.params.max_retries}}
      --judge-inference-params concurrency={{config.params.extra.judge.parallelism}},retries={{config.params.max_retries}}  {%
      if config.params.extra.dataset is defined and config.params.extra.dataset %}
      --dataset ''{{config.params.extra.dataset}}''{% endif %} {% if config.params.extra.policy
      is defined and config.params.extra.policy %} --policy ''{{config.params.extra.policy}}''{%
      endif %} {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}
      {% endif %} {% if config.params.extra.judge.model_id is not none %} --judge-model-name
      {{config.params.extra.judge.model_id}} {% endif %} {% if config.type == "aegis_v2_reasoning"
      %} {% if config.params.extra.evaluate_reasoning_traces  %} --evaluate-reasoning-traces
      {% endif %} {% endif %}'
    framework_name: safety_eval
    pkg_name: safety_eval
    config:
      params:
        max_new_tokens: 6144
        max_retries: 5
        parallelism: 8
        task: wildguard
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          judge:
            url: null
            model_id: null
            api_key: null
            parallelism: 32
            request_timeout: 60
            max_retries: 16
      supported_endpoint_types:
      - completions
      type: wildguard_completions
    target:
      api_endpoint:
        stream: false
- name: compliance
  description: "Compliance integrity benchmark \u2014 evaluates model responses against\
    \ a policy YAML using an LLM judge (chat)."
  harness: safety_eval
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} {% if config.params.extra.judge.api_key is not none %}export JUDGE_API_KEY=${{config.params.extra.judge.api_key}}
      && {% endif %} safety-eval  --model-name  {{target.api_endpoint.model_id}} --model-url
      {{target.api_endpoint.url}} --model-type {{target.api_endpoint.type}}  --judge-url  {{config.params.extra.judge.url}}   --results-dir
      {{config.output_dir}}   --eval {{config.params.task}}  --mut-inference-params
      max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},concurrency={{config.params.parallelism}},retries={{config.params.max_retries}}
      --judge-inference-params concurrency={{config.params.extra.judge.parallelism}},retries={{config.params.max_retries}}  {%
      if config.params.extra.dataset is defined and config.params.extra.dataset %}
      --dataset ''{{config.params.extra.dataset}}''{% endif %} {% if config.params.extra.policy
      is defined and config.params.extra.policy %} --policy ''{{config.params.extra.policy}}''{%
      endif %} {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}
      {% endif %} {% if config.params.extra.judge.model_id is not none %} --judge-model-name
      {{config.params.extra.judge.model_id}} {% endif %} {% if config.type == "aegis_v2_reasoning"
      %} {% if config.params.extra.evaluate_reasoning_traces  %} --evaluate-reasoning-traces
      {% endif %} {% endif %}'
    framework_name: safety_eval
    pkg_name: safety_eval
    config:
      params:
        max_new_tokens: 6144
        max_retries: 5
        parallelism: 8
        task: compliance
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          judge:
            url: null
            model_id: null
            api_key: null
            parallelism: 32
            request_timeout: 60
            max_retries: 16
      supported_endpoint_types:
      - chat
      type: compliance
    target:
      api_endpoint:
        stream: false
- name: medcalc_bench
  description: A dataset which consists of a patient note, a question requesting to
    compute a specific medical value, and a ground truth answer (Khandekar et al.,
    2024).
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: medcalc_bench
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: medcalc_bench
    target:
      api_endpoint: {}
- name: medec
  description: A dataset containing medical narratives with error detection and correction
    pairs (Abacha et al., 2025).
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: medec
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: medec
    target:
      api_endpoint: {}
- name: head_qa
  description: A collection of biomedical multiple-choice questions for testing medical
    knowledge (Vilares et al., 2019).
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: head_qa
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: head_qa
    target:
      api_endpoint: {}
- name: medbullets
  description: A USMLE-style medical question dataset with multiple-choice answers
    and explanations (MedBullets, 2025).
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: medbullets
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: medbullets
    target:
      api_endpoint: {}
- name: pubmed_qa
  description: A dataset that provides PubMed abstracts and asks associated questions
    (yes/no/maybe format).
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: pubmed_qa
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: pubmed_qa
    target:
      api_endpoint: {}
- name: ehr_sql
  description: Given a natural language instruction, generate an SQL query that would
    be used in clinical research.
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: ehr_sql
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: ehr_sql
    target:
      api_endpoint: {}
- name: race_based_med
  description: A collection of LLM outputs in response to medical questions with race-based
    biases, with the objective being to classify whether the output contains racially
    biased content.
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: race_based_med
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: race_based_med
    target:
      api_endpoint: {}
- name: medhallu
  description: A dataset of PubMed articles and associated questions, with the objective
    being to classify whether the answer is factual or hallucinated.
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: medhallu
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: medhallu
    target:
      api_endpoint: {}
- name: mtsamples_replicate
  description: Generate treatment plans based on clinical notes
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: mtsamples_replicate
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: mtsamples_replicate
    target:
      api_endpoint: {}
- name: aci_bench
  description: Extract and structure information from patient-doctor conversations
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: aci_bench
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: aci_bench
    target:
      api_endpoint: {}
- name: mtsamples_procedures
  description: Document and extract information about medical procedures
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: mtsamples_procedures
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: mtsamples_procedures
    target:
      api_endpoint: {}
- name: medication_qa
  description: Answer consumer medication-related questions
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: medication_qa
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: medication_qa
    target:
      api_endpoint: {}
- name: med_dialog_healthcaremagic
  description: Generate summaries of doctor-patient conversations, healthcaremagic
    version
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: med_dialog
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: healthcaremagic
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: med_dialog_healthcaremagic
    target:
      api_endpoint: {}
- name: med_dialog_icliniq
  description: Generate summaries of doctor-patient conversations, icliniq version
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: med_dialog
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: icliniq
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: med_dialog_icliniq
    target:
      api_endpoint: {}
- name: medi_qa
  description: Retrieve and rank answers based on medical question understanding
  harness: helm
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: medi_qa
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: medi_qa
    target:
      api_endpoint: {}
- name: tooltalk
  description: ToolTalk task with default settings.
  harness: tooltalk
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}API_KEY=${{target.api_endpoint.api_key_name}}{%
      endif %} python -m tooltalk.evaluation.evaluate_{{''openai'' if ''azure'' in
      target.api_endpoint.url or ''api.openai'' in target.api_endpoint.url else ''nim''}}
      --dataset data/easy --database data/databases --model {{target.api_endpoint.model_id}}
      {% if config.params.max_new_tokens is not none %}--max_new_tokens {{config.params.max_new_tokens}}{%
      endif %} {% if config.params.temperature is not none %}--temperature {{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}--top_p {{config.params.top_p}}{%
      endif %} --api_mode all --output_dir {{config.output_dir}} --url {{target.api_endpoint.url}}
      {% if config.params.limit_samples is not none %}--first_n {{config.params.limit_samples}}{%
      endif %}'
    framework_name: tooltalk
    pkg_name: tooltalk
    config:
      params:
        extra: {}
      supported_endpoint_types:
      - chat
      type: tooltalk
    target:
      api_endpoint: {}
- name: genai_perf_summarization
  description: GenAI Perf speed evaluation for chat endpoint, summarization task -
    long input, short output
  harness: genai_perf_eval
  defaults:
    command: genai_perf_eval --model_id {{target.api_endpoint.model_id}} --url {{target.api_endpoint.url}}  {%
      if target.api_endpoint.api_key_name is not none %}--api-key {{target.api_endpoint.api_key_name}}
      {% endif %} --concurrencies {{config.params.parallelism}} --isl {{config.params.extra.isl}}
      --osl {{config.params.extra.osl}}  --tokenizer {{config.params.extra.tokenizer}}
      --endpoint-type {{target.api_endpoint.type}} --artifact-dir {{config.output_dir}}
      {% if target.api_endpoint.stream %}--streaming {% endif %}{% if config.params.extra.warmup
      %}--warmup{% endif %}
    framework_name: genai_perf_eval
    pkg_name: genai_perf
    config:
      params:
        parallelism: 1
        extra:
          tokenizer: null
          warmup: true
          isl: 5000
          osl: 500
      supported_endpoint_types:
      - chat
      type: genai_perf_summarization
    target:
      api_endpoint: {}
- name: genai_perf_generation
  description: GenAI Perf speed evaluation for chat endpoint, generation task - short
    input, long output
  harness: genai_perf_eval
  defaults:
    command: genai_perf_eval --model_id {{target.api_endpoint.model_id}} --url {{target.api_endpoint.url}}  {%
      if target.api_endpoint.api_key_name is not none %}--api-key {{target.api_endpoint.api_key_name}}
      {% endif %} --concurrencies {{config.params.parallelism}} --isl {{config.params.extra.isl}}
      --osl {{config.params.extra.osl}}  --tokenizer {{config.params.extra.tokenizer}}
      --endpoint-type {{target.api_endpoint.type}} --artifact-dir {{config.output_dir}}
      {% if target.api_endpoint.stream %}--streaming {% endif %}{% if config.params.extra.warmup
      %}--warmup{% endif %}
    framework_name: genai_perf_eval
    pkg_name: genai_perf
    config:
      params:
        parallelism: 1
        extra:
          tokenizer: null
          warmup: true
          isl: 500
          osl: 5000
      supported_endpoint_types:
      - chat
      type: genai_perf_generation
    target:
      api_endpoint: {}
- name: genai_perf_summarization_completions
  description: GenAI Perf speed evaluation for completions endpoint, summarization
    task - long input, short output
  harness: genai_perf_eval
  defaults:
    command: genai_perf_eval --model_id {{target.api_endpoint.model_id}} --url {{target.api_endpoint.url}}  {%
      if target.api_endpoint.api_key_name is not none %}--api-key {{target.api_endpoint.api_key_name}}
      {% endif %} --concurrencies {{config.params.parallelism}} --isl {{config.params.extra.isl}}
      --osl {{config.params.extra.osl}}  --tokenizer {{config.params.extra.tokenizer}}
      --endpoint-type {{target.api_endpoint.type}} --artifact-dir {{config.output_dir}}
      {% if target.api_endpoint.stream %}--streaming {% endif %}{% if config.params.extra.warmup
      %}--warmup{% endif %}
    framework_name: genai_perf_eval
    pkg_name: genai_perf
    config:
      params:
        parallelism: 1
        task: genai_perf_summarization
        extra:
          tokenizer: null
          warmup: true
          isl: 5000
          osl: 500
      supported_endpoint_types:
      - completions
      type: genai_perf_summarization_completions
    target:
      api_endpoint: {}
- name: genai_perf_generation_completions
  description: GenAI Perf speed evaluation for completions endpoint, generation task
    - short input, long output
  harness: genai_perf_eval
  defaults:
    command: genai_perf_eval --model_id {{target.api_endpoint.model_id}} --url {{target.api_endpoint.url}}  {%
      if target.api_endpoint.api_key_name is not none %}--api-key {{target.api_endpoint.api_key_name}}
      {% endif %} --concurrencies {{config.params.parallelism}} --isl {{config.params.extra.isl}}
      --osl {{config.params.extra.osl}}  --tokenizer {{config.params.extra.tokenizer}}
      --endpoint-type {{target.api_endpoint.type}} --artifact-dir {{config.output_dir}}
      {% if target.api_endpoint.stream %}--streaming {% endif %}{% if config.params.extra.warmup
      %}--warmup{% endif %}
    framework_name: genai_perf_eval
    pkg_name: genai_perf
    config:
      params:
        parallelism: 1
        task: genai_perf_generation
        extra:
          tokenizer: null
          warmup: true
          isl: 500
          osl: 5000
      supported_endpoint_types:
      - completions
      type: genai_perf_generation_completions
    target:
      api_endpoint: {}
- name: mmath_en
  description: English mmath
  harness: mmath
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} mmath --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --lang
      {{config.params.extra.language}} --output-dir {{config.output_dir}} --parallelism
      {{config.params.parallelism}} --retries {{config.params.max_retries}} --max-tokens
      {{config.params.max_new_tokens}} --temperature {{config.params.temperature}}
      --top-p {{config.params.top_p}} --request-timeout {{config.params.request_timeout}}
      --n-samples {{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}}{% endif %}'
    framework_name: mmath
    pkg_name: mmath
    config:
      params:
        max_new_tokens: 32768
        max_retries: 5
        parallelism: 8
        temperature: 0.6
        request_timeout: 3600
        top_p: 0.95
        extra:
          language: en
          n_samples: 4
      supported_endpoint_types:
      - chat
      type: mmath_en
    target:
      api_endpoint:
        stream: false
- name: mmath_zh
  description: Chinese mmath
  harness: mmath
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} mmath --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --lang
      {{config.params.extra.language}} --output-dir {{config.output_dir}} --parallelism
      {{config.params.parallelism}} --retries {{config.params.max_retries}} --max-tokens
      {{config.params.max_new_tokens}} --temperature {{config.params.temperature}}
      --top-p {{config.params.top_p}} --request-timeout {{config.params.request_timeout}}
      --n-samples {{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}}{% endif %}'
    framework_name: mmath
    pkg_name: mmath
    config:
      params:
        max_new_tokens: 32768
        max_retries: 5
        parallelism: 8
        temperature: 0.6
        request_timeout: 3600
        top_p: 0.95
        extra:
          language: zh
          n_samples: 4
      supported_endpoint_types:
      - chat
      type: mmath_zh
    target:
      api_endpoint:
        stream: false
- name: mmath_ar
  description: Arabic mmath
  harness: mmath
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} mmath --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --lang
      {{config.params.extra.language}} --output-dir {{config.output_dir}} --parallelism
      {{config.params.parallelism}} --retries {{config.params.max_retries}} --max-tokens
      {{config.params.max_new_tokens}} --temperature {{config.params.temperature}}
      --top-p {{config.params.top_p}} --request-timeout {{config.params.request_timeout}}
      --n-samples {{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}}{% endif %}'
    framework_name: mmath
    pkg_name: mmath
    config:
      params:
        max_new_tokens: 32768
        max_retries: 5
        parallelism: 8
        temperature: 0.6
        request_timeout: 3600
        top_p: 0.95
        extra:
          language: ar
          n_samples: 4
      supported_endpoint_types:
      - chat
      type: mmath_ar
    target:
      api_endpoint:
        stream: false
- name: mmath_es
  description: Spanish mmath
  harness: mmath
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} mmath --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --lang
      {{config.params.extra.language}} --output-dir {{config.output_dir}} --parallelism
      {{config.params.parallelism}} --retries {{config.params.max_retries}} --max-tokens
      {{config.params.max_new_tokens}} --temperature {{config.params.temperature}}
      --top-p {{config.params.top_p}} --request-timeout {{config.params.request_timeout}}
      --n-samples {{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}}{% endif %}'
    framework_name: mmath
    pkg_name: mmath
    config:
      params:
        max_new_tokens: 32768
        max_retries: 5
        parallelism: 8
        temperature: 0.6
        request_timeout: 3600
        top_p: 0.95
        extra:
          language: es
          n_samples: 4
      supported_endpoint_types:
      - chat
      type: mmath_es
    target:
      api_endpoint:
        stream: false
- name: mmath_fr
  description: French mmath
  harness: mmath
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} mmath --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --lang
      {{config.params.extra.language}} --output-dir {{config.output_dir}} --parallelism
      {{config.params.parallelism}} --retries {{config.params.max_retries}} --max-tokens
      {{config.params.max_new_tokens}} --temperature {{config.params.temperature}}
      --top-p {{config.params.top_p}} --request-timeout {{config.params.request_timeout}}
      --n-samples {{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}}{% endif %}'
    framework_name: mmath
    pkg_name: mmath
    config:
      params:
        max_new_tokens: 32768
        max_retries: 5
        parallelism: 8
        temperature: 0.6
        request_timeout: 3600
        top_p: 0.95
        extra:
          language: fr
          n_samples: 4
      supported_endpoint_types:
      - chat
      type: mmath_fr
    target:
      api_endpoint:
        stream: false
- name: mmath_ja
  description: Japanese mmath
  harness: mmath
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} mmath --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --lang
      {{config.params.extra.language}} --output-dir {{config.output_dir}} --parallelism
      {{config.params.parallelism}} --retries {{config.params.max_retries}} --max-tokens
      {{config.params.max_new_tokens}} --temperature {{config.params.temperature}}
      --top-p {{config.params.top_p}} --request-timeout {{config.params.request_timeout}}
      --n-samples {{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}}{% endif %}'
    framework_name: mmath
    pkg_name: mmath
    config:
      params:
        max_new_tokens: 32768
        max_retries: 5
        parallelism: 8
        temperature: 0.6
        request_timeout: 3600
        top_p: 0.95
        extra:
          language: en
          n_samples: 4
      supported_endpoint_types:
      - chat
      type: mmath_ja
    target:
      api_endpoint:
        stream: false
- name: mmath_ko
  description: Korean mmath
  harness: mmath
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} mmath --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --lang
      {{config.params.extra.language}} --output-dir {{config.output_dir}} --parallelism
      {{config.params.parallelism}} --retries {{config.params.max_retries}} --max-tokens
      {{config.params.max_new_tokens}} --temperature {{config.params.temperature}}
      --top-p {{config.params.top_p}} --request-timeout {{config.params.request_timeout}}
      --n-samples {{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}}{% endif %}'
    framework_name: mmath
    pkg_name: mmath
    config:
      params:
        max_new_tokens: 32768
        max_retries: 5
        parallelism: 8
        temperature: 0.6
        request_timeout: 3600
        top_p: 0.95
        extra:
          language: ko
          n_samples: 4
      supported_endpoint_types:
      - chat
      type: mmath_ko
    target:
      api_endpoint:
        stream: false
- name: mmath_pt
  description: Portuguese mmath
  harness: mmath
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} mmath --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --lang
      {{config.params.extra.language}} --output-dir {{config.output_dir}} --parallelism
      {{config.params.parallelism}} --retries {{config.params.max_retries}} --max-tokens
      {{config.params.max_new_tokens}} --temperature {{config.params.temperature}}
      --top-p {{config.params.top_p}} --request-timeout {{config.params.request_timeout}}
      --n-samples {{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}}{% endif %}'
    framework_name: mmath
    pkg_name: mmath
    config:
      params:
        max_new_tokens: 32768
        max_retries: 5
        parallelism: 8
        temperature: 0.6
        request_timeout: 3600
        top_p: 0.95
        extra:
          language: pt
          n_samples: 4
      supported_endpoint_types:
      - chat
      type: mmath_pt
    target:
      api_endpoint:
        stream: false
- name: mmath_th
  description: Thai mmath
  harness: mmath
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} mmath --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --lang
      {{config.params.extra.language}} --output-dir {{config.output_dir}} --parallelism
      {{config.params.parallelism}} --retries {{config.params.max_retries}} --max-tokens
      {{config.params.max_new_tokens}} --temperature {{config.params.temperature}}
      --top-p {{config.params.top_p}} --request-timeout {{config.params.request_timeout}}
      --n-samples {{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}}{% endif %}'
    framework_name: mmath
    pkg_name: mmath
    config:
      params:
        max_new_tokens: 32768
        max_retries: 5
        parallelism: 8
        temperature: 0.6
        request_timeout: 3600
        top_p: 0.95
        extra:
          language: th
          n_samples: 4
      supported_endpoint_types:
      - chat
      type: mmath_th
    target:
      api_endpoint:
        stream: false
- name: mmath_vi
  description: Vietnamese mmath
  harness: mmath
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key_name}}  &&
      {% endif %} mmath --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --lang
      {{config.params.extra.language}} --output-dir {{config.output_dir}} --parallelism
      {{config.params.parallelism}} --retries {{config.params.max_retries}} --max-tokens
      {{config.params.max_new_tokens}} --temperature {{config.params.temperature}}
      --top-p {{config.params.top_p}} --request-timeout {{config.params.request_timeout}}
      --n-samples {{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}}{% endif %}'
    framework_name: mmath
    pkg_name: mmath
    config:
      params:
        max_new_tokens: 32768
        max_retries: 5
        parallelism: 8
        temperature: 0.6
        request_timeout: 3600
        top_p: 0.95
        extra:
          language: vi
          n_samples: 4
      supported_endpoint_types:
      - chat
      type: mmath_vi
    target:
      api_endpoint:
        stream: false
- name: tau2_bench_telecom
  description: tau2-bench - Telecom Domain (used by Artificial Analysis Index v2)
  harness: tau2_bench
  defaults:
    command: '{% if config.params.extra.cache.enabled %}export LLM_CACHE_ENABLED=true
      && export CACHE_TYPE=disk && export CACHE_DIR={{config.params.extra.cache.cache_dir}}
      && {% endif %} tau2 run --domain {{config.params.task}}  --agent-llm openai/{{target.api_endpoint.model_id}}  --user-llm
      openai/{{config.params.extra.user.model_id}}  {% if config.params.extra.judge.enabled
      %}--judge-llm openai/{{config.params.extra.judge.model_id}}{% endif %}  {% if
      target.api_endpoint.api_key_name is not none %}--agent-api-key {{target.api_endpoint.api_key_name}}{%
      endif %}  {% if config.params.extra.user.api_key is not none %}--user-api-key
      {{config.params.extra.user.api_key}}{% endif %} {% if config.params.extra.judge.enabled
      and config.params.extra.judge.api_key is not none %}--judge-api-key {{config.params.extra.judge.api_key}}{%
      endif %} --agent-llm-args ''{"base_url": "{{target.api_endpoint.url}}", "temperature":
      {{config.params.temperature}}, "top_p": {{config.params.top_p}}, "max_completion_tokens":
      {{config.params.max_new_tokens}}, "timeout": {{config.params.request_timeout}}{%
      if config.params.extra.agent_args is defined and config.params.extra.agent_args
      is not none %}{% for key, value in config.params.extra.agent_args.items() %},
      "{{key}}": {{value|tojson}}{% endfor %}{% endif %}}''  --user-llm-args ''{"base_url":
      "{{config.params.extra.user.url}}", "temperature": {{config.params.extra.user.temperature}},
      "top_p": {{config.params.extra.user.top_p}}, "max_completion_tokens": {{config.params.extra.user.max_new_tokens}},
      "timeout": {{config.params.extra.user.request_timeout}}{% if config.params.extra.user.args
      is defined and config.params.extra.user.args is not none %}{% for key, value
      in config.params.extra.user.args.items() %}, "{{key}}": {{value|tojson}}{% endfor
      %}{% endif %}}''  {% if config.params.extra.judge.enabled %}--judge-llm-args
      ''{"base_url": "{{config.params.extra.judge.url}}", "temperature": {{config.params.extra.judge.temperature}},
      "top_p": {{config.params.extra.judge.top_p}}, "max_completion_tokens": {{config.params.extra.judge.max_new_tokens}},
      "timeout": {{config.params.extra.judge.request_timeout}}{% if config.params.extra.judge.args
      is defined and config.params.extra.judge.args is not none %}{% for key, value
      in config.params.extra.judge.args.items() %}, "{{key}}": {{value|tojson}}{%
      endfor %}{% endif %}}''{% endif %} {% if config.params.extra.judge.enabled %}--judge-system-prompt
      "{{config.params.extra.judge.system_prompt}}"{% endif %} {% if config.params.extra.judge.enabled
      %}--judge-window-size {{config.params.extra.judge_window_size}}{% endif %} --max-concurrency
      {{config.params.parallelism}} --max-retries {{config.params.max_retries}}  --max-steps
      {{config.params.extra.max_steps}} --results-dir {{config.output_dir}} --num-trials
      {{config.params.extra.n_samples}} {% if config.params.limit_samples is not none
      %} --num-tasks {{config.params.limit_samples}} {% endif %} {% if config.params.extra.skip_failed_samples
      %} --skip-failed-samples {% endif %}'
    framework_name: tau2_bench
    pkg_name: nvidia_tau2
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: telecom
        temperature: 0.0
        request_timeout: 3600
        top_p: 0.95
        extra:
          n_samples: 3
          max_steps: 100
          judge_window_size: 30
          skip_failed_samples: false
          cache:
            enabled: true
            cache_dir: .cache/llm_cache
          user:
            url: https://integrate.api.nvidia.com/v1/chat/completions
            model_id: nvdev/qwen/qwen-235b
            api_key: USER_API_KEY
            temperature: 0.0
            max_new_tokens: 4096
            top_p: 0.95
            request_timeout: 3600
          judge:
            enabled: false
            url: https://integrate.api.nvidia.com/v1/chat/completions
            model_id: openai/gpt-oss-120b
            system_prompt: Reasoning:medium
            api_key: JUDGE_API_KEY
            temperature: 0.6
            max_new_tokens: 16000
            top_p: 0.95
            request_timeout: 3600
      supported_endpoint_types:
      - chat
      type: tau2_bench_telecom
    target:
      api_endpoint:
        stream: false
- name: tau2_bench_airline
  description: tau2-bench - Airline Domain
  harness: tau2_bench
  defaults:
    command: '{% if config.params.extra.cache.enabled %}export LLM_CACHE_ENABLED=true
      && export CACHE_TYPE=disk && export CACHE_DIR={{config.params.extra.cache.cache_dir}}
      && {% endif %} tau2 run --domain {{config.params.task}}  --agent-llm openai/{{target.api_endpoint.model_id}}  --user-llm
      openai/{{config.params.extra.user.model_id}}  {% if config.params.extra.judge.enabled
      %}--judge-llm openai/{{config.params.extra.judge.model_id}}{% endif %}  {% if
      target.api_endpoint.api_key_name is not none %}--agent-api-key {{target.api_endpoint.api_key_name}}{%
      endif %}  {% if config.params.extra.user.api_key is not none %}--user-api-key
      {{config.params.extra.user.api_key}}{% endif %} {% if config.params.extra.judge.enabled
      and config.params.extra.judge.api_key is not none %}--judge-api-key {{config.params.extra.judge.api_key}}{%
      endif %} --agent-llm-args ''{"base_url": "{{target.api_endpoint.url}}", "temperature":
      {{config.params.temperature}}, "top_p": {{config.params.top_p}}, "max_completion_tokens":
      {{config.params.max_new_tokens}}, "timeout": {{config.params.request_timeout}}{%
      if config.params.extra.agent_args is defined and config.params.extra.agent_args
      is not none %}{% for key, value in config.params.extra.agent_args.items() %},
      "{{key}}": {{value|tojson}}{% endfor %}{% endif %}}''  --user-llm-args ''{"base_url":
      "{{config.params.extra.user.url}}", "temperature": {{config.params.extra.user.temperature}},
      "top_p": {{config.params.extra.user.top_p}}, "max_completion_tokens": {{config.params.extra.user.max_new_tokens}},
      "timeout": {{config.params.extra.user.request_timeout}}{% if config.params.extra.user.args
      is defined and config.params.extra.user.args is not none %}{% for key, value
      in config.params.extra.user.args.items() %}, "{{key}}": {{value|tojson}}{% endfor
      %}{% endif %}}''  {% if config.params.extra.judge.enabled %}--judge-llm-args
      ''{"base_url": "{{config.params.extra.judge.url}}", "temperature": {{config.params.extra.judge.temperature}},
      "top_p": {{config.params.extra.judge.top_p}}, "max_completion_tokens": {{config.params.extra.judge.max_new_tokens}},
      "timeout": {{config.params.extra.judge.request_timeout}}{% if config.params.extra.judge.args
      is defined and config.params.extra.judge.args is not none %}{% for key, value
      in config.params.extra.judge.args.items() %}, "{{key}}": {{value|tojson}}{%
      endfor %}{% endif %}}''{% endif %} {% if config.params.extra.judge.enabled %}--judge-system-prompt
      "{{config.params.extra.judge.system_prompt}}"{% endif %} {% if config.params.extra.judge.enabled
      %}--judge-window-size {{config.params.extra.judge_window_size}}{% endif %} --max-concurrency
      {{config.params.parallelism}} --max-retries {{config.params.max_retries}}  --max-steps
      {{config.params.extra.max_steps}} --results-dir {{config.output_dir}} --num-trials
      {{config.params.extra.n_samples}} {% if config.params.limit_samples is not none
      %} --num-tasks {{config.params.limit_samples}} {% endif %} {% if config.params.extra.skip_failed_samples
      %} --skip-failed-samples {% endif %}'
    framework_name: tau2_bench
    pkg_name: nvidia_tau2
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: airline
        temperature: 0.0
        request_timeout: 3600
        top_p: 0.95
        extra:
          n_samples: 3
          max_steps: 100
          judge_window_size: 30
          skip_failed_samples: false
          cache:
            enabled: true
            cache_dir: .cache/llm_cache
          user:
            url: https://integrate.api.nvidia.com/v1/chat/completions
            model_id: nvdev/qwen/qwen-235b
            api_key: USER_API_KEY
            temperature: 0.0
            max_new_tokens: 4096
            top_p: 0.95
            request_timeout: 3600
          judge:
            enabled: false
            url: https://integrate.api.nvidia.com/v1/chat/completions
            model_id: openai/gpt-oss-120b
            system_prompt: Reasoning:medium
            api_key: JUDGE_API_KEY
            temperature: 0.6
            max_new_tokens: 16000
            top_p: 0.95
            request_timeout: 3600
      supported_endpoint_types:
      - chat
      type: tau2_bench_airline
    target:
      api_endpoint:
        stream: false
- name: tau2_bench_retail
  description: tau2-bench - Retail Domain
  harness: tau2_bench
  defaults:
    command: '{% if config.params.extra.cache.enabled %}export LLM_CACHE_ENABLED=true
      && export CACHE_TYPE=disk && export CACHE_DIR={{config.params.extra.cache.cache_dir}}
      && {% endif %} tau2 run --domain {{config.params.task}}  --agent-llm openai/{{target.api_endpoint.model_id}}  --user-llm
      openai/{{config.params.extra.user.model_id}}  {% if config.params.extra.judge.enabled
      %}--judge-llm openai/{{config.params.extra.judge.model_id}}{% endif %}  {% if
      target.api_endpoint.api_key_name is not none %}--agent-api-key {{target.api_endpoint.api_key_name}}{%
      endif %}  {% if config.params.extra.user.api_key is not none %}--user-api-key
      {{config.params.extra.user.api_key}}{% endif %} {% if config.params.extra.judge.enabled
      and config.params.extra.judge.api_key is not none %}--judge-api-key {{config.params.extra.judge.api_key}}{%
      endif %} --agent-llm-args ''{"base_url": "{{target.api_endpoint.url}}", "temperature":
      {{config.params.temperature}}, "top_p": {{config.params.top_p}}, "max_completion_tokens":
      {{config.params.max_new_tokens}}, "timeout": {{config.params.request_timeout}}{%
      if config.params.extra.agent_args is defined and config.params.extra.agent_args
      is not none %}{% for key, value in config.params.extra.agent_args.items() %},
      "{{key}}": {{value|tojson}}{% endfor %}{% endif %}}''  --user-llm-args ''{"base_url":
      "{{config.params.extra.user.url}}", "temperature": {{config.params.extra.user.temperature}},
      "top_p": {{config.params.extra.user.top_p}}, "max_completion_tokens": {{config.params.extra.user.max_new_tokens}},
      "timeout": {{config.params.extra.user.request_timeout}}{% if config.params.extra.user.args
      is defined and config.params.extra.user.args is not none %}{% for key, value
      in config.params.extra.user.args.items() %}, "{{key}}": {{value|tojson}}{% endfor
      %}{% endif %}}''  {% if config.params.extra.judge.enabled %}--judge-llm-args
      ''{"base_url": "{{config.params.extra.judge.url}}", "temperature": {{config.params.extra.judge.temperature}},
      "top_p": {{config.params.extra.judge.top_p}}, "max_completion_tokens": {{config.params.extra.judge.max_new_tokens}},
      "timeout": {{config.params.extra.judge.request_timeout}}{% if config.params.extra.judge.args
      is defined and config.params.extra.judge.args is not none %}{% for key, value
      in config.params.extra.judge.args.items() %}, "{{key}}": {{value|tojson}}{%
      endfor %}{% endif %}}''{% endif %} {% if config.params.extra.judge.enabled %}--judge-system-prompt
      "{{config.params.extra.judge.system_prompt}}"{% endif %} {% if config.params.extra.judge.enabled
      %}--judge-window-size {{config.params.extra.judge_window_size}}{% endif %} --max-concurrency
      {{config.params.parallelism}} --max-retries {{config.params.max_retries}}  --max-steps
      {{config.params.extra.max_steps}} --results-dir {{config.output_dir}} --num-trials
      {{config.params.extra.n_samples}} {% if config.params.limit_samples is not none
      %} --num-tasks {{config.params.limit_samples}} {% endif %} {% if config.params.extra.skip_failed_samples
      %} --skip-failed-samples {% endif %}'
    framework_name: tau2_bench
    pkg_name: nvidia_tau2
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: retail
        temperature: 0.0
        request_timeout: 3600
        top_p: 0.95
        extra:
          n_samples: 3
          max_steps: 100
          judge_window_size: 30
          skip_failed_samples: false
          cache:
            enabled: true
            cache_dir: .cache/llm_cache
          user:
            url: https://integrate.api.nvidia.com/v1/chat/completions
            model_id: nvdev/qwen/qwen-235b
            api_key: USER_API_KEY
            temperature: 0.0
            max_new_tokens: 4096
            top_p: 0.95
            request_timeout: 3600
          judge:
            enabled: false
            url: https://integrate.api.nvidia.com/v1/chat/completions
            model_id: openai/gpt-oss-120b
            system_prompt: Reasoning:medium
            api_key: JUDGE_API_KEY
            temperature: 0.6
            max_new_tokens: 16000
            top_p: 0.95
            request_timeout: 3600
      supported_endpoint_types:
      - chat
      type: tau2_bench_retail
    target:
      api_endpoint:
        stream: false
- name: ruler-chat
  description: RULER (chat mode) without specified context length. A user must explicitly
    specify `max_seq_length` parameter.
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: null
          subtasks: all
      supported_endpoint_types:
      - chat
      type: ruler-chat
    target:
      api_endpoint: {}
- name: ruler-completions
  description: RULER (completions mode) without specified context length. A user must
    explicitly specify `max_seq_length` parameter.
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: null
          subtasks: all
      supported_endpoint_types:
      - completions
      type: ruler-completions
    target:
      api_endpoint: {}
- name: ruler-4k-chat
  description: RULER with context length of 4k (chat mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 4000
          subtasks: all
      supported_endpoint_types:
      - chat
      type: ruler-4k-chat
    target:
      api_endpoint: {}
- name: ruler-4k-completions
  description: RULER with context length of 4k (completions mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 4000
          subtasks: all
      supported_endpoint_types:
      - completions
      type: ruler-4k-completions
    target:
      api_endpoint: {}
- name: ruler-8k-chat
  description: RULER with context length of 8k (chat mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 8000
          subtasks: all
      supported_endpoint_types:
      - chat
      type: ruler-8k-chat
    target:
      api_endpoint: {}
- name: ruler-8k-completions
  description: RULER with context length of 8k (completions mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 8000
          subtasks: all
      supported_endpoint_types:
      - completions
      type: ruler-8k-completions
    target:
      api_endpoint: {}
- name: ruler-16k-chat
  description: RULER with context length of 16k (chat mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 16000
          subtasks: all
      supported_endpoint_types:
      - chat
      type: ruler-16k-chat
    target:
      api_endpoint: {}
- name: ruler-16k-completions
  description: RULER with context length of 16k (completions mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 16000
          subtasks: all
      supported_endpoint_types:
      - completions
      type: ruler-16k-completions
    target:
      api_endpoint: {}
- name: ruler-32k-chat
  description: RULER with context length of 32k (chat mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 32000
          subtasks: all
      supported_endpoint_types:
      - chat
      type: ruler-32k-chat
    target:
      api_endpoint: {}
- name: ruler-32k-completions
  description: RULER with context length of 32k (completions mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 32000
          subtasks: all
      supported_endpoint_types:
      - completions
      type: ruler-32k-completions
    target:
      api_endpoint: {}
- name: ruler-64k-chat
  description: RULER with context length of 64k (chat mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 64000
          subtasks: all
      supported_endpoint_types:
      - chat
      type: ruler-64k-chat
    target:
      api_endpoint: {}
- name: ruler-64k-completions
  description: RULER with context length of 64k (completions mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 64000
          subtasks: all
      supported_endpoint_types:
      - completions
      type: ruler-64k-completions
    target:
      api_endpoint: {}
- name: ruler-128k-chat
  description: RULER with context length of 128k (chat mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 128000
          subtasks: all
      supported_endpoint_types:
      - chat
      type: ruler-128k-chat
    target:
      api_endpoint: {}
- name: ruler-128k-completions
  description: RULER with context length of 128k (completions mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 128000
          subtasks: all
      supported_endpoint_types:
      - completions
      type: ruler-128k-completions
    target:
      api_endpoint: {}
- name: ruler-256k-chat
  description: RULER with context length of 256k (chat mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 256000
          subtasks: all
      supported_endpoint_types:
      - chat
      type: ruler-256k-chat
    target:
      api_endpoint: {}
- name: ruler-256k-completions
  description: RULER with context length of 256k (completions mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 256000
          subtasks: all
      supported_endpoint_types:
      - completions
      type: ruler-256k-completions
    target:
      api_endpoint: {}
- name: ruler-512k-chat
  description: RULER with context length of 512k (chat mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 512000
          subtasks: all
      supported_endpoint_types:
      - chat
      type: ruler-512k-chat
    target:
      api_endpoint: {}
- name: ruler-512k-completions
  description: RULER with context length of 512k (completions mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 512000
          subtasks: all
      supported_endpoint_types:
      - completions
      type: ruler-512k-completions
    target:
      api_endpoint: {}
- name: ruler-1m-chat
  description: RULER with context length of 1M (chat mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 1000000
          subtasks: all
      supported_endpoint_types:
      - chat
      type: ruler-1m-chat
    target:
      api_endpoint: {}
- name: ruler-1m-completions
  description: RULER with context length of 1M (completions mode)
  harness: ruler
  defaults:
    command: python -c "import nltk;nltk.download('punkt_tab');nltk.download('punkt')"
      && {% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      &&{% endif %} long_context_eval --url {{target.api_endpoint.url}} --tasks "{{config.params.extra.subtasks}}"
      --result_dir {{config.output_dir}} --model {{target.api_endpoint.model_id}}
      --mode {% if target.api_endpoint.type == "completions" %}completion{% elif target.api_endpoint.type
      == "chat" %}chat{% endif %} --tokenizer_path "{{config.params.extra.tokenizer}}"
      --tokenizer_type "{{config.params.extra.tokenizer_backend}}" --temperature {{config.params.temperature}}
      --top_p {{config.params.top_p}} {% if config.params.limit_samples is not none
      %}--num_samples {{config.params.limit_samples}}{% endif %} {% if config.params.extra.max_seq_length
      is defined %}--max_seq_length {{config.params.extra.max_seq_length}}{% endif
      %} --timeout {{config.params.request_timeout}} --threads {{config.params.parallelism}}
      {% if config.params.max_new_tokens is not none %}--tokens_to_generate {{config.params.max_new_tokens}}{%
      endif %}
    framework_name: ruler
    pkg_name: long_context_eval
    config:
      params:
        parallelism: 1
        temperature: 0.0
        request_timeout: 300
        top_p: 0.0001
        extra:
          tokenizer: null
          tokenizer_backend: hf
          max_seq_length: 1000000
          subtasks: all
      supported_endpoint_types:
      - completions
      type: ruler-1m-completions
    target:
      api_endpoint: {}
- name: mmlu_test
  description: Task for detecting contamination with the MMLU test set
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: mmlu_test
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: mmlu_test
    target:
      api_endpoint: {}
- name: gpqa_diamond
  description: Task for detecting contamination with the GPQA diamond
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: gpqa_diamond
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: gpqa_diamond
    target:
      api_endpoint: {}
- name: gsm8k_train
  description: Task for detecting contamination with the GSM8K train set
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: gsm8k_train
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: gsm8k_train
    target:
      api_endpoint: {}
- name: gsm8k_test
  description: Task for detecting contamination with the GSM8K test set
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: gsm8k_test
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: gsm8k_test
    target:
      api_endpoint: {}
- name: ifeval
  description: Task for detecting contamination with the IFeval dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: ifeval
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: ifeval
    target:
      api_endpoint: {}
- name: mmlu_pro_test
  description: Task for detecting contamination with the MMLU-Pro test set
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: mmlu_pro_test
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: mmlu_pro_test
    target:
      api_endpoint: {}
- name: openai_humaneval
  description: Task for detecting contamination with the OpenAI HumanEval dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: openai_humaneval
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: openai_humaneval
    target:
      api_endpoint: {}
- name: frames
  description: Task for detecting contamination with the FRAMES dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: frames
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: frames
    target:
      api_endpoint: {}
- name: hellaswag_test
  description: Task for detecting contamination with the Hellaswag test set
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: hellaswag_test
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: hellaswag_test
    target:
      api_endpoint: {}
- name: hellaswag_train
  description: Task for detecting contamination with the Hellaswag train set
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: hellaswag_train
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: hellaswag_train
    target:
      api_endpoint: {}
- name: aime_2025
  description: Task for detecting contamination with the AIME 2025 dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: aime_2025
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: aime_2025
    target:
      api_endpoint: {}
- name: aime_2024
  description: Task for detecting contamination with the AIME 2024 dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: aime_2024
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: aime_2024
    target:
      api_endpoint: {}
- name: livecodebench_v1
  description: Task for detecting contamination with the LiveCodeBench v1 dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: livecodebench_v1
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: livecodebench_v1
    target:
      api_endpoint: {}
- name: livecodebench_v5
  description: Task for detecting contamination with the LiveCodeBench v5 dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: livecodebench_v5
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: livecodebench_v5
    target:
      api_endpoint: {}
- name: bfcl_v3
  description: Task for detecting contamination with the BFCL v3 dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: bfcl_v3
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: bfcl_v3
    target:
      api_endpoint: {}
- name: bbq
  description: Task for detecting contamination with the BBQ dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: bbq
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: bbq
    target:
      api_endpoint: {}
- name: reward_bench_v1
  description: Task for detecting contamination with the Reward Bench v1 dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: reward_bench_v1
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: reward_bench_v1
    target:
      api_endpoint: {}
- name: reward_bench_v2
  description: Task for detecting contamination with the Reward Bench v2 dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: reward_bench_v2
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: reward_bench_v2
    target:
      api_endpoint: {}
- name: math_500_problem
  description: Task for detecting contamination with the Math 500 dataset (problem
    statements)
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: math_500_problem
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: math_500_problem
    target:
      api_endpoint: {}
- name: math_500_solution
  description: Task for detecting contamination with the Math 500 dataset (solutions)
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: math_500_solution
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: math_500_solution
    target:
      api_endpoint: {}
- name: swebench_test
  description: Task for detecting contamination with the SWE-bench dataset (test split)
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: swebench_test
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: swebench_test
    target:
      api_endpoint: {}
- name: swebench_train
  description: Task for detecting contamination with the SWE-bench dataset (train
    split)
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: swebench_train
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: swebench_train
    target:
      api_endpoint: {}
- name: hle
  description: Task for detecting contamination with the HLE dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: hle
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: hle
    target:
      api_endpoint: {}
- name: ifbench
  description: Task for detecting contamination with the IFBench dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: ifbench
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: ifbench
    target:
      api_endpoint: {}
- name: scicode
  description: Task for detecting contamination with the SciCode dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: scicode
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: scicode
    target:
      api_endpoint: {}
- name: terminalbench
  description: Task for detecting contamination with the Terminal-Bench dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: terminalbench
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: terminalbench
    target:
      api_endpoint: {}
- name: taubench
  description: Task for detecting contamination with the Tau-bench dataset
  harness: codec
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_KEY=${{target.api_endpoint.api_key_name}}
      && {% endif %} codec --model {{target.api_endpoint.model_id}} --eval_name {{config.params.task}}
      --contamination_type {{config.params.extra.contamination_type}} --url {{target.api_endpoint.url}}
      --temperature {{config.params.temperature}} --top_p {{config.params.top_p}}
      --n_context_seeds {{config.params.extra.n_context_seeds}} --out_dir {{config.output_dir}}/results
      --cache_dir {{config.output_dir}}/cache --num_threads {{config.params.parallelism}}
      --max_retries {{config.params.max_retries}} --timeout {{config.params.request_timeout}}
      --min_length {{config.params.extra.min_length}} --max_length {{config.params.extra.max_length}}
      {% if config.params.limit_samples is not none %} --n_samples {{config.params.limit_samples}}{%
      endif %}'
    framework_name: codec
    pkg_name: codec
    config:
      params:
        limit_samples: 1000
        max_retries: 10
        parallelism: 20
        task: taubench
        temperature: 0.0
        request_timeout: 120
        top_p: 1.0
        extra:
          contamination_type: in_context
          n_context_seeds: 5
          min_length: 100
          max_length: 2048
      supported_endpoint_types:
      - completions
      type: taubench
    target:
      api_endpoint: {}
- name: fiqa
  description: Financial Opinion Mining and Question Answering
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: FiQA2018
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: fiqa
    target:
      api_endpoint: {}
- name: nano_fiqa
  description: NanoFiQA2018 is a smaller subset of the Financial Opinion Mining and
    Question Answering dataset.
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: NanoFiQA2018Retrieval
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: train
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: nano_fiqa
    target:
      api_endpoint: {}
- name: hotpotqa
  description: HotpotQA is a question answering dataset featuring natural, multi-hop
    questions, with strong supervision for supporting facts to enable more explainable
    question answering systems.
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: HotpotQA
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: hotpotqa
    target:
      api_endpoint: {}
- name: nq
  description: Natural Questions (NQ) contains real user questions issued to Google
    search, and answers found from Wikipedia by annotators. NQ is designed for the
    training and evaluation of automatic question answering systems.
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: NQ
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: nq
    target:
      api_endpoint: {}
- name: miracl
  description: MIRACL (Multilingual Information Retrieval Across a Continuum of Languages)
    is a multilingual retrieval dataset that focuses on search across 18 different
    languages.
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: MIRACLRetrieval
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: miracl
    target:
      api_endpoint: {}
- name: miracl_lite
  description: MIRACL (Multilingual Information Retrieval Across a Continuum of Languages)
    is a multilingual retrieval dataset that focuses on search across 18 different
    languages.
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: MIRACLRetrieval
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: true
          language: null
      supported_endpoint_types:
      - embedding
      type: miracl_lite
    target:
      api_endpoint: {}
- name: mlqa
  description: MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating
    cross-lingual question answering performance.
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: MLQARetrieval
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: mlqa
    target:
      api_endpoint: {}
- name: mldr
  description: MLDR
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: MultiLongDocRetrieval
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: mldr
    target:
      api_endpoint: {}
- name: custom_beir_task
  description: Custom BEIR-formatted text retrieval benchmark
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: custom_beir_task
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: custom_beir_task
    target:
      api_endpoint: {}
- name: MTEB
  description: MTEB
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: MTEB(eng, v2)
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: MTEB
    target:
      api_endpoint: {}
- name: MTEB_NL_RETRIEVAL
  description: MTEB_NL_RETRIEVAL
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: MTEB(nld, v1, retrieval)
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: MTEB_NL_RETRIEVAL
    target:
      api_endpoint: {}
- name: MMTEB
  description: MMTEB
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: MTEB(Multilingual, v2)
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: MMTEB
    target:
      api_endpoint: {}
- name: RTEB
  description: RTEB
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: RTEB(beta)
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: RTEB
    target:
      api_endpoint: {}
- name: MTEB_VDR
  description: MTEB Visual Document Retrieval benchmark
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: VisualDocumentRetrieval
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: MTEB_VDR
    target:
      api_endpoint: {}
- name: ViDoReV1
  description: ViDoReV1
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: ViDoRe(v1)
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: ViDoReV1
    target:
      api_endpoint: {}
- name: ViDoReV2
  description: ViDoReV2
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: ViDoRe(v2)
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: ViDoReV2
    target:
      api_endpoint: {}
- name: ViDoReV3
  description: ViDoReV3
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: ViDoRe(v3)
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: ViDoReV3
    target:
      api_endpoint: {}
- name: ViDoReV3_Text
  description: ViDoReV3 Text (text_image markdown only)
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: ViDoRe(v3, Text)
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: ViDoReV3_Text
    target:
      api_endpoint: {}
- name: ViDoReV3_Text_Image
  description: ViDoReV3 Text+Image (text_image markdown + images)
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: ViDoRe(v3, Text+Image)
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: ViDoReV3_Text_Image
    target:
      api_endpoint: {}
- name: techqa
  description: NVIDIA TechQA
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: TechQA
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: techqa
    target:
      api_endpoint: {}
- name: nvidia_vidore_v1
  description: NVIDIA ViDoReV1
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: nvidia_vidore_v1
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: nvidia_vidore_v1
    target:
      api_endpoint: {}
- name: nvidia_vidore_v2
  description: NVIDIA ViDoReV2
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: nvidia_vidore_v2
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: nvidia_vidore_v2
    target:
      api_endpoint: {}
- name: nvidia_vidore_v3
  description: NVIDIA ViDoReV3
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: nvidia_vidore_v3
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: nvidia_vidore_v3
    target:
      api_endpoint: {}
- name: nvidia_digital_corpora_10k_text
  description: NVIDIA Internal - Digital Corpora10k Text Retrieval
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: nvidia_digital_corpora_10k_text
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: nvidia_digital_corpora_10k_text
    target:
      api_endpoint: {}
- name: nvidia_digital_corpora_10k
  description: NVIDIA Internal - Digital Corpora10k Retrieval
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: nvidia_digital_corpora_10k
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: nvidia_digital_corpora_10k
    target:
      api_endpoint: {}
- name: nvidia_earnings_v2_text
  description: NVIDIA Internal - Earnings V2 Text Retrieval
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: nvidia_earnings_v2_text
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: nvidia_earnings_v2_text
    target:
      api_endpoint: {}
- name: nvidia_earnings_v2
  description: NVIDIA Internal - Earnings V2 Multimodal Retrieval
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: nvidia_earnings_v2
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: nvidia_earnings_v2
    target:
      api_endpoint: {}
- name: nvidia_vidore_v1_text
  description: NVIDIA Internal - ViDoReV1 Text Retrieval
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: NVIDIA ViDoRe V1 (Text)
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: nvidia_vidore_v1_text
    target:
      api_endpoint: {}
- name: nvidia_vidore_v2_text
  description: NVIDIA Internal - ViDoReV2 Text Retrieval
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: NVIDIA ViDoRe V2 (Text)
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: nvidia_vidore_v2_text
    target:
      api_endpoint: {}
- name: nvidia_vidore_v3_text
  description: NVIDIA Internal - ViDoReV3 Text Retrieval
  harness: mteb
  defaults:
    command: '{% if target.api_endpoint.api_key_name is not none %}export API_TOKEN=${{target.api_endpoint.api_key_name}}
      &&{% endif %} {% if config.params.extra.dataset_path is not none %} export MTEB_INTERNAL_DATASET_PATH={{config.params.extra.dataset_path}}
      &&{% endif %} {% if config.params.extra.ranker.api_key is not none %}export
      RANKER_API_TOKEN=${{config.params.extra.ranker.api_key}} &&{% endif %} mteb  --encoder_name
      {{target.api_endpoint.model_id}} --encoder_url {{target.api_endpoint.url}} --task
      "{{config.params.task}}" --workdir {{config.output_dir}} --batch_size {{config.params.extra.batch_size}}
      --async_limit {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --request_timeout {{config.params.request_timeout}} {% if config.params.extra.cache_path
      is not none %} --cache_path {{config.params.extra.cache_path}}{% endif %} {%
      if config.params.extra.args is not none %} {{config.params.extra.args}} {% endif
      %} {% if config.params.extra.language is not none %} --langs {{config.params.extra.language}}
      {% endif %} {% if config.params.extra.query_prompt_template is not none %} --query_prompt_template
      "{{config.params.extra.query_prompt_template}}"{% endif %} {% if config.params.extra.document_prompt_template
      is not none %} --document_prompt_template "{{config.params.extra.document_prompt_template}}"{%
      endif %} {% if config.params.extra.ranker.model_id is not none %} --ranker_name
      {{config.params.extra.ranker.model_id}} --ranker_url {{config.params.extra.ranker.url}}
      --ranker_endpoint_type {{config.params.extra.ranker.endpoint_type}}{% endif
      %} --truncate {{config.params.extra.truncate}} --top_k {{config.params.extra.top_k}}
      {% if config.params.extra.version_lite is not none%} --version_lite {{config.params.extra.version_lite}}
      {% endif %} {% if config.params.extra.eval_split is not none %} --eval_split
      {{config.params.extra.eval_split}} {% endif %}'
    framework_name: mteb
    pkg_name: mteb
    config:
      params:
        max_retries: 10
        parallelism: 20
        task: NVIDIA ViDoRe V3 (Text)
        request_timeout: 300
        extra:
          query_prompt_template: null
          document_prompt_template: null
          ranker:
            model_id: null
            url: null
            api_key: null
            endpoint_type: nim
          top_k: 40
          truncate: END
          batch_size: 128
          eval_split: test
          dataset_path: null
          cache_path: null
          args: null
          version_lite: null
          language: null
      supported_endpoint_types:
      - embedding
      type: nvidia_vidore_v3_text
    target:
      api_endpoint: {}
- name: aa_lcr
  description: A challenging benchmark measuring language models' ability to extract,
    reason about, and synthesize information from long-form documents ranging from
    10k to 100k tokens (measured using the cl100k_base tokenizer).
  harness: AA-LCR
  defaults:
    command: aa_lcr --model={{target.api_endpoint.model_id}} --endpoint_url={{target.api_endpoint.url}}  --temperature={{config.params.temperature}}
      --top_p={{config.params.top_p}} --request_timeout={{config.params.request_timeout}}  {%
      if config.params.limit_samples is not none %}--limit {{config.params.limit_samples}}{%
      endif %} --output_dir={{config.output_dir}}  {% if target.api_endpoint.api_key_name
      is not none %}--api_key_name={{target.api_endpoint.api_key_name}}{% endif %}
      --max_retries={{config.params.max_retries}}  --max_new_tokens={{config.params.max_new_tokens}}
      --async_limit={{config.params.parallelism}} --num_repeats={{config.params.extra.n_samples}}
      --seed={{config.params.extra.seed}} --judge_model={{config.params.extra.judge.model_id}}
      --judge_url={{config.params.extra.judge.url}} --judge_temperature={{config.params.extra.judge.temperature}}
      --judge_top_p={{config.params.extra.judge.top_p}} --judge_max_new_tokens={{config.params.extra.judge.max_new_tokens}}
      --judge_async_limit={{config.params.extra.judge.parallelism}} {% if config.params.extra.judge.api_key
      is defined %}--judge_api_key_name={{config.params.extra.judge.api_key}}{% endif
      %}
    framework_name: AA-LCR
    pkg_name: aa_lcr
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        temperature: 0.0
        request_timeout: 600
        top_p: 1.0
        extra:
          n_samples: 3
          seed: 42
          judge:
            url: https://integrate.api.nvidia.com/v1/chat/completions
            model_id: nvdev/qwen/qwen-235b
            request_timeout: 600
            max_retries: 30
            temperature: 0.0
            top_p: 1.0
            max_new_tokens: 1024
            parallelism: 10
            api_key: JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: aa_lcr
    target:
      api_endpoint: {}
