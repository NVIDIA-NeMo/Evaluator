metadata:
  mapping_toml_checksum: sha256:8dd63e7a829ef6dc57c961b3728a49624a4c5faa168b850c451a6bfa41910dc8
  num_tasks: 306
tasks:
- name: mmlu
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark covers
    57 subjects across various fields, testing both world knowledge and problem-solving
    abilities. - This variant uses text generation.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_str
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
          args: --trust_remote_code
      supported_endpoint_types:
      - completions
      type: mmlu
    target:
      api_endpoint:
        stream: false
- name: mmlu_instruct
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark covers
    57 subjects across various fields, testing both world knowledge and problem-solving
    abilities. - This variant defaults to zero-shot evaluation and instructs the model
    to produce a single letter response.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_str
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
          args: --trust_remote_code --add_instruction
      supported_endpoint_types:
      - chat
      - completions
      type: mmlu_instruct
    target:
      api_endpoint:
        stream: false
- name: mmlu_cot_0_shot_chat
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark covers
    57 subjects across various fields, testing both world knowledge and problem-solving
    abilities. - This variant defaults to chain-of-thought zero-shot evaluation.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_cot_0_shot_chat
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          args: --trust_remote_code
      supported_endpoint_types:
      - chat
      type: mmlu_cot_0_shot_chat
    target:
      api_endpoint:
        stream: false
- name: ifeval
  description: IFEval is a dataset designed to test a model's ability to follow explicit
    instructions, such as "include keyword x" or "use format y." The focus is on the
    model's adherence to formatting instructions rather than the content generated,
    allowing for the use of strict and rigorous metrics.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: ifeval
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: ifeval
    target:
      api_endpoint:
        stream: false
- name: mmlu_pro
  description: MMLU-Pro is a refined version of the MMLU dataset with 10 choices instead
    of 4.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_pro
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - chat
      - completions
      type: mmlu_pro
    target:
      api_endpoint:
        stream: false
- name: mmlu_pro_instruct
  description: '- MMLU-Pro is a refined version of the MMLU dataset with 10 choices
    instead of 4. - This variant applies a chat template and defaults to zero-shot
    evaluation.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: mmlu_pro
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
      supported_endpoint_types:
      - chat
      type: mmlu_pro_instruct
    target:
      api_endpoint:
        stream: false
- name: mmlu_redux
  description: MMLU-Redux is a subset of 3,000 manually re-annotated questions across
    30 MMLU subjects.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_redux
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: mmlu_redux
    target:
      api_endpoint:
        stream: false
- name: mmlu_redux_instruct
  description: '- MMLU-Redux is a subset of 3,000 manually re-annotated questions
    across 30 MMLU subjects. - This variant applies a chat template and defaults to
    zero-shot evaluation.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 8192
        max_retries: 5
        parallelism: 10
        task: mmlu_redux
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
          args: --add_instruction
      supported_endpoint_types:
      - chat
      type: mmlu_redux_instruct
    target:
      api_endpoint:
        stream: false
- name: m_mmlu_id_str
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark translated
    to Indonesian with string-based evaluation.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: m_mmlu_id_str
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
          args: --trust_remote_code
      supported_endpoint_types:
      - chat
      - completions
      type: m_mmlu_id_str
    target:
      api_endpoint:
        stream: false
- name: gsm8k
  description: The GSM8K benchmark evaluates the arithmetic reasoning of large language
    models using 1,319 grade school math word problems.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: gsm8k
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: gsm8k
    target:
      api_endpoint:
        stream: false
- name: gsm8k_cot_instruct
  description: '- The GSM8K benchmark evaluates the arithmetic reasoning of large
    language models using 1,319 grade school math word problems. - This variant defaults
    to chain-of-thought zero-shot evaluation with custom instructions.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: gsm8k_zeroshot_cot
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          args: --add_instruction
      supported_endpoint_types:
      - chat
      type: gsm8k_cot_instruct
    target:
      api_endpoint:
        stream: false
- name: gsm8k_cot_zeroshot
  description: '- The GSM8K benchmark evaluates the arithmetic reasoning of large
    language models using 1,319 grade school math word problems. - This variant defaults
    to chain-of-thought zero-shot evaluation.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: gsm8k_cot_zeroshot
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: gsm8k_cot_zeroshot
    target:
      api_endpoint:
        stream: false
- name: gsm8k_cot_llama
  description: '- The GSM8K benchmark evaluates the arithmetic reasoning of large
    language models using 1,319 grade school math word problems. - This variant defaults
    to chain-of-thought evaluation - implementation taken from llama.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: gsm8k_cot_llama
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: gsm8k_cot_llama
    target:
      api_endpoint:
        stream: false
- name: gsm8k_cot_zeroshot_llama
  description: '- The GSM8K benchmark evaluates the arithmetic reasoning of large
    language models using 1,319 grade school math word problems. - This variant defaults
    to chain-of-thought zero-shot evaluation - implementation taken from llama.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: gsm8k_cot_llama
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
      supported_endpoint_types:
      - chat
      type: gsm8k_cot_zeroshot_llama
    target:
      api_endpoint:
        stream: false
- name: humaneval_instruct
  description: '- The HumanEval benchmark measures functional correctness for synthesizing
    programs from docstrings. - Implementation taken from llama.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: humaneval_instruct
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: humaneval_instruct
    target:
      api_endpoint:
        stream: false
- name: mbpp_plus
  description: MBPP EvalPlus is an extension of the MBPP benchmark with 35x more test
    cases.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mbpp_plus
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      - completions
      type: mbpp_plus
    target:
      api_endpoint:
        stream: false
- name: mgsm
  description: '- The Multilingual Grade School Math (MGSM) benchmark consists of
    250 grade-school math problems from the GSM8K dataset, translated into ten languages.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mgsm_direct
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: mgsm
    target:
      api_endpoint:
        stream: false
- name: mgsm_cot
  description: '- The Multilingual Grade School Math (MGSM) benchmark consists of
    250 grade-school math problems from the GSM8K dataset, translated into ten languages.
    - This variant defaults to chain-of-thought evaluation.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: mgsm_cot_native
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
      supported_endpoint_types:
      - chat
      - completions
      type: mgsm_cot
    target:
      api_endpoint:
        stream: false
- name: wikilingua
  description: '- The WikiLingua benchmark is a large-scale, multilingual dataset
    designed for evaluating cross-lingual abstractive summarization systems.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: wikilingua
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          args: --trust_remote_code
      supported_endpoint_types:
      - chat
      type: wikilingua
    target:
      api_endpoint:
        stream: false
- name: winogrande
  description: WinoGrande is a collection of 44k problems formulated as a fill-in-a-blank
    task with binary options testing commonsense reasoning.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: winogrande
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: winogrande
    target:
      api_endpoint:
        stream: false
- name: arc_challenge
  description: The ARC challenge dataset consists of 2,590 multiple-choice science
    exam questions.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: arc_challenge
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: arc_challenge
    target:
      api_endpoint:
        stream: false
- name: arc_challenge_chat
  description: '- The ARC challenge dataset consists of 2,590 multiple-choice science
    exam questions. - This variant applies a chat template and defaults to zero-shot
    evaluation.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: arc_challenge_chat
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 0
      supported_endpoint_types:
      - chat
      type: arc_challenge_chat
    target:
      api_endpoint:
        stream: false
- name: hellaswag
  description: The HellaSwag benchmark tests a language model's commonsense reasoning
    by having it choose the most logical ending for a given story.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: hellaswag
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 10
      supported_endpoint_types:
      - completions
      type: hellaswag
    target:
      api_endpoint:
        stream: false
- name: truthfulqa
  description: '- The TruthfulQA benchmark measures the truthfulness of language models
    in generating answers to questions. - It consists of 817 questions across 38 categories,
    such as health, law, finance, and politics, designed to test whether models can
    avoid generating false answers that mimic common human misconceptions.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: truthfulqa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      type: truthfulqa
    target:
      api_endpoint:
        stream: false
- name: bbh
  description: The BIG-Bench Hard (BBH) benchmark is a part of the BIG-Bench evaluation
    suite, focusing on 23 particularly difficult tasks that current language models
    struggle with.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: leaderboard_bbh
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: bbh
    target:
      api_endpoint:
        stream: false
- name: bbh_instruct
  description: '- The BIG-Bench Hard (BBH) benchmark is a part of the BIG-Bench evaluation
    suite, focusing on 23 particularly difficult tasks that current language models
    struggle with. - This variant aaplies chat template and defaults to zero-shot
    evaluation.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: bbh_zeroshot
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: bbh_instruct
    target:
      api_endpoint:
        stream: false
- name: musr
  description: The MuSR (Multistep Soft Reasoning) benchmark evaluates the reasoning
    capabilities of large language models through complex, multistep tasks specified
    in natural language narratives.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: leaderboard_musr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: musr
    target:
      api_endpoint:
        stream: false
- name: gpqa
  description: The GPQA (Graduate-Level Google-Proof Q&A) benchmark is a challenging
    dataset of 448 multiple-choice questions in biology, physics, and chemistry.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: leaderboard_gpqa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: gpqa
    target:
      api_endpoint:
        stream: false
- name: gpqa_diamond_cot
  description: '- The GPQA (Graduate-Level Google-Proof Q&A) benchmark is a challenging
    dataset of 448 multiple-choice questions in biology, physics, and chemistry. -
    This variant uses the Diamond subset and defaults to zero-shot chain-of-thought
    evaluation.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: gpqa_diamond_cot_zeroshot
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: gpqa_diamond_cot
    target:
      api_endpoint:
        stream: false
- name: frames_naive
  description: Frames Naive uses the prompt as input without additional context
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 10
        task: frames_naive
        temperature: 0.0
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: frames_naive
    target:
      api_endpoint:
        stream: false
- name: frames_naive_with_links
  description: Frames Naive with Links provides the prompt and relevant Wikipedia
    article links
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 10
        task: frames_naive_with_links
        temperature: 0.0
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: frames_naive_with_links
    target:
      api_endpoint:
        stream: false
- name: frames_oracle
  description: Frames Oracle (long context) provides prompts and relevant text from
    curated and processed Wikipedia articles from "parasail-ai/frames-benchmark-wikipedia".
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 10
        task: frames_oracle
        temperature: 0.0
        request_timeout: 1000
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      type: frames_oracle
    target:
      api_endpoint:
        stream: false
- name: commonsense_qa
  description: '- CommonsenseQA is a multiple-choice question answering dataset that
    requires different types of commonsense knowledge to predict the correct answers.
    - It contains 12,102 questions with one correct answer and four distractor answers.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: commonsense_qa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 7
      supported_endpoint_types:
      - completions
      type: commonsense_qa
    target:
      api_endpoint:
        stream: false
- name: openbookqa
  description: '- OpenBookQA is a question-answering dataset modeled after open book
    exams for assessing human understanding of a subject. - Answering OpenBookQA questions
    requires additional broad common knowledge, not contained in the book. - The questions,
    by design, are answered incorrectly by both a retrieval-based algorithm and a
    word co-occurrence algorithm.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: openbookqa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: openbookqa
    target:
      api_endpoint:
        stream: false
- name: mmlu_logits
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark covers
    57 subjects across various fields, testing both world knowledge and problem-solving
    abilities. - This variant uses the logits of the model to evaluate the accuracy.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: mmlu_logits
    target:
      api_endpoint:
        stream: false
- name: piqa
  description: '- Physical Interaction: Question Answering (PIQA) is a physical commonsense
    reasoning benchmark designed to investigate the physical knowledge of large language
    models.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: piqa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: piqa
    target:
      api_endpoint:
        stream: false
- name: social_iqa
  description: '- Social IQa contains 38,000 multiple choice questions for probing
    emotional and social intelligence in a variety of everyday situations.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: social_iqa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          args: --trust_remote_code
      supported_endpoint_types:
      - completions
      type: social_iqa
    target:
      api_endpoint:
        stream: false
- name: adlr_agieval_en_cot
  description: Version of the AGIEval-EN-CoT benchmark used by NVIDIA Applied Deep
    Learning Research team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_agieval_en_cot
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: adlr_agieval_en_cot
    target:
      api_endpoint:
        stream: false
- name: adlr_math_500_4_shot_sampled
  description: MATH-500 Sampled version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_math_500_4_shot_sampled
        temperature: 0.7
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 4
      supported_endpoint_types:
      - completions
      type: adlr_math_500_4_shot_sampled
    target:
      api_endpoint:
        stream: false
- name: adlr_race
  description: RACE version used by NVIDIA Applied Deep Learning Research team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_race
        temperature: 1.0
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: adlr_race
    target:
      api_endpoint:
        stream: false
- name: adlr_truthfulqa_mc2
  description: TruthfulQA-MC2 version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_truthfulqa_mc2
        temperature: 1.0
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: adlr_truthfulqa_mc2
    target:
      api_endpoint:
        stream: false
- name: adlr_arc_challenge_llama_25_shot
  description: ARC-Challenge-Llama version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_arc_challenge_llama
        temperature: 1.0
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 25
      supported_endpoint_types:
      - completions
      type: adlr_arc_challenge_llama_25_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_gpqa_diamond_cot_5_shot
  description: Version of the GPQA-Diamond-CoT benchmark used by NVIDIA Applied Deep
    Learning Research team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_gpqa_diamond_cot_5_shot
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: adlr_gpqa_diamond_cot_5_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_mmlu
  description: MMLU version used by NVIDIA Applied Deep Learning Research team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_str
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
          args: --trust_remote_code
      supported_endpoint_types:
      - completions
      type: adlr_mmlu
    target:
      api_endpoint:
        stream: false
- name: adlr_mmlu_pro_5_shot_base
  description: MMLU-Pro 5-shot base version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_mmlu_pro_5_shot_base
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: adlr_mmlu_pro_5_shot_base
    target:
      api_endpoint:
        stream: false
- name: adlr_minerva_math_nemo_4_shot
  description: Minerva-Math version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_minerva_math_nemo
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 4
      supported_endpoint_types:
      - completions
      type: adlr_minerva_math_nemo_4_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_gsm8k_cot_8_shot
  description: GSM8K-CoT version used by NVIDIA Applied Deep Learning Research team
    (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_gsm8k_fewshot_cot
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 8
      supported_endpoint_types:
      - completions
      type: adlr_gsm8k_cot_8_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_humaneval_greedy
  description: HumanEval Greedy version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_humaneval_greedy
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: adlr_humaneval_greedy
    target:
      api_endpoint:
        stream: false
- name: adlr_humaneval_sampled
  description: HumanEval Sampled version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_humaneval_sampled
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: adlr_humaneval_sampled
    target:
      api_endpoint:
        stream: false
- name: adlr_mbpp_sanitized_3_shot_greedy
  description: MBPP Greedy version used by NVIDIA Applied Deep Learning Research team
    (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_mbpp_sanitized_3_shot_greedy
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 3
      supported_endpoint_types:
      - completions
      type: adlr_mbpp_sanitized_3_shot_greedy
    target:
      api_endpoint:
        stream: false
- name: adlr_mbpp_sanitized_3_shot_sampled
  description: MBPP Sampled version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_mbpp_sanitized_3shot_sampled
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 3
      supported_endpoint_types:
      - completions
      type: adlr_mbpp_sanitized_3_shot_sampled
    target:
      api_endpoint:
        stream: false
- name: adlr_global_mmlu_lite_5_shot
  description: Global-MMLU subset (8 languages - es, de, fr, zh, it, ja, pt, ko) used
    by NVIDIA Applied Deep Learning Research team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_global_mmlu
        temperature: 1.0
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: adlr_global_mmlu_lite_5_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_mgsm_native_cot_8_shot
  description: MGSM native CoT subset (6 languages - es, de, fr, zh, ja, ru) used
    by NVIDIA Applied Deep Learning Research team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: adlr_mgsm_native_cot_8_shot
        temperature: 0.0
        request_timeout: 30
        top_p: 1.0e-05
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 8
      supported_endpoint_types:
      - completions
      type: adlr_mgsm_native_cot_8_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_commonsense_qa_7_shot
  description: CommonsenseQA version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: commonsense_qa
        temperature: 1.0
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 7
      supported_endpoint_types:
      - completions
      type: adlr_commonsense_qa_7_shot
    target:
      api_endpoint:
        stream: false
- name: adlr_winogrande_5_shot
  description: Winogrande version used by NVIDIA Applied Deep Learning Research team
    (ADLR).
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: winogrande
        temperature: 1.0
        request_timeout: 30
        top_p: 1.0
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 5
      supported_endpoint_types:
      - completions
      type: adlr_winogrande_5_shot
    target:
      api_endpoint:
        stream: false
- name: bbq
  description: The BBQ (Bias Benchmark for QA) is a benchmark designed to measure
    social biases in question answering systems. It contains ambiguous questions spanning
    9 categories - disability, gender, nationality, physical appearance, race/ethnicity,
    religion, sexual orientation, socioeconomic status, and age.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: bbq_generate
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      - completions
      type: bbq
    target:
      api_endpoint:
        stream: false
- name: arc_multilingual
  description: The multilingual versions of the ARC challenge dataset.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: arc_multilingual
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: arc_multilingual
    target:
      api_endpoint:
        stream: false
- name: hellaswag_multilingual
  description: The multilingual versions of the HellaSwag benchmark.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: hellaswag_multilingual
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
          num_fewshot: 10
      supported_endpoint_types:
      - completions
      type: hellaswag_multilingual
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      - completions
      type: mmlu_prox
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_fr
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    (French dataset)
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_fr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      - completions
      type: mmlu_prox_fr
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_de
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    (German dataset)
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_de
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      - completions
      type: mmlu_prox_de
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_it
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    (Italian dataset)
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_it
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      - completions
      type: mmlu_prox_it
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_ja
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    (Japanese dataset)
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_ja
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      - completions
      type: mmlu_prox_ja
    target:
      api_endpoint:
        stream: false
- name: mmlu_prox_es
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    (Spanish dataset)
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: mmlu_prox_es
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - chat
      - completions
      type: mmlu_prox_es
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full
  description: Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English.
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_am
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the AM subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_am
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_am
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ar
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the AR subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ar
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ar
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_bn
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the BN subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_bn
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_bn
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_cs
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the CS subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_cs
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_cs
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_de
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the DE subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_de
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_de
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_el
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the EL subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_el
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_el
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_en
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the EN subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_en
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_en
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_es
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the ES subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_es
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_es
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_fa
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the FA subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_fa
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_fa
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_fil
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the FIL subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_fil
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_fil
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_fr
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the FR subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_fr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_fr
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ha
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the HA subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ha
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ha
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_he
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the HE subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_he
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_he
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_hi
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the HI subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_hi
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_hi
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_id
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the ID subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_id
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_id
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ig
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the IG subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ig
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ig
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_it
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the IT subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_it
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_it
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ja
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the JA subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ja
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ja
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ko
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the KO subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ko
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ko
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ky
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the KY subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ky
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ky
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_lt
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the LT subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_lt
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_lt
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_mg
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the MG subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_mg
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_mg
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ms
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the MS subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ms
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ms
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ne
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the NE subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ne
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ne
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_nl
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the NL subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_nl
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_nl
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ny
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the NY subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ny
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ny
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_pl
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the PL subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_pl
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_pl
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_pt
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the PT subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_pt
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_pt
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ro
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the RO subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ro
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ro
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_ru
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the RU subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_ru
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_ru
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_si
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the SI subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_si
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_si
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_sn
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the SN subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_sn
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_sn
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_so
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the SO subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_so
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_so
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_sr
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the SR subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_sr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_sr
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_sv
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the SV subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_sv
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_sv
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_sw
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the SW subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_sw
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_sw
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_te
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the TE subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_te
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_te
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_tr
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the TR subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_tr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_tr
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_uk
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the UK subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_uk
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_uk
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_vi
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the VI subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_vi
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_vi
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_yo
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the YO subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_yo
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_yo
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_full_zh
  description: '- Global-MMLU is a multilingual evaluation set spanning 42 languages,
    including English. - This variant uses the ZH subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_full_zh
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_full_zh
    target:
      api_endpoint:
        stream: false
- name: global_mmlu
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - It is designed for efficient evaluation
    of multilingual models in 15 languages (including English).'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_ar
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the AR subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_ar
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_ar
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_bn
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the BN subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_bn
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_bn
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_de
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the DE subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_de
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_de
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_en
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the EN subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_en
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_en
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_es
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the ES subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_es
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_es
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_fr
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the FR subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_fr
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_fr
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_hi
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the HI subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_hi
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_hi
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_id
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the ID subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_id
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_id
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_it
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the IT subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_it
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_it
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_ja
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the JA subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_ja
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_ja
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_ko
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the KO subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_ko
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_ko
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_pt
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the PT subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_pt
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_pt
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_sw
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the SW subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_sw
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_sw
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_yo
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the YO subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_yo
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_yo
    target:
      api_endpoint:
        stream: false
- name: global_mmlu_zh
  description: '- Global-MMLU-Lite is a balanced collection of culturally sensitive
    and culturally agnostic MMLU tasks. - This variant uses the ZH subset.'
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: global_mmlu_zh
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: global_mmlu_zh
    target:
      api_endpoint:
        stream: false
- name: agieval
  description: AGIEval - A Human-Centric Benchmark for Evaluating Foundation Models
  harness: lm-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/lm-evaluation-harness:25.11
  container_digest: sha256:e14fe5dfef7b4a3d4b307dc8e23381ac342516a43f822f54159555d574e9b5d7
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
      is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %}
      --model {% if target.api_endpoint.type == "completions" %}local-completions{%
      elif target.api_endpoint.type == "chat" %}local-chat-completions{% endif %}
      --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
      if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
      endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
      config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
      target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
      --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
      == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
      is not none or config.params.top_p is not none or config.params.max_new_tokens
      is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
      config.params.temperature }}{% endif %}{% if config.params.top_p is not none
      %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens
      is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{%
      endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio
      {{ config.params.extra.downsampling_ratio }}{% endif %}'
    framework_name: lm-evaluation-harness
    pkg_name: lm_evaluation_harness
    config:
      params:
        max_retries: 5
        parallelism: 10
        task: agieval
        temperature: 1.0e-07
        request_timeout: 30
        top_p: 0.9999999
        extra:
          tokenizer: null
          tokenizer_backend: None
          downsampling_ratio: null
          tokenized_requests: false
      supported_endpoint_types:
      - completions
      type: agieval
    target:
      api_endpoint:
        stream: false
- name: mtbench
  description: Standard MT-Bench
  harness: mtbench
  container: nvcr.io/nvidia/eval-factory/mtbench:25.11
  container_digest: sha256:057cec7f1928b6bf84943f97ee73c184a90497982796878ed9d3945cf7cf2463
  defaults:
    command: 'mtbench-evaluator {% if target.api_endpoint.model_id is not none %}
      --model {{target.api_endpoint.model_id}}{% endif %} {% if target.api_endpoint.url
      is not none %} --url {{target.api_endpoint.url}}{% endif %} {% if target.api_endpoint.api_key
      is not none %} --api_key {{target.api_endpoint.api_key}}{% endif %} {% if config.params.request_timeout
      is not none %} --timeout {{config.params.request_timeout}}{% endif %} {% if
      config.params.max_retries is not none %} --max_retries {{config.params.max_retries}}{%
      endif %} {% if config.params.parallelism is not none %} --parallelism {{config.params.parallelism}}{%
      endif %} {% if config.params.max_new_tokens is not none %} --max_tokens {{config.params.max_new_tokens}}{%
      endif %} --workdir {{config.output_dir}} {% if config.params.temperature is
      not none %} --temperature {{config.params.temperature}}{% endif %} {% if config.params.top_p
      is not none %} --top_p {{config.params.top_p}}{% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.limit_samples
      is not none %}--first_n {{config.params.limit_samples}}{% endif %} --generate
      --judge {% if config.params.extra.judge.url is not none %} --judge_url {{config.params.extra.judge.url}}{%
      endif %} {% if config.params.extra.judge.model_id is not none %} --judge_model
      {{config.params.extra.judge.model_id}}{% endif %} {% if config.params.extra.judge.api_key
      is not none %} --judge_api_key_name {{config.params.extra.judge.api_key}}{%
      endif %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %}     '
    framework_name: mtbench
    pkg_name: mtbench
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: mtbench
        request_timeout: 30
        extra:
          judge:
            url: null
            model_id: gpt-4
            api_key: null
            request_timeout: 60
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 2048
      supported_endpoint_types:
      - chat
      type: mtbench
    target:
      api_endpoint: {}
- name: mtbench-cor1
  description: Corrected MT-Bench
  harness: mtbench
  container: nvcr.io/nvidia/eval-factory/mtbench:25.11
  container_digest: sha256:057cec7f1928b6bf84943f97ee73c184a90497982796878ed9d3945cf7cf2463
  defaults:
    command: 'mtbench-evaluator {% if target.api_endpoint.model_id is not none %}
      --model {{target.api_endpoint.model_id}}{% endif %} {% if target.api_endpoint.url
      is not none %} --url {{target.api_endpoint.url}}{% endif %} {% if target.api_endpoint.api_key
      is not none %} --api_key {{target.api_endpoint.api_key}}{% endif %} {% if config.params.request_timeout
      is not none %} --timeout {{config.params.request_timeout}}{% endif %} {% if
      config.params.max_retries is not none %} --max_retries {{config.params.max_retries}}{%
      endif %} {% if config.params.parallelism is not none %} --parallelism {{config.params.parallelism}}{%
      endif %} {% if config.params.max_new_tokens is not none %} --max_tokens {{config.params.max_new_tokens}}{%
      endif %} --workdir {{config.output_dir}} {% if config.params.temperature is
      not none %} --temperature {{config.params.temperature}}{% endif %} {% if config.params.top_p
      is not none %} --top_p {{config.params.top_p}}{% endif %} {% if config.params.extra.args
      is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.limit_samples
      is not none %}--first_n {{config.params.limit_samples}}{% endif %} --generate
      --judge {% if config.params.extra.judge.url is not none %} --judge_url {{config.params.extra.judge.url}}{%
      endif %} {% if config.params.extra.judge.model_id is not none %} --judge_model
      {{config.params.extra.judge.model_id}}{% endif %} {% if config.params.extra.judge.api_key
      is not none %} --judge_api_key_name {{config.params.extra.judge.api_key}}{%
      endif %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %}     '
    framework_name: mtbench
    pkg_name: mtbench
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: mtbench-cor1
        request_timeout: 30
        extra:
          judge:
            url: null
            model_id: gpt-4
            api_key: null
            request_timeout: 60
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 2048
          args: --judge_reference_model gpt-4-0125-preview
      supported_endpoint_types:
      - chat
      type: mtbench-cor1
    target:
      api_endpoint: {}
- name: ifbench
  description: IFBench with vanilla settings
  harness: ifbench
  container: nvcr.io/nvidia/eval-factory/ifbench:25.11
  container_digest: sha256:e116972dc8ad4b0cfc6bc8fa54a8eaa232c453a69cb1cbe3b4c3b674d68a14f9
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}  &&
      {% endif %} ifbench --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --results-dir
      {{config.output_dir}} --inference-params max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}}
      --parallelism {{config.params.parallelism}} --retries {{config.params.max_retries}}
      {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}
      {% endif %}'
    framework_name: ifbench
    pkg_name: ifbench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 8
        task: ifbench
        temperature: 0.01
        top_p: 0.95
        extra: {}
      supported_endpoint_types:
      - chat
      type: ifbench
    target:
      api_endpoint:
        stream: false
- name: ifbench_aa_v2
  description: IFBench - params aligned with Artificial Analysis Index v2
  harness: ifbench
  container: nvcr.io/nvidia/eval-factory/ifbench:25.11
  container_digest: sha256:e116972dc8ad4b0cfc6bc8fa54a8eaa232c453a69cb1cbe3b4c3b674d68a14f9
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}  &&
      {% endif %} ifbench --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --results-dir
      {{config.output_dir}} --inference-params max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}}
      --parallelism {{config.params.parallelism}} --retries {{config.params.max_retries}}
      {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}
      {% endif %}'
    framework_name: ifbench
    pkg_name: ifbench
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 8
        task: ifbench_aa_v2
        temperature: 0.0
        top_p: 0.95
        extra: {}
      supported_endpoint_types:
      - chat
      type: ifbench_aa_v2
    target:
      api_endpoint:
        stream: false
- name: AIME_2025
  description: AIME 2025 questions, math
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: AIME_2025
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
          n_samples: 10
      supported_endpoint_types:
      - chat
      type: AIME_2025
    target:
      api_endpoint: {}
- name: AIME_2024
  description: AIME 2024 questions, math
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: AIME_2024
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: AIME_2024
    target:
      api_endpoint: {}
- name: AA_AIME_2024
  description: AIME 2024 questions, math, using Artificial Analysis's setup.
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: AA_AIME_2024
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
          n_samples: 10
      supported_endpoint_types:
      - chat
      type: AA_AIME_2024
    target:
      api_endpoint: {}
- name: AA_math_test_500
  description: Open Ai math test 500, using Artificial Analysis's setup.
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: AA_math_test_500
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
          n_samples: 3
      supported_endpoint_types:
      - chat
      type: AA_math_test_500
    target:
      api_endpoint: {}
- name: math_test_500
  description: Open AI math test 500
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: math_test_500
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: math_test_500
    target:
      api_endpoint: {}
- name: mgsm
  description: MGSM is a benchmark of grade-school math problems. The same 250 problems
    from GSM8K are each translated via human annotators in 10 languages.
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mgsm
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mgsm
    target:
      api_endpoint: {}
- name: humaneval
  description: HumanEval evaluates the performance in Python code generation tasks.
    It used to measure functional correctness for synthesizing programs from docstrings.
    It consists of 164 original programming problems, assessing language comprehension,
    algorithms, and simple mathematics, with some comparable to simple software interview
    questions.
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: humaneval
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
          n_samples: 1
      supported_endpoint_types:
      - chat
      type: humaneval
    target:
      api_endpoint: {}
- name: humanevalplus
  description: HumanEvalPlus is a dataset of 164 programming problems, assessing language
    comprehension, algorithms, and simple mathematics, with some comparable to simple
    software interview questions.
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: humanevalplus
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
          n_samples: 1
      supported_endpoint_types:
      - chat
      type: humanevalplus
    target:
      api_endpoint: {}
- name: mmlu_pro
  description: MMLU-Pro dataset is a more robust and challenging massive multi-task
    understanding dataset tailored to more rigorously benchmark large language models'
    capabilities. This dataset contains 12K complex questions across various disciplines.
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_pro
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pro
    target:
      api_endpoint: {}
- name: mmlu_am
  description: Global-MMLU 0-shot CoT in Amharic (am)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_am
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_am
    target:
      api_endpoint: {}
- name: mmlu_ar
  description: Global-MMLU 0-shot CoT in Arabic (ar)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ar
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ar
    target:
      api_endpoint: {}
- name: mmlu_bn
  description: Global-MMLU 0-shot CoT in Bengali (bn)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_bn
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_bn
    target:
      api_endpoint: {}
- name: mmlu_cs
  description: Global-MMLU 0-shot CoT in Czech (cs)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_cs
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_cs
    target:
      api_endpoint: {}
- name: mmlu_de
  description: Global-MMLU 0-shot CoT in German (de)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_de
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_de
    target:
      api_endpoint: {}
- name: mmlu_el
  description: Global-MMLU 0-shot CoT in Greek (el)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_el
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_el
    target:
      api_endpoint: {}
- name: mmlu_en
  description: Global-MMLU 0-shot CoT in English (en)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_en
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_en
    target:
      api_endpoint: {}
- name: mmlu_es
  description: Global-MMLU 0-shot CoT in Spanish (es)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_es
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_es
    target:
      api_endpoint: {}
- name: mmlu_fa
  description: Global-MMLU 0-shot CoT in Persian (fa)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_fa
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_fa
    target:
      api_endpoint: {}
- name: mmlu_fil
  description: Global-MMLU 0-shot CoT in Filipino (fil)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_fil
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_fil
    target:
      api_endpoint: {}
- name: mmlu_fr
  description: Global-MMLU 0-shot CoT in French (fr)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_fr
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_fr
    target:
      api_endpoint: {}
- name: mmlu_ha
  description: Global-MMLU 0-shot CoT in Hausa (ha)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ha
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ha
    target:
      api_endpoint: {}
- name: mmlu_he
  description: Global-MMLU 0-shot CoT in Hebrew (he)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_he
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_he
    target:
      api_endpoint: {}
- name: mmlu_hi
  description: Global-MMLU 0-shot CoT in Hindi (hi)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_hi
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_hi
    target:
      api_endpoint: {}
- name: mmlu_id
  description: Global-MMLU 0-shot CoT in Indonesian (id)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_id
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_id
    target:
      api_endpoint: {}
- name: mmlu_ig
  description: Global-MMLU 0-shot CoT in Igbo (ig)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ig
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ig
    target:
      api_endpoint: {}
- name: mmlu_it
  description: Global-MMLU 0-shot CoT in Italian (it)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_it
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_it
    target:
      api_endpoint: {}
- name: mmlu_ja
  description: Global-MMLU 0-shot CoT in Japanese (ja)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ja
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ja
    target:
      api_endpoint: {}
- name: mmlu_ko
  description: Global-MMLU 0-shot CoT in Korean (ko)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ko
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ko
    target:
      api_endpoint: {}
- name: mmlu_ky
  description: Global-MMLU 0-shot CoT in Kyrgyz (ky)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ky
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ky
    target:
      api_endpoint: {}
- name: mmlu_lt
  description: Global-MMLU 0-shot CoT in Lithuanian (lt)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_lt
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_lt
    target:
      api_endpoint: {}
- name: mmlu_mg
  description: Global-MMLU 0-shot CoT in Malagasy (mg)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_mg
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_mg
    target:
      api_endpoint: {}
- name: mmlu_ms
  description: Global-MMLU 0-shot CoT in Malay (ms)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ms
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ms
    target:
      api_endpoint: {}
- name: mmlu_ne
  description: Global-MMLU 0-shot CoT in Nepali (ne)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ne
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ne
    target:
      api_endpoint: {}
- name: mmlu_nl
  description: Global-MMLU 0-shot CoT in Dutch (nl)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_nl
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_nl
    target:
      api_endpoint: {}
- name: mmlu_ny
  description: Global-MMLU 0-shot CoT in Nyanja (ny)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ny
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ny
    target:
      api_endpoint: {}
- name: mmlu_pl
  description: Global-MMLU 0-shot CoT in Polish (pl)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_pl
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pl
    target:
      api_endpoint: {}
- name: mmlu_pt
  description: Global-MMLU 0-shot CoT in Portuguese (pt)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_pt
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pt
    target:
      api_endpoint: {}
- name: mmlu_ro
  description: Global-MMLU 0-shot CoT in Romanian (ro)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ro
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ro
    target:
      api_endpoint: {}
- name: mmlu_ru
  description: Global-MMLU 0-shot CoT in Russian (ru)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ru
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ru
    target:
      api_endpoint: {}
- name: mmlu_si
  description: Global-MMLU 0-shot CoT in Sinhala (si)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_si
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_si
    target:
      api_endpoint: {}
- name: mmlu_sn
  description: Global-MMLU 0-shot CoT in Shona (sn)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_sn
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_sn
    target:
      api_endpoint: {}
- name: mmlu_so
  description: Global-MMLU 0-shot CoT in Somali (so)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_so
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_so
    target:
      api_endpoint: {}
- name: mmlu_sr
  description: Global-MMLU 0-shot CoT in Serbian (sr)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_sr
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_sr
    target:
      api_endpoint: {}
- name: mmlu_sv
  description: Global-MMLU 0-shot CoT in Swedish (sv)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_sv
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_sv
    target:
      api_endpoint: {}
- name: mmlu_sw
  description: Global-MMLU 0-shot CoT in Swahili (sw)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_sw
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_sw
    target:
      api_endpoint: {}
- name: mmlu_te
  description: Global-MMLU 0-shot CoT in Telugu (te)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_te
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_te
    target:
      api_endpoint: {}
- name: mmlu_tr
  description: Global-MMLU 0-shot CoT in Turkish (tr)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_tr
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_tr
    target:
      api_endpoint: {}
- name: mmlu_uk
  description: Global-MMLU 0-shot CoT in Ukrainian (uk)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_uk
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_uk
    target:
      api_endpoint: {}
- name: mmlu_vi
  description: Global-MMLU 0-shot CoT in Vietnamese (vi)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_vi
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_vi
    target:
      api_endpoint: {}
- name: mmlu_yo
  description: Global-MMLU 0-shot CoT in Yoruba (yo)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_yo
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_yo
    target:
      api_endpoint: {}
- name: mmlu_ar-lite
  description: Global-MMLU-Lite 0-shot CoT in Arabic (ar)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ar-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ar-lite
    target:
      api_endpoint: {}
- name: mmlu_bn-lite
  description: Global-MMLU-Lite 0-shot CoT in Bengali (bn)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_bn-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_bn-lite
    target:
      api_endpoint: {}
- name: mmlu_de-lite
  description: Global-MMLU-Lite 0-shot CoT in German (de)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_de-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_de-lite
    target:
      api_endpoint: {}
- name: mmlu_en-lite
  description: Global-MMLU-Lite 0-shot CoT in English (en)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_en-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_en-lite
    target:
      api_endpoint: {}
- name: mmlu_es-lite
  description: Global-MMLU-Lite 0-shot CoT in Spanish (es)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_es-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_es-lite
    target:
      api_endpoint: {}
- name: mmlu_fr-lite
  description: Global-MMLU-Lite 0-shot CoT in French (fr)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_fr-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_fr-lite
    target:
      api_endpoint: {}
- name: mmlu_hi-lite
  description: Global-MMLU-Lite 0-shot CoT in Hindi (hi)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_hi-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_hi-lite
    target:
      api_endpoint: {}
- name: mmlu_id-lite
  description: Global-MMLU-Lite 0-shot CoT in Indonesian (id)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_id-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_id-lite
    target:
      api_endpoint: {}
- name: mmlu_it-lite
  description: Global-MMLU-Lite 0-shot CoT in Italian (it)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_it-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_it-lite
    target:
      api_endpoint: {}
- name: mmlu_ja-lite
  description: Global-MMLU-Lite 0-shot CoT in Japanese (ja)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ja-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ja-lite
    target:
      api_endpoint: {}
- name: mmlu_ko-lite
  description: Global-MMLU-Lite 0-shot CoT in Korean (ko)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_ko-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_ko-lite
    target:
      api_endpoint: {}
- name: mmlu_my-lite
  description: Global-MMLU-Lite 0-shot CoT in Malay (my)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_my-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_my-lite
    target:
      api_endpoint: {}
- name: mmlu_pt-lite
  description: Global-MMLU-Lite 0-shot CoT in Portuguese (pt)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_pt-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pt-lite
    target:
      api_endpoint: {}
- name: mmlu_sw-lite
  description: Global-MMLU-Lite 0-shot CoT in Swahili (sw)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_sw-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_sw-lite
    target:
      api_endpoint: {}
- name: mmlu_yo-lite
  description: Global-MMLU-Lite 0-shot CoT in Yoruba (yo)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_yo-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_yo-lite
    target:
      api_endpoint: {}
- name: mmlu_zh-lite
  description: Global-MMLU-Lite 0-shot CoT in Chinese (Simplified) (zh)
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_zh-lite
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_zh-lite
    target:
      api_endpoint: {}
- name: mmlu
  description: MMLU 0-shot CoT
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu
    target:
      api_endpoint: {}
- name: gpqa_diamond
  description: gpqa_diamond 0-shot CoT
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: gpqa_diamond
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: gpqa_diamond
    target:
      api_endpoint: {}
- name: gpqa_extended
  description: gpqa_extended 0-shot CoT
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: gpqa_extended
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: gpqa_extended
    target:
      api_endpoint: {}
- name: gpqa_main
  description: gpqa_main 0-shot CoT
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: gpqa_main
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: gpqa_main
    target:
      api_endpoint: {}
- name: simpleqa
  description: A factuality benchmark called SimpleQA that measures the ability for
    language models to answer short, fact-seeking questions.
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: simpleqa
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: simpleqa
    target:
      api_endpoint: {}
- name: aime_2025_nemo
  description: AIME 2025 questions, math, using NeMo's alignment template
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: aime_2025_nemo
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
          n_samples: 10
      supported_endpoint_types:
      - chat
      type: aime_2025_nemo
    target:
      api_endpoint: {}
- name: aime_2024_nemo
  description: AIME 2024 questions, math, using NeMo's alignment template
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: aime_2024_nemo
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
          n_samples: 10
      supported_endpoint_types:
      - chat
      type: aime_2024_nemo
    target:
      api_endpoint: {}
- name: math_test_500_nemo
  description: math_test_500 questions, math, using NeMo's alignment template
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: math_test_500_nemo
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
          n_samples: 3
      supported_endpoint_types:
      - chat
      type: math_test_500_nemo
    target:
      api_endpoint: {}
- name: gpqa_diamond_nemo
  description: gpqa_diamond questions, reasoning, using NeMo's alignment template
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: gpqa_diamond_nemo
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
          n_samples: 5
      supported_endpoint_types:
      - chat
      type: gpqa_diamond_nemo
    target:
      api_endpoint: {}
- name: gpqa_diamond_aa_v2_llama_4
  description: gpqa_diamond questions with custom regex extraction patterns for Llama
    4
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: gpqa_diamond
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config:
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_colon_llama4
            - regex: (?i)(?:the )?best? answer is\s*[\*\_,{}\.]*([A-D])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_is_llama4
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
          n_samples: 5
      supported_endpoint_types:
      - chat
      type: gpqa_diamond_aa_v2_llama_4
    target:
      api_endpoint: {}
- name: gpqa_diamond_aa_v2
  description: gpqa_diamond questions with custom regex extraction patterns for AA
    v2
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: gpqa_diamond
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config:
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: aa_v2_regex
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
          n_samples: 5
      supported_endpoint_types:
      - chat
      type: gpqa_diamond_aa_v2
    target:
      api_endpoint: {}
- name: AIME_2025_aa_v2
  description: AIME 2025 questions, math - params aligned with Artificial Analysis
    Index v2
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: AIME_2025
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
          n_samples: 10
      supported_endpoint_types:
      - chat
      type: AIME_2025_aa_v2
    target:
      api_endpoint: {}
- name: mgsm_aa_v2
  description: MGSM is a benchmark of grade-school math problems - params aligned
    with Artificial Analysis Index v2
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: mgsm
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mgsm_aa_v2
    target:
      api_endpoint: {}
- name: mmlu_pro_aa_v2
  description: MMLU-Pro - params aligned with Artificial Analysis Index v2
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: mmlu_pro
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pro_aa_v2
    target:
      api_endpoint: {}
- name: mmlu_llama_4
  description: MMLU questions with custom regex extraction patterns for Llama 4
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config:
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_colon_llama4
            - regex: (?i)(?:the )?best? answer is\s*[\*\_,{}\.]*([A-D])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_is_llama4
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_llama_4
    target:
      api_endpoint: {}
- name: mmlu_pro_llama_4
  description: MMLU-Pro questions with custom regex extraction patterns for Llama
    4
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: mmlu_pro
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config:
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_colon_llama4
            - regex: (?i)(?:the )?best? answer is\s*[\*\_,{}\.]*([A-D])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_is_llama4
          judge:
            url: null
            model_id: null
            api_key: null
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: mmlu_pro_llama_4
    target:
      api_endpoint: {}
- name: healthbench
  description: HealthBench is an open-source benchmark measuring the performance and
    safety of large language models in healthcare.
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: healthbench
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: healthbench
    target:
      api_endpoint: {}
- name: healthbench_consensus
  description: HealthBench is an open-source benchmark measuring the performance and
    safety of large language models in healthcare. The consensus subset measures 34
    particularly important aspects of model behavior and has been validated by the
    consensus of multiple physicians.
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: healthbench_consensus
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: healthbench_consensus
    target:
      api_endpoint: {}
- name: healthbench_hard
  description: HealthBench is an open-source benchmark measuring the performance and
    safety of large language models in healthcare. The hard subset consists of 1000
    examples chosen because they are difficult for current frontier models.
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: healthbench_hard
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: healthbench_hard
    target:
      api_endpoint: {}
- name: browsecomp
  description: BrowseComp is a benchmark for measuring the ability for agents to browse
    the web.
  harness: simple-evals
  container: nvcr.io/nvidia/eval-factory/simple-evals:25.11
  container_digest: sha256:b094b89f5a7258820ca23ef7020e5b1484a9b27f8c5cd9636d0479c049f0ba1d
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
      | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
      "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
      "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
      --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
      {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
      --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
      --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
      --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
      is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {%
      if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt
      {% endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
      {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
      is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
      is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
      config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
      endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
      {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
      is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif
      %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
      {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
      is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
      endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
      {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
      is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %}
      {% if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens
      {{config.params.extra.judge.max_tokens}}{% endif %} {% if config.params.extra.judge.max_concurrent_requests
      is not none %} --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
      endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
      is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
      endif %}'
    framework_name: simple_evals
    pkg_name: simple_evals
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: browsecomp
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          downsampling_ratio: null
          add_system_prompt: false
          custom_config: null
          judge:
            url: null
            model_id: null
            api_key: JUDGE_API_KEY
            backend: openai
            request_timeout: 600
            max_retries: 16
            temperature: 0.0
            top_p: 0.0001
            max_tokens: 1024
            max_concurrent_requests: null
      supported_endpoint_types:
      - chat
      type: browsecomp
    target:
      api_endpoint: {}
- name: humaneval
  description: HumanEval is used to measure functional correctness for synthesizing
    programs from docstrings. It consists of 164 original programming problems, assessing
    language comprehension, algorithms, and simple mathematics, with some comparable
    to simple software interview questions.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: humaneval
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 20
      supported_endpoint_types:
      - completions
      type: humaneval
    target:
      api_endpoint: {}
- name: humaneval_instruct
  description: InstructHumanEval is a modified version of OpenAI HumanEval. For a
    given prompt, we extracted its signature, its docstring as well as its header
    to create a flexing setting which would allow to evaluation instruction-tuned
    LLM. The delimiters used in the instruction-tuning procedure can be use to build
    and instruction that would allow the model to elicit its best capabilities.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: instruct-humaneval-nocontext-py
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 20
      supported_endpoint_types:
      - chat
      type: humaneval_instruct
    target:
      api_endpoint: {}
- name: humanevalplus
  description: HumanEvalPlus is a modified version of HumanEval containing 80x more
    test cases.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: humanevalplus
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: humanevalplus
    target:
      api_endpoint: {}
- name: mbpp
  description: MBPP consists of Python programming problems, designed to be solvable
    by entry level programmers, covering programming fundamentals, standard library
    functionality, and so on. Each problem consists of a task description, code solution
    and 3 automated test cases.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 10
        task: mbpp
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 10
      supported_endpoint_types:
      - completions
      - chat
      type: mbpp
    target:
      api_endpoint: {}
- name: mbppplus
  description: MBPP+ is a modified version of MBPP containing 35x more test cases.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 10
        task: mbppplus
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      - chat
      type: mbppplus
    target:
      api_endpoint: {}
- name: mbppplus_nemo
  description: MBPP+NeMo is a modified version of MBPP+ that uses the NeMo alignment
    prompt template.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 10
        task: mbppplus_nemo
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - chat
      type: mbppplus_nemo
    target:
      api_endpoint: {}
- name: multiple-py
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "py" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-py
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-py
    target:
      api_endpoint: {}
- name: multiple-sh
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "sh" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-sh
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-sh
    target:
      api_endpoint: {}
- name: multiple-cpp
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "cpp" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-cpp
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-cpp
    target:
      api_endpoint: {}
- name: multiple-cs
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "cs" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-cs
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-cs
    target:
      api_endpoint: {}
- name: multiple-d
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "d" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-d
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-d
    target:
      api_endpoint: {}
- name: multiple-go
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "go" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-go
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-go
    target:
      api_endpoint: {}
- name: multiple-java
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "java" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-java
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-java
    target:
      api_endpoint: {}
- name: multiple-js
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "js" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-js
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-js
    target:
      api_endpoint: {}
- name: multiple-jl
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "jl" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-jl
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-jl
    target:
      api_endpoint: {}
- name: multiple-lua
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "lua" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-lua
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-lua
    target:
      api_endpoint: {}
- name: multiple-pl
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "pl" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-pl
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-pl
    target:
      api_endpoint: {}
- name: multiple-php
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "php" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-php
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-php
    target:
      api_endpoint: {}
- name: multiple-r
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "r" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-r
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-r
    target:
      api_endpoint: {}
- name: multiple-rkt
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "rkt" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-rkt
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-rkt
    target:
      api_endpoint: {}
- name: multiple-rb
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "rb" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-rb
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-rb
    target:
      api_endpoint: {}
- name: multiple-rs
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "rs" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-rs
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-rs
    target:
      api_endpoint: {}
- name: multiple-scala
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "scala" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-scala
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-scala
    target:
      api_endpoint: {}
- name: multiple-swift
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "swift" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-swift
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-swift
    target:
      api_endpoint: {}
- name: multiple-ts
  description: MultiPL-E is a suite of coding tasks for many programming languages.
    This task covers the "ts" subset.
  harness: bigcode-evaluation-harness
  container: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness:25.11
  container_digest: sha256:fc34efe95cb18377b697a6e9a87ae8706dba37a1c7f690a58d22c52bfacf23d1
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
      endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
      %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
      {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
      "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
      --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
      --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
      is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
      --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
      --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
      if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
      %}'
    framework_name: bigcode-evaluation-harness
    pkg_name: bigcode_evaluation_harness
    config:
      params:
        max_new_tokens: 1024
        max_retries: 5
        parallelism: 10
        task: multiple-ts
        temperature: 0.1
        request_timeout: 30
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
      supported_endpoint_types:
      - completions
      type: multiple-ts
    target:
      api_endpoint: {}
- name: codegeneration_release_latest
  description: Code generation latest version
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_latest
      supported_endpoint_types:
      - chat
      type: codegeneration_release_latest
    target:
      api_endpoint: {}
- name: codegeneration_release_v1
  description: The initial release of the dataset (v1) with problems released between
    May 2023 and Mar 2024 containing 400 problems.
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v1
      supported_endpoint_types:
      - chat
      type: codegeneration_release_v1
    target:
      api_endpoint: {}
- name: codegeneration_release_v2
  description: The updated release of the dataset (v2) with problems released between
    May 2023 and May 2024 containing 511 problems.
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v2
      supported_endpoint_types:
      - chat
      type: codegeneration_release_v2
    target:
      api_endpoint: {}
- name: codegeneration_release_v3
  description: The updated release of the dataset (v3) with problems released between
    May 2023 and Jul 2024 containing 612 problems.
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v3
      supported_endpoint_types:
      - chat
      type: codegeneration_release_v3
    target:
      api_endpoint: {}
- name: codegeneration_release_v4
  description: The updated release of the dataset (v4) with problems released between
    May 2023 and Sep 2024 containing 713 problems.
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v4
      supported_endpoint_types:
      - chat
      type: codegeneration_release_v4
    target:
      api_endpoint: {}
- name: codegeneration_release_v5
  description: The updated release of the dataset (v5) with problems released between
    May 2023 and Jan 2025 containing 880 problems.
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v5
      supported_endpoint_types:
      - chat
      type: codegeneration_release_v5
    target:
      api_endpoint: {}
- name: codegeneration_release_v6
  description: The updated release of the dataset (v6) with problems released between
    May 2023 and Apr 2025 containing 1055 problems.
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v6
      supported_endpoint_types:
      - chat
      type: codegeneration_release_v6
    target:
      api_endpoint: {}
- name: codegeneration_notfast
  description: Not fast version of code generation (v2).
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          args: --not_fast
      supported_endpoint_types:
      - chat
      type: codegeneration_notfast
    target:
      api_endpoint: {}
- name: testoutputprediction
  description: Solve the natural language task on a specified input, evaluating the
    ability to generate testing outputs. The model is given the natural language problem
    description and an input, and the output should be the output for the problem.
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: testoutputprediction
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_latest
      supported_endpoint_types:
      - chat
      type: testoutputprediction
    target:
      api_endpoint: {}
- name: codeexecution_v2
  description: "\u201CExecute\u201D a program on an input, evaluating code comprehension\
    \ ability. The model is given a program and an input, and the output should be\
    \ the result."
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codeexecution
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: false
          release_version: release_v2
      supported_endpoint_types:
      - chat
      type: codeexecution_v2
    target:
      api_endpoint: {}
- name: codeexecution_v2_cot
  description: "\u201CCoT. Execute\u201D a program on an input, evaluating code comprehension\
    \ ability. The model is given a program and an input, and the output should be\
    \ the result. Chain-of-Thought version of the task."
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codeexecution
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 10
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: null
          end_date: null
          cot_code_execution: true
          release_version: release_v2
      supported_endpoint_types:
      - chat
      type: codeexecution_v2_cot
    target:
      api_endpoint: {}
- name: livecodebench_0724_0125
  description: '- Code generation evaluating code comprehension ability. The model
    is given a program and an input, and the output should be the result. - The data
    period and sampling parameters used by Artificial Analaysis (https://artificialanalysis.ai/methodology/intelligence-benchmarking)'
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 3
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: 2024-07-01
          end_date: 2025-01-01
          cot_code_execution: false
          release_version: release_v5
      supported_endpoint_types:
      - chat
      type: livecodebench_0724_0125
    target:
      api_endpoint: {}
- name: livecodebench_aa_v2
  description: '- Code generation evaluating code comprehension ability. The model
    is given a program and an input, and the output should be the result. - The data
    period and sampling parameters used by Artificial Analaysis (https://artificialanalysis.ai/methodology/intelligence-benchmarking)'
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 3
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: 2024-07-01
          end_date: 2025-01-01
          cot_code_execution: false
          release_version: release_v5
      supported_endpoint_types:
      - chat
      type: livecodebench_aa_v2
    target:
      api_endpoint: {}
- name: livecodebench_0824_0225
  description:
  - Code generation evaluating code comprehension ability. The model is given a program
    and an input, and the output should be the result.
  - The data period and sampling parameters used by NeMo Alignment team.
  harness: livecodebench
  container: nvcr.io/nvidia/eval-factory/livecodebench:25.11
  container_digest: sha256:2aa133755bb408bd94f931c5797a46e0fa80ea3161a46caefd1649aa567c8b66
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\
      \n            --scenario {{config.params.task}} \\\n            --release_version\
      \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
      \ \\\n            --temperature {{config.params.temperature}} \\\n         \
      \   --top_p {{config.params.top_p}} \\\n            --evaluate \\\n        \
      \    --codegen_n {{config.params.extra.n_samples}} \\\n            --use_cache\
      \ \\\n            --cache_batch_size {{config.params.extra.cache_batch_size}}\
      \ \\\n            --num_process_evaluate {{config.params.extra.num_process_evaluate}}\
      \ \\\n            --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
      \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
      \ \\\n            --multiprocess {{config.params.parallelism}} \\\n        \
      \    --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
      \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
      \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
      \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
      \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
      \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
      \ }} {% endif %}\n"
    framework_name: livecodebench
    pkg_name: livecodebench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        task: codegeneration
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          n_samples: 3
          num_process_evaluate: 5
          cache_batch_size: 10
          support_system_role: false
          start_date: 2024-08-01
          end_date: 2025-02-01
          cot_code_execution: false
          release_version: release_v5
      supported_endpoint_types:
      - chat
      type: livecodebench_0824_0225
    target:
      api_endpoint: {}
- name: scicode
  description: '- SciCode is a challenging benchmark designed to evaluate the capabilities
    of LLMs in generating code for solving realistic scientific research problems.
    - This variant does not include scientist-annotated background in the prompts.'
  harness: scicode
  container: nvcr.io/nvidia/eval-factory/scicode:25.11
  container_digest: sha256:0b1aeb52d2231ded3d3a005333997b95981c01c64db101ca2fe35695e8b532bb
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} scicode_eval --model {{target.api_endpoint.model_id}} --url {{target.api_endpoint.url}}
      --output-dir {{config.output_dir}}/scicode_results --log-dir {{config.output_dir}}/logs
      {% if config.params.temperature is not none %}--temperature={{config.params.temperature}}{%
      endif %} {% if config.params.limit_samples is not none %}--limit-samples={{config.params.limit_samples}}{%
      endif %} --n-samples={{config.params.extra.n_samples}} --extra-params top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},max_tokens={{config.params.max_new_tokens}},max_retries={{config.params.max_retries}}
      {% if config.params.extra.with_background %}--with-background {% endif %} {%
      if config.params.extra.include_dev %}--include-dev{% endif %} {% if config.params.extra.eval_threads
      is not none %}--eval-threads={{config.params.extra.eval_threads}}{% endif %}
      --concurrent-requests={{config.params.parallelism}}'
    framework_name: scicode
    pkg_name: scicode
    config:
      params:
        max_new_tokens: 2048
        max_retries: 2
        parallelism: 1
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          with_background: false
          include_dev: false
          n_samples: 1
          eval_threads: null
      supported_endpoint_types:
      - chat
      type: scicode
    target:
      api_endpoint:
        stream: false
- name: scicode_background
  description: '- SciCode is a challenging benchmark designed to evaluate the capabilities
    of LLMs in generating code for solving realistic scientific research problems.
    - This variant includes scientist-annotated background in the prompts.'
  harness: scicode
  container: nvcr.io/nvidia/eval-factory/scicode:25.11
  container_digest: sha256:0b1aeb52d2231ded3d3a005333997b95981c01c64db101ca2fe35695e8b532bb
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} scicode_eval --model {{target.api_endpoint.model_id}} --url {{target.api_endpoint.url}}
      --output-dir {{config.output_dir}}/scicode_results --log-dir {{config.output_dir}}/logs
      {% if config.params.temperature is not none %}--temperature={{config.params.temperature}}{%
      endif %} {% if config.params.limit_samples is not none %}--limit-samples={{config.params.limit_samples}}{%
      endif %} --n-samples={{config.params.extra.n_samples}} --extra-params top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},max_tokens={{config.params.max_new_tokens}},max_retries={{config.params.max_retries}}
      {% if config.params.extra.with_background %}--with-background {% endif %} {%
      if config.params.extra.include_dev %}--include-dev{% endif %} {% if config.params.extra.eval_threads
      is not none %}--eval-threads={{config.params.extra.eval_threads}}{% endif %}
      --concurrent-requests={{config.params.parallelism}}'
    framework_name: scicode
    pkg_name: scicode
    config:
      params:
        max_new_tokens: 2048
        max_retries: 2
        parallelism: 1
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          with_background: true
          include_dev: false
          n_samples: 1
          eval_threads: null
      supported_endpoint_types:
      - chat
      type: scicode_background
    target:
      api_endpoint:
        stream: false
- name: scicode_aa_v2
  description: '- SciCode is a challenging benchmark designed to evaluate the capabilities
    of LLMs in generating code for solving realistic scientific research problems.
    - This variant mimicks setup used by Artificial Analysis in their Intelligence
    Benchmark (v2). - It includes scientist-annotated background in the prompts and
    uses all available problems for evaluation (including "dev" set).'
  harness: scicode
  container: nvcr.io/nvidia/eval-factory/scicode:25.11
  container_digest: sha256:0b1aeb52d2231ded3d3a005333997b95981c01c64db101ca2fe35695e8b532bb
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} scicode_eval --model {{target.api_endpoint.model_id}} --url {{target.api_endpoint.url}}
      --output-dir {{config.output_dir}}/scicode_results --log-dir {{config.output_dir}}/logs
      {% if config.params.temperature is not none %}--temperature={{config.params.temperature}}{%
      endif %} {% if config.params.limit_samples is not none %}--limit-samples={{config.params.limit_samples}}{%
      endif %} --n-samples={{config.params.extra.n_samples}} --extra-params top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},max_tokens={{config.params.max_new_tokens}},max_retries={{config.params.max_retries}}
      {% if config.params.extra.with_background %}--with-background {% endif %} {%
      if config.params.extra.include_dev %}--include-dev{% endif %} {% if config.params.extra.eval_threads
      is not none %}--eval-threads={{config.params.extra.eval_threads}}{% endif %}
      --concurrent-requests={{config.params.parallelism}}'
    framework_name: scicode
    pkg_name: scicode
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 1
        temperature: 0.0
        request_timeout: 60
        top_p: 1.0e-05
        extra:
          with_background: true
          include_dev: true
          n_samples: 3
          eval_threads: null
      supported_endpoint_types:
      - chat
      type: scicode_aa_v2
    target:
      api_endpoint:
        stream: false
- name: hle
  description: Text-only questions from Humanity's Last Exam
  harness: hle
  container: nvcr.io/nvidia/eval-factory/hle:25.11
  container_digest: sha256:98117c3a5c21bf61a68eb7cf7c4e4881a18b9da89325f51d87110e9273f53a58
  defaults:
    command: hle_eval --dataset=cais/hle --model_name={{target.api_endpoint.model_id}}
      --model_url={{target.api_endpoint.url}}  --temperature={{config.params.temperature}}
      --top_p={{config.params.top_p}} --timeout={{config.params.request_timeout}}  {%
      if config.params.limit_samples is not none %}--limit {{config.params.limit_samples}}{%
      endif %} --output_dir={{config.output_dir}}  {% if target.api_endpoint.api_key
      is not none %}--api_key_name={{target.api_endpoint.api_key}}{% endif %} --max_retries={{config.params.max_retries}}
      --num_workers={{config.params.parallelism}}  --max_new_tokens={{config.params.max_new_tokens}}
      --text_only --generate --judge
    framework_name: hle
    pkg_name: hle
    config:
      params:
        max_new_tokens: 8192
        max_retries: 30
        parallelism: 10
        task: hle
        temperature: 0.0
        request_timeout: 600
        top_p: 1.0
        extra: {}
      supported_endpoint_types:
      - chat
      type: hle
    target:
      api_endpoint: {}
- name: hle_aa_v2
  description: Text-only questions from Humanity's Last Exam and params aligned with
    Artificial Analysis Index v2
  harness: hle
  container: nvcr.io/nvidia/eval-factory/hle:25.11
  container_digest: sha256:98117c3a5c21bf61a68eb7cf7c4e4881a18b9da89325f51d87110e9273f53a58
  defaults:
    command: hle_eval --dataset=cais/hle --model_name={{target.api_endpoint.model_id}}
      --model_url={{target.api_endpoint.url}}  --temperature={{config.params.temperature}}
      --top_p={{config.params.top_p}} --timeout={{config.params.request_timeout}}  {%
      if config.params.limit_samples is not none %}--limit {{config.params.limit_samples}}{%
      endif %} --output_dir={{config.output_dir}}  {% if target.api_endpoint.api_key
      is not none %}--api_key_name={{target.api_endpoint.api_key}}{% endif %} --max_retries={{config.params.max_retries}}
      --num_workers={{config.params.parallelism}}  --max_new_tokens={{config.params.max_new_tokens}}
      --text_only --generate --judge
    framework_name: hle
    pkg_name: hle
    config:
      params:
        max_new_tokens: 16384
        max_retries: 30
        parallelism: 10
        task: hle_aa_v2
        temperature: 0.0
        request_timeout: 600
        top_p: 1.0
        extra: {}
      supported_endpoint_types:
      - chat
      type: hle_aa_v2
    target:
      api_endpoint: {}
- name: bfclv3
  description: BFCL v3 with Single-turn and Multi-turn, Live and Non-Live, AST and
    Exec evaluation. Not using native function calling.
  harness: bfcl
  container: nvcr.io/nvidia/eval-factory/bfcl:25.11
  container_digest: sha256:79038a60e720557a403aad772943c4a03ffb4ffbd9406afd8ab6a5a190ffc4e3
  defaults:
    command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
      \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
      \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n\
      \  --dataset_path {{config.params.extra.custom_dataset.path}} \\\n  --test_category\
      \ {{config.params.task}} \\\n  --processing_output_dir {{config.output_dir ~\
      \ \"/custom_dataset_processing\"}} \\\n  {% if config.params.extra.custom_dataset.data_template_path\
      \ %}--data_template_path {{config.params.extra.custom_dataset.data_template_path}}{%\
      \ endif %}) && \\\necho \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n\
      {% endif -%}\n{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%\
      \ endif %} bfcl generate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
      \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
      \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key\
      \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{% endif %} bfcl\
      \ evaluate --model {{target.api_endpoint.model_id}} --test-category {{config.params.task}}\
      \ --model-mapping oai --result-dir {{config.output_dir}} --score-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
    framework_name: bfcl
    pkg_name: bfcl
    config:
      params:
        parallelism: 10
        task: all
        extra:
          native_calling: false
          custom_dataset:
            path: null
            format: null
            data_template_path: null
      supported_endpoint_types:
      - chat
      - vlm
      type: bfclv3
    target:
      api_endpoint: {}
- name: bfclv3_ast
  description: BFCL v3 with Single-turn and Multi-turn, Live and Non-Live, AST evaluation.
    Uses native function calling.
  harness: bfcl
  container: nvcr.io/nvidia/eval-factory/bfcl:25.11
  container_digest: sha256:79038a60e720557a403aad772943c4a03ffb4ffbd9406afd8ab6a5a190ffc4e3
  defaults:
    command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
      \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
      \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n\
      \  --dataset_path {{config.params.extra.custom_dataset.path}} \\\n  --test_category\
      \ {{config.params.task}} \\\n  --processing_output_dir {{config.output_dir ~\
      \ \"/custom_dataset_processing\"}} \\\n  {% if config.params.extra.custom_dataset.data_template_path\
      \ %}--data_template_path {{config.params.extra.custom_dataset.data_template_path}}{%\
      \ endif %}) && \\\necho \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n\
      {% endif -%}\n{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%\
      \ endif %} bfcl generate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
      \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
      \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key\
      \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{% endif %} bfcl\
      \ evaluate --model {{target.api_endpoint.model_id}} --test-category {{config.params.task}}\
      \ --model-mapping oai --result-dir {{config.output_dir}} --score-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
    framework_name: bfcl
    pkg_name: bfcl
    config:
      params:
        parallelism: 10
        task: multi_turn,ast
        extra:
          native_calling: true
          custom_dataset:
            path: null
            format: null
            data_template_path: null
      supported_endpoint_types:
      - chat
      - vlm
      type: bfclv3_ast
    target:
      api_endpoint: {}
- name: bfclv3_ast_prompting
  description: BFCL v3 with Single-turn and Multi-turn, Live and Non-Live, AST evaluation.
    Not using native function calling.
  harness: bfcl
  container: nvcr.io/nvidia/eval-factory/bfcl:25.11
  container_digest: sha256:79038a60e720557a403aad772943c4a03ffb4ffbd9406afd8ab6a5a190ffc4e3
  defaults:
    command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
      \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
      \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n\
      \  --dataset_path {{config.params.extra.custom_dataset.path}} \\\n  --test_category\
      \ {{config.params.task}} \\\n  --processing_output_dir {{config.output_dir ~\
      \ \"/custom_dataset_processing\"}} \\\n  {% if config.params.extra.custom_dataset.data_template_path\
      \ %}--data_template_path {{config.params.extra.custom_dataset.data_template_path}}{%\
      \ endif %}) && \\\necho \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n\
      {% endif -%}\n{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%\
      \ endif %} bfcl generate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
      \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
      \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key\
      \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{% endif %} bfcl\
      \ evaluate --model {{target.api_endpoint.model_id}} --test-category {{config.params.task}}\
      \ --model-mapping oai --result-dir {{config.output_dir}} --score-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
    framework_name: bfcl
    pkg_name: bfcl
    config:
      params:
        parallelism: 10
        task: multi_turn,ast
        extra:
          native_calling: false
          custom_dataset:
            path: null
            format: null
            data_template_path: null
      supported_endpoint_types:
      - chat
      - vlm
      type: bfclv3_ast_prompting
    target:
      api_endpoint: {}
- name: bfclv2
  description: BFCL v2 with Single-turn, Live and Non-Live, AST and Exec evaluation.
    Not using native function calling.
  harness: bfcl
  container: nvcr.io/nvidia/eval-factory/bfcl:25.11
  container_digest: sha256:79038a60e720557a403aad772943c4a03ffb4ffbd9406afd8ab6a5a190ffc4e3
  defaults:
    command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
      \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
      \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n\
      \  --dataset_path {{config.params.extra.custom_dataset.path}} \\\n  --test_category\
      \ {{config.params.task}} \\\n  --processing_output_dir {{config.output_dir ~\
      \ \"/custom_dataset_processing\"}} \\\n  {% if config.params.extra.custom_dataset.data_template_path\
      \ %}--data_template_path {{config.params.extra.custom_dataset.data_template_path}}{%\
      \ endif %}) && \\\necho \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n\
      {% endif -%}\n{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%\
      \ endif %} bfcl generate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
      \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
      \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key\
      \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{% endif %} bfcl\
      \ evaluate --model {{target.api_endpoint.model_id}} --test-category {{config.params.task}}\
      \ --model-mapping oai --result-dir {{config.output_dir}} --score-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
    framework_name: bfcl
    pkg_name: bfcl
    config:
      params:
        parallelism: 10
        task: single_turn
        extra:
          native_calling: false
          custom_dataset:
            path: null
            format: null
            data_template_path: null
      supported_endpoint_types:
      - chat
      - vlm
      type: bfclv2
    target:
      api_endpoint: {}
- name: bfclv2_ast
  description: BFCL v2 with Single-turn, Live and Non-Live, AST evaluation only. Uses
    native function calling.
  harness: bfcl
  container: nvcr.io/nvidia/eval-factory/bfcl:25.11
  container_digest: sha256:79038a60e720557a403aad772943c4a03ffb4ffbd9406afd8ab6a5a190ffc4e3
  defaults:
    command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
      \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
      \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n\
      \  --dataset_path {{config.params.extra.custom_dataset.path}} \\\n  --test_category\
      \ {{config.params.task}} \\\n  --processing_output_dir {{config.output_dir ~\
      \ \"/custom_dataset_processing\"}} \\\n  {% if config.params.extra.custom_dataset.data_template_path\
      \ %}--data_template_path {{config.params.extra.custom_dataset.data_template_path}}{%\
      \ endif %}) && \\\necho \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n\
      {% endif -%}\n{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%\
      \ endif %} bfcl generate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
      \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
      \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key\
      \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{% endif %} bfcl\
      \ evaluate --model {{target.api_endpoint.model_id}} --test-category {{config.params.task}}\
      \ --model-mapping oai --result-dir {{config.output_dir}} --score-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
    framework_name: bfcl
    pkg_name: bfcl
    config:
      params:
        parallelism: 10
        task: ast
        extra:
          native_calling: true
          custom_dataset:
            path: null
            format: null
            data_template_path: null
      supported_endpoint_types:
      - chat
      - vlm
      type: bfclv2_ast
    target:
      api_endpoint: {}
- name: bfclv2_ast_prompting
  description: BFCL v2 with Single-turn, Live and Non-Live, AST evaluation only. Not
    using native function calling.
  harness: bfcl
  container: nvcr.io/nvidia/eval-factory/bfcl:25.11
  container_digest: sha256:79038a60e720557a403aad772943c4a03ffb4ffbd9406afd8ab6a5a190ffc4e3
  defaults:
    command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
      \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
      \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n\
      \  --dataset_path {{config.params.extra.custom_dataset.path}} \\\n  --test_category\
      \ {{config.params.task}} \\\n  --processing_output_dir {{config.output_dir ~\
      \ \"/custom_dataset_processing\"}} \\\n  {% if config.params.extra.custom_dataset.data_template_path\
      \ %}--data_template_path {{config.params.extra.custom_dataset.data_template_path}}{%\
      \ endif %}) && \\\necho \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n\
      {% endif -%}\n{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%\
      \ endif %} bfcl generate --model {{target.api_endpoint.model_id}} --test-category\
      \ {{config.params.task}} --model-mapping oai --result-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
      \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
      \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key\
      \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{% endif %} bfcl\
      \ evaluate --model {{target.api_endpoint.model_id}} --test-category {{config.params.task}}\
      \ --model-mapping oai --result-dir {{config.output_dir}} --score-dir {{config.output_dir}}\
      \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
    framework_name: bfcl
    pkg_name: bfcl
    config:
      params:
        parallelism: 10
        task: ast
        extra:
          native_calling: false
          custom_dataset:
            path: null
            format: null
            data_template_path: null
      supported_endpoint_types:
      - chat
      - vlm
      type: bfclv2_ast_prompting
    target:
      api_endpoint: {}
- name: report_generation
  description: Generate professional reports and evaluate them (full pipeline)
  harness: profbench
  container: nvcr.io/nvidia/eval-factory/profbench:25.11
  container_digest: sha256:7e07d7758c55aaf9a341941b3eb1e5b37d50ee48a710af58916c35597de15256
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} {% if config.params.extra.run_generation %}\n  python -m\
      \ profbench.run_report_generation \\\n    --model {{target.api_endpoint.model_id}}\
      \ \\\n    --library {{config.params.extra.library}} \\\n    --timeout {{config.params.request_timeout}}\
      \ \\\n    --parallel {{config.params.parallelism}} \\\n    --retry-attempts\
      \ {{config.params.max_retries}} \\\n    --folder {{config.output_dir}}{% if\
      \ target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
      \ endif %}{% if config.params.extra.version is not none %} --version {{config.params.extra.version}}{%\
      \ endif %}{% if config.params.extra.web_search %} --web-search{% endif %}{%\
      \ if config.params.extra.reasoning %} --reasoning{% endif %}{% if config.params.extra.reasoning_effort\
      \ is not none %} --reasoning-effort {{config.params.extra.reasoning_effort}}{%\
      \ endif %}{% if config.params.limit_samples is not none %} --limit-samples {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
      \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
      \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
      \ endif %} && \n  GENERATION_OUTPUT=$(ls -t {{config.output_dir}}/*.jsonl |\
      \ head -1) && \n{% endif %} {% if config.params.extra.run_judge_generated %}\n\
      \  python -m profbench.run_best_llm_judge_on_generated_reports \\\n    --filename\
      \ $GENERATION_OUTPUT \\\n    --api-key $API_KEY \\\n    --model {{target.api_endpoint.model_id}}\
      \ \\\n    --library {{config.params.extra.library}} \\\n    --timeout {{config.params.request_timeout}}\
      \ \\\n    --parallel {{config.params.parallelism}} \\\n    --retry-attempts\
      \ {{config.params.max_retries}} \\\n    --output-folder {{config.output_dir}}/judgements{%\
      \ if target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
      \ endif %}{% if config.params.limit_samples is not none %} --limit-samples {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
      \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
      \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
      \ endif %} && \n  JUDGE_OUTPUT=$(ls -t {{config.output_dir}}/judgements/*.jsonl\
      \ | head -1) && \n  python -m profbench.score_report_generation $JUDGE_OUTPUT\n\
      {% endif %} {% if config.params.extra.run_judge_provided %}\n  python -m profbench.run_llm_judge_on_provided_reports\
      \ \\\n    --model {{target.api_endpoint.model_id}} \\\n    --library {{config.params.extra.library}}\
      \ \\\n    --timeout {{config.params.request_timeout}} \\\n    --parallel {{config.params.parallelism}}\
      \ \\\n    --retry-attempts {{config.params.max_retries}} \\\n    --folder {{config.output_dir}}{%\
      \ if target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
      \ endif %}{% if config.params.extra.reasoning %} --reasoning{% endif %}{% if\
      \ config.params.extra.reasoning_effort is not none %} --reasoning-effort {{config.params.extra.reasoning_effort}}{%\
      \ endif %}{% if config.params.extra.debug %} --debug{% endif %}{% if config.params.limit_samples\
      \ is not none %} --limit-samples {{config.params.limit_samples}}{% endif %}{%\
      \ if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
      \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
      \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
      \ endif %} && \n  JUDGE_OUTPUT=$(ls -t {{config.output_dir}}/*.jsonl | head\
      \ -1) && \n  python -m profbench.score_llm_judge $JUDGE_OUTPUT\n{% endif %}\n"
    framework_name: profbench
    pkg_name: profbench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        temperature: 0.0
        request_timeout: 600
        top_p: 1.0e-05
        extra:
          run_generation: true
          run_judge_generated: true
          run_judge_provided: false
          library: openai
          version: lite
          web_search: false
          reasoning: false
          reasoning_effort: null
          debug: false
      supported_endpoint_types:
      - chat
      type: report_generation
    target:
      api_endpoint: {}
- name: llm_judge
  description: Run LLM judge on provided ProfBench reports and score them
  harness: profbench
  container: nvcr.io/nvidia/eval-factory/profbench:25.11
  container_digest: sha256:7e07d7758c55aaf9a341941b3eb1e5b37d50ee48a710af58916c35597de15256
  defaults:
    command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
      \ && \n{% endif %} {% if config.params.extra.run_generation %}\n  python -m\
      \ profbench.run_report_generation \\\n    --model {{target.api_endpoint.model_id}}\
      \ \\\n    --library {{config.params.extra.library}} \\\n    --timeout {{config.params.request_timeout}}\
      \ \\\n    --parallel {{config.params.parallelism}} \\\n    --retry-attempts\
      \ {{config.params.max_retries}} \\\n    --folder {{config.output_dir}}{% if\
      \ target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
      \ endif %}{% if config.params.extra.version is not none %} --version {{config.params.extra.version}}{%\
      \ endif %}{% if config.params.extra.web_search %} --web-search{% endif %}{%\
      \ if config.params.extra.reasoning %} --reasoning{% endif %}{% if config.params.extra.reasoning_effort\
      \ is not none %} --reasoning-effort {{config.params.extra.reasoning_effort}}{%\
      \ endif %}{% if config.params.limit_samples is not none %} --limit-samples {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
      \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
      \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
      \ endif %} && \n  GENERATION_OUTPUT=$(ls -t {{config.output_dir}}/*.jsonl |\
      \ head -1) && \n{% endif %} {% if config.params.extra.run_judge_generated %}\n\
      \  python -m profbench.run_best_llm_judge_on_generated_reports \\\n    --filename\
      \ $GENERATION_OUTPUT \\\n    --api-key $API_KEY \\\n    --model {{target.api_endpoint.model_id}}\
      \ \\\n    --library {{config.params.extra.library}} \\\n    --timeout {{config.params.request_timeout}}\
      \ \\\n    --parallel {{config.params.parallelism}} \\\n    --retry-attempts\
      \ {{config.params.max_retries}} \\\n    --output-folder {{config.output_dir}}/judgements{%\
      \ if target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
      \ endif %}{% if config.params.limit_samples is not none %} --limit-samples {{config.params.limit_samples}}{%\
      \ endif %}{% if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
      \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
      \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
      \ endif %} && \n  JUDGE_OUTPUT=$(ls -t {{config.output_dir}}/judgements/*.jsonl\
      \ | head -1) && \n  python -m profbench.score_report_generation $JUDGE_OUTPUT\n\
      {% endif %} {% if config.params.extra.run_judge_provided %}\n  python -m profbench.run_llm_judge_on_provided_reports\
      \ \\\n    --model {{target.api_endpoint.model_id}} \\\n    --library {{config.params.extra.library}}\
      \ \\\n    --timeout {{config.params.request_timeout}} \\\n    --parallel {{config.params.parallelism}}\
      \ \\\n    --retry-attempts {{config.params.max_retries}} \\\n    --folder {{config.output_dir}}{%\
      \ if target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
      \ endif %}{% if config.params.extra.reasoning %} --reasoning{% endif %}{% if\
      \ config.params.extra.reasoning_effort is not none %} --reasoning-effort {{config.params.extra.reasoning_effort}}{%\
      \ endif %}{% if config.params.extra.debug %} --debug{% endif %}{% if config.params.limit_samples\
      \ is not none %} --limit-samples {{config.params.limit_samples}}{% endif %}{%\
      \ if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
      \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
      \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
      \ endif %} && \n  JUDGE_OUTPUT=$(ls -t {{config.output_dir}}/*.jsonl | head\
      \ -1) && \n  python -m profbench.score_llm_judge $JUDGE_OUTPUT\n{% endif %}\n"
    framework_name: profbench
    pkg_name: profbench
    config:
      params:
        max_new_tokens: 4096
        max_retries: 5
        parallelism: 10
        temperature: 0.0
        request_timeout: 600
        top_p: 1.0e-05
        extra:
          run_generation: false
          run_judge_generated: false
          run_judge_provided: true
          library: openai
          version: lite
          web_search: false
          reasoning: false
          reasoning_effort: null
          debug: false
      supported_endpoint_types:
      - chat
      type: llm_judge
    target:
      api_endpoint: {}
- name: ai2d_judge
  description: A benchmark for evaluating diagram understanding capabilities of large
    vision-language models.
  harness: vlmevalkit
  container: nvcr.io/nvidia/eval-factory/vlmevalkit:25.11
  container_digest: sha256:ef22dbb3056032735b3e9f31d5c6c28e2af6e5b12b00fff3f5934345c60be17f
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: AI2D_TEST
            class: ImageMCQDataset
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
      supported_endpoint_types:
      - vlm
      type: ai2d_judge
    target:
      api_endpoint: {}
- name: chartqa
  description: A Benchmark for Question Answering about Charts with Visual and Logical
    Reasoning
  harness: vlmevalkit
  container: nvcr.io/nvidia/eval-factory/vlmevalkit:25.11
  container_digest: sha256:ef22dbb3056032735b3e9f31d5c6c28e2af6e5b12b00fff3f5934345c60be17f
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: ChartQA_TEST
            class: ImageVQADataset
      supported_endpoint_types:
      - vlm
      type: chartqa
    target:
      api_endpoint: {}
- name: mathvista-mini
  description: Evaluating Math Reasoning in Visual Contexts
  harness: vlmevalkit
  container: nvcr.io/nvidia/eval-factory/vlmevalkit:25.11
  container_digest: sha256:ef22dbb3056032735b3e9f31d5c6c28e2af6e5b12b00fff3f5934345c60be17f
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: MathVista_MINI
            class: MathVista
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
      supported_endpoint_types:
      - vlm
      type: mathvista-mini
    target:
      api_endpoint: {}
- name: mmmu_judge
  description: A benchmark for evaluating multimodal models on massive multi-discipline
    tasks demanding college-level subject knowledge and deliberate reasoning.
  harness: vlmevalkit
  container: nvcr.io/nvidia/eval-factory/vlmevalkit:25.11
  container_digest: sha256:ef22dbb3056032735b3e9f31d5c6c28e2af6e5b12b00fff3f5934345c60be17f
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: MMMU_DEV_VAL
            class: MMMUDataset
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
      supported_endpoint_types:
      - vlm
      type: mmmu_judge
    target:
      api_endpoint: {}
- name: ocrbench
  description: Comprehensive evaluation benchmark designed to assess the OCR capabilities
    of Large Multimodal Models
  harness: vlmevalkit
  container: nvcr.io/nvidia/eval-factory/vlmevalkit:25.11
  container_digest: sha256:ef22dbb3056032735b3e9f31d5c6c28e2af6e5b12b00fff3f5934345c60be17f
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: OCRBench
            class: OCRBench
      supported_endpoint_types:
      - vlm
      type: ocrbench
    target:
      api_endpoint: {}
- name: ocr_reasoning
  description: Comprehensive benchmark of 1,069 human-annotated examples designed
    to evaluate multimodal large language models on text-rich image reasoning tasks
    by assessing both final answers and the reasoning process across six core abilities
    and 18 practical tasks.
  harness: vlmevalkit
  container: nvcr.io/nvidia/eval-factory/vlmevalkit:25.11
  container_digest: sha256:ef22dbb3056032735b3e9f31d5c6c28e2af6e5b12b00fff3f5934345c60be17f
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: OCR_Reasoning
            class: OCR_Reasoning
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
      supported_endpoint_types:
      - vlm
      type: ocr_reasoning
    target:
      api_endpoint: {}
- name: slidevqa
  description: Evaluates ability to answer questions about slide decks by selecting
    relevant slides from multiple images
  harness: vlmevalkit
  container: nvcr.io/nvidia/eval-factory/vlmevalkit:25.11
  container_digest: sha256:ef22dbb3056032735b3e9f31d5c6c28e2af6e5b12b00fff3f5934345c60be17f
  defaults:
    command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
      : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
      : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
      : \"{{target.api_endpoint.api_key}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
      \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
      \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n   \
      \   \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
      \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
      \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
      : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
      \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
      {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"\
      system_prompt\": \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
      \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif\
      \ %}\n    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\"\
      : {\n      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
      : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
      \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
      \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
      \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
      \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
      \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
      \ endif %}\n"
    framework_name: vlmevalkit
    pkg_name: vlmevalkit
    config:
      params:
        max_new_tokens: 2048
        max_retries: 5
        parallelism: 4
        temperature: 0.0
        request_timeout: 60
        extra:
          dataset:
            name: SLIDEVQA
            class: SlideVQA
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
      supported_endpoint_types:
      - vlm
      type: slidevqa
    target:
      api_endpoint: {}
- name: garak
  description: Task for running the default set of Garak probes
  harness: garak
  container: nvcr.io/nvidia/eval-factory/garak:25.11
  container_digest: sha256:b2f476c52cc3e188f4317dc6da380269d74ede895d86dd6a3e60bb78a3fbfb12
  defaults:
    command: "cat > garak_config.yaml << 'EOF'\n{% if config.params.extra.seed is\
      \ not none %}run:\n  seed: {{config.params.extra.seed}}{% endif %}\nplugins:\n\
      \  {% if config.params.extra.probes is not none %}probe_spec: {{config.params.extra.probes}}{%\
      \ endif %}\n  extended_detectors: true\n  target_type: {% if target.api_endpoint.type\
      \ == \"completions\" %}nim.NVOpenAICompletion{% elif target.api_endpoint.type\
      \ == \"chat\" %}nim.NVOpenAIChat{% endif %}\n  target_name: {{target.api_endpoint.model_id}}\n\
      \  generators:\n    nim:\n      uri: {{target.api_endpoint.url | replace('/chat/completions',\
      \ '') | replace('/completions', '')}}\n      {% if config.params.temperature\
      \ is not none %}temperature: {{config.params.temperature}}{% endif %}\n    \
      \  {% if config.params.top_p is not none %}top_p: {{config.params.top_p}}{%\
      \ endif %}\n      {% if config.params.max_new_tokens is not none %}max_tokens:\
      \ {{config.params.max_new_tokens}}{% endif %}\nsystem:\n  parallel_attempts:\
      \ {{config.params.parallelism}}\n  lite: false\nEOF\n{% if target.api_endpoint.api_key\
      \ is not none %}\nexport NIM_API_KEY=${{target.api_endpoint.api_key}} &&\n{%\
      \ else %}\nexport NIM_API_KEY=dummy &&\n{% endif %}\nexport XDG_DATA_HOME={{config.output_dir}}\
      \ &&\ngarak --config garak_config.yaml --report_prefix=results\n"
    framework_name: garak
    pkg_name: garak
    config:
      params:
        max_new_tokens: 150
        parallelism: 32
        task: garak
        temperature: 0.1
        top_p: 0.7
        extra:
          probes: null
          seed: null
      supported_endpoint_types:
      - chat
      - completions
      type: garak
    target:
      api_endpoint: {}
- name: ns_aime2024
  description: AIME2024
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: aime24
        extra:
          use_sandbox: false
          num_repeats: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: true
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: math_judge
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      type: ns_aime2024
    target: {}
- name: ns_aime2025
  description: AIME2025
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: aime25
        extra:
          use_sandbox: false
          num_repeats: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: true
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: math_judge
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      type: ns_aime2025
    target: {}
- name: ns_gpqa
  description: GPQA Diamond
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: gpqa
        extra:
          use_sandbox: false
          num_repeats: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      type: ns_gpqa
    target: {}
- name: ns_bfcl_v3
  description: BFCLv3
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: bfcl_v3
        extra:
          use_sandbox: false
          num_repeats: null
          args: ++use_client_parsing=False
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      type: ns_bfcl_v3
    target: {}
- name: ns_bfcl_v4
  description: BFCLv4
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: bfcl_v4
        extra:
          use_sandbox: false
          num_repeats: null
          args: ++use_client_parsing=False
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      type: ns_bfcl_v4
    target: {}
- name: ns_livecodebench
  description: LiveCodeBench
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: livecodebench
        extra:
          use_sandbox: false
          num_repeats: null
          args: null
          system_message: null
          dataset_split: test_v6_2408_2505
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      type: ns_livecodebench
    target: {}
- name: ns_hle
  description: HumanityLastExam
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: hle
        extra:
          use_sandbox: false
          num_repeats: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      type: ns_hle
    target: {}
- name: ns_ruler
  description: RULER - Long Context Understanding
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: ruler.evaluation_128k
        extra:
          use_sandbox: false
          num_repeats: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
          ruler:
            data_dir: /workspace/ruler_data
            cluster: local
            setup: evaluation_128k
            max_seq_length: 131072
            tokenizer_path: null
            template_tokens: 50
            num_samples: null
            tasks: null
      type: ns_ruler
    target: {}
- name: ns_mmlu
  description: MMLU
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: mmlu
        extra:
          use_sandbox: false
          num_repeats: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      type: ns_mmlu
    target: {}
- name: ns_mmlu_pro
  description: MMLU-PRO
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: mmlu-pro
        extra:
          use_sandbox: false
          num_repeats: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      type: ns_mmlu_pro
    target: {}
- name: ns_scicode
  description: SciCode
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: scicode
        extra:
          use_sandbox: true
          num_repeats: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      type: ns_scicode
    target: {}
- name: ns_aa_lcr
  description: AA-LCR
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: aalcr
        extra:
          use_sandbox: false
          num_repeats: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: true
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: 0.0
            top_p: 1.0
            max_new_tokens: 4096
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      type: ns_aa_lcr
    target: {}
- name: ns_ifbench
  description: IFBench - Instruction Following Benchmark
  harness: nemo-skills
  container: nvcr.io/nvidia/eval-factory/nemo_skills:25.11
  container_digest: sha256:f8269f1c6662017e60dd116b9f462db98349d9715214d6d6055fffd2ab12978d
  defaults:
    command: cd /nemo_run/code && {% if config.params.extra.use_sandbox %}python -m
      nemo_skills.code_execution.local_sandbox.local_sandbox_server > {{config.output_dir}}/sandbox.log
      2>&1 & SANDBOX_PID=$! && sleep 3 && {% endif %}{% if not config.params.task.startswith('ruler')
      %} ns prepare_data {{config.params.task}} {% else %} mkdir -p {{config.params.extra.ruler.data_dir}}
      && ln -sf {{config.params.extra.ruler.data_dir}} /nemo_run/code/ruler_data &&  ns
      prepare_data ruler --data_dir={{config.params.extra.ruler.data_dir}} --cluster={{config.params.extra.ruler.cluster}}
      --setup={{config.params.extra.ruler.setup}} --max_seq_length={{config.params.extra.ruler.max_seq_length}}
      --tokenizer_path={{config.params.extra.ruler.tokenizer_path}} {% if config.params.extra.ruler.template_tokens
      is not none %}--template_tokens={{config.params.extra.ruler.template_tokens}}{%
      endif %} {% if config.params.extra.ruler.num_samples is not none %}--num_samples={{config.params.extra.ruler.num_samples}}{%
      elif config.params.limit_samples is not none %}--num_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.extra.ruler.tasks is not none %}--tasks {% for
      task in config.params.extra.ruler.tasks %}{{task}}{% if not loop.last %} {%
      endif %}{% endfor %}{% endif %} {% endif %} && ns eval --server_type=openai
      --model={{target.api_endpoint.model_id}} --server_address={{target.api_endpoint.url}}
      --benchmarks={{config.params.task}}{% if config.params.extra.num_repeats is
      not none and config.params.extra.num_repeats > 1 %}:{{config.params.extra.num_repeats}}{%
      endif %} --output_dir={{config.output_dir}} {% if config.params.extra.dataset_split
      is not none %}--split={{config.params.extra.dataset_split}}{% endif %} {% if
      config.params.extra.ruler.data_dir is not none %}--data_dir={{config.params.extra.ruler.data_dir}}{%
      endif %} {% if target.api_endpoint.api_key is not none %}++server.api_key_env_var={{target.api_endpoint.api_key}}{%
      endif %} ++inference.tokens_to_generate={{config.params.max_new_tokens}} {%
      if config.params.extra.system_message is not none %} ++system_message='{{config.params.extra.system_message}}'
      {% endif %} {% if config.params.limit_samples is not none %}++max_samples={{config.params.limit_samples}}{%
      endif %} {% if config.params.parallelism is not none %}++max_concurrent_requests={{config.params.parallelism}}{%
      endif %} {% if config.params.temperature is not none %}++inference.temperature={{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}++inference.top_p={{config.params.top_p}}{%
      endif %} {% if config.params.extra.args is not none %} {{config.params.extra.args}}
      {% endif %} {% if config.params.extra.judge_support and config.params.extra.judge.url
      is not none %} --judge_model={{config.params.extra.judge.model_id}} --judge_server_address={{config.params.extra.judge.url}}
      --judge_server_type=openai {% if config.params.extra.judge.generation_type is
      not none %} --judge_generation_type={{config.params.extra.judge.generation_type}}
      {% endif %} --extra_judge_args="++server.api_key_env_var={{config.params.extra.judge.api_key}}
      {%- if config.params.extra.judge.temperature is not none %} ++inference.temperature={{config.params.extra.judge.temperature}}{%
      endif %} {%- if config.params.extra.judge.top_p is not none %} ++inference.top_p={{config.params.extra.judge.top_p}}{%
      endif %} {%- if config.params.extra.judge.max_new_tokens is not none %} ++inference.tokens_to_generate={{config.params.extra.judge.max_new_tokens}}{%
      endif %}" {% endif %} {% if config.params.extra.use_sandbox %} ; EXIT_CODE=$?
      ; kill $SANDBOX_PID 2>/dev/null || true ; exit $EXIT_CODE{% endif %}
    framework_name: nemo_skills
    pkg_name: nemo_skills
    config:
      params:
        max_new_tokens: 65536
        parallelism: 16
        task: ifbench
        extra:
          use_sandbox: false
          num_repeats: null
          args: null
          system_message: null
          dataset_split: null
          judge_support: false
          judge:
            url: null
            model_id: null
            api_key: null
            generation_type: null
            random_seed: 1234
            temperature: null
            top_p: null
            max_new_tokens: null
          ruler:
            data_dir: null
            cluster: null
            setup: null
            max_seq_length: null
            tokenizer_path: null
            template_tokens: null
            num_samples: null
            tasks: null
      type: ns_ifbench
    target: {}
- name: aegis_v2
  description: Aegis V2 without evaluating reasoning traces. This version is used
    by the NeMo Safety Toolkit.
  harness: safety-eval
  container: nvcr.io/nvidia/eval-factory/safety-harness:25.11
  container_digest: sha256:5e2eec51a9ef5c8900849197956c0cbf547fc177f3aa64dacc4373d9fc541a06
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}  &&
      {% endif %} {% if config.params.extra.judge.api_key is not none %}export JUDGE_API_KEY=${{config.params.extra.judge.api_key}}
      && {% endif %} safety-eval  --model-name  {{target.api_endpoint.model_id}} --model-url
      {{target.api_endpoint.url}} --model-type {{target.api_endpoint.type}}  --judge-url  {{config.params.extra.judge.url}}   --results-dir
      {{config.output_dir}}   --eval {{config.params.task}}  --mut-inference-params
      max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},concurrency={{config.params.parallelism}},retries={{config.params.max_retries}}
      --judge-inference-params concurrency={{config.params.extra.judge.parallelism}},retries={{config.params.max_retries}}  {%
      if config.params.extra.dataset is defined and config.params.extra.dataset %}
      --dataset {{config.params.extra.dataset}}{% endif %} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}} {% endif %} {% if config.params.extra.judge.model_id
      is not none %} --judge-model-name {{config.params.extra.judge.model_id}} {%
      endif %} {% if config.type == "aegis_v2_reasoning" %} {% if config.params.extra.evaluate_reasoning_traces  %}
      --evaluate-reasoning-traces {% endif %} {% endif %}'
    framework_name: safety_eval
    pkg_name: safety_eval
    config:
      params:
        max_new_tokens: 6144
        max_retries: 5
        parallelism: 8
        task: aegis_v2
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          judge:
            url: null
            model_id: null
            api_key: null
            parallelism: 32
            request_timeout: 60
            max_retries: 16
          evaluate_reasoning_traces: false
      supported_endpoint_types:
      - chat
      - completions
      type: aegis_v2
    target:
      api_endpoint:
        stream: false
- name: aegis_v2_reasoning
  description: Aegis V2 with evaluating reasoning traces.
  harness: safety-eval
  container: nvcr.io/nvidia/eval-factory/safety-harness:25.11
  container_digest: sha256:5e2eec51a9ef5c8900849197956c0cbf547fc177f3aa64dacc4373d9fc541a06
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}  &&
      {% endif %} {% if config.params.extra.judge.api_key is not none %}export JUDGE_API_KEY=${{config.params.extra.judge.api_key}}
      && {% endif %} safety-eval  --model-name  {{target.api_endpoint.model_id}} --model-url
      {{target.api_endpoint.url}} --model-type {{target.api_endpoint.type}}  --judge-url  {{config.params.extra.judge.url}}   --results-dir
      {{config.output_dir}}   --eval {{config.params.task}}  --mut-inference-params
      max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},concurrency={{config.params.parallelism}},retries={{config.params.max_retries}}
      --judge-inference-params concurrency={{config.params.extra.judge.parallelism}},retries={{config.params.max_retries}}  {%
      if config.params.extra.dataset is defined and config.params.extra.dataset %}
      --dataset {{config.params.extra.dataset}}{% endif %} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}} {% endif %} {% if config.params.extra.judge.model_id
      is not none %} --judge-model-name {{config.params.extra.judge.model_id}} {%
      endif %} {% if config.type == "aegis_v2_reasoning" %} {% if config.params.extra.evaluate_reasoning_traces  %}
      --evaluate-reasoning-traces {% endif %} {% endif %}'
    framework_name: safety_eval
    pkg_name: safety_eval
    config:
      params:
        max_new_tokens: 6144
        max_retries: 5
        parallelism: 8
        task: aegis_v2
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          judge:
            url: null
            model_id: null
            api_key: null
            parallelism: 32
            request_timeout: 60
            max_retries: 16
          evaluate_reasoning_traces: true
      supported_endpoint_types:
      - chat
      - completions
      type: aegis_v2_reasoning
    target:
      api_endpoint:
        stream: false
- name: wildguard
  description: Wildguard
  harness: safety-eval
  container: nvcr.io/nvidia/eval-factory/safety-harness:25.11
  container_digest: sha256:5e2eec51a9ef5c8900849197956c0cbf547fc177f3aa64dacc4373d9fc541a06
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}  &&
      {% endif %} {% if config.params.extra.judge.api_key is not none %}export JUDGE_API_KEY=${{config.params.extra.judge.api_key}}
      && {% endif %} safety-eval  --model-name  {{target.api_endpoint.model_id}} --model-url
      {{target.api_endpoint.url}} --model-type {{target.api_endpoint.type}}  --judge-url  {{config.params.extra.judge.url}}   --results-dir
      {{config.output_dir}}   --eval {{config.params.task}}  --mut-inference-params
      max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},concurrency={{config.params.parallelism}},retries={{config.params.max_retries}}
      --judge-inference-params concurrency={{config.params.extra.judge.parallelism}},retries={{config.params.max_retries}}  {%
      if config.params.extra.dataset is defined and config.params.extra.dataset %}
      --dataset {{config.params.extra.dataset}}{% endif %} {% if config.params.limit_samples
      is not none %} --limit {{config.params.limit_samples}} {% endif %} {% if config.params.extra.judge.model_id
      is not none %} --judge-model-name {{config.params.extra.judge.model_id}} {%
      endif %} {% if config.type == "aegis_v2_reasoning" %} {% if config.params.extra.evaluate_reasoning_traces  %}
      --evaluate-reasoning-traces {% endif %} {% endif %}'
    framework_name: safety_eval
    pkg_name: safety_eval
    config:
      params:
        max_new_tokens: 6144
        max_retries: 5
        parallelism: 8
        task: wildguard
        temperature: 0.6
        request_timeout: 30
        top_p: 0.95
        extra:
          judge:
            url: null
            model_id: null
            api_key: null
            parallelism: 32
            request_timeout: 60
            max_retries: 16
      supported_endpoint_types:
      - chat
      - completions
      type: wildguard
    target:
      api_endpoint:
        stream: false
- name: medcalc_bench
  description: A dataset which consists of a patient note, a question requesting to
    compute a specific medical value, and a ground truth answer (Khandekar et al.,
    2024).
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: medcalc_bench
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: medcalc_bench
    target:
      api_endpoint: {}
- name: medec
  description: A dataset containing medical narratives with error detection and correction
    pairs (Abacha et al., 2025).
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: medec
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: medec
    target:
      api_endpoint: {}
- name: head_qa
  description: A collection of biomedical multiple-choice questions for testing medical
    knowledge (Vilares et al., 2019).
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: head_qa
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: head_qa
    target:
      api_endpoint: {}
- name: medbullets
  description: A USMLE-style medical question dataset with multiple-choice answers
    and explanations (MedBullets, 2025).
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: medbullets
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: medbullets
    target:
      api_endpoint: {}
- name: pubmed_qa
  description: A dataset that provides PubMed abstracts and asks associated questions
    (yes/no/maybe format).
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: pubmed_qa
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: pubmed_qa
    target:
      api_endpoint: {}
- name: ehr_sql
  description: Given a natural language instruction, generate an SQL query that would
    be used in clinical research.
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: ehr_sql
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: ehr_sql
    target:
      api_endpoint: {}
- name: race_based_med
  description: A collection of LLM outputs in response to medical questions with race-based
    biases, with the objective being to classify whether the output contains racially
    biased content.
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: race_based_med
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: race_based_med
    target:
      api_endpoint: {}
- name: medhallu
  description: A dataset of PubMed articles and associated questions, with the objective
    being to classify whether the answer is factual or hallucinated.
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: medhallu
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: medhallu
    target:
      api_endpoint: {}
- name: mtsamples_replicate
  description: Generate treatment plans based on clinical notes
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: mtsamples_replicate
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: mtsamples_replicate
    target:
      api_endpoint: {}
- name: aci_bench
  description: Extract and structure information from patient-doctor conversations
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: aci_bench
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: aci_bench
    target:
      api_endpoint: {}
- name: mtsamples_procedures
  description: Document and extract information about medical procedures
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: mtsamples_procedures
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: mtsamples_procedures
    target:
      api_endpoint: {}
- name: medication_qa
  description: Answer consumer medication-related questions
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: medication_qa
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: medication_qa
    target:
      api_endpoint: {}
- name: med_dialog_healthcaremagic
  description: Generate summaries of doctor-patient conversations, healthcaremagic
    version
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: med_dialog
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: healthcaremagic
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: med_dialog_healthcaremagic
    target:
      api_endpoint: {}
- name: med_dialog_icliniq
  description: Generate summaries of doctor-patient conversations, icliniq version
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: med_dialog
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: icliniq
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: med_dialog_icliniq
    target:
      api_endpoint: {}
- name: medi_qa
  description: Retrieve and rank answers based on medical question understanding
  harness: helm
  container: nvcr.io/nvidia/eval-factory/helm:25.11
  container_digest: sha256:a07bcdf0e6436cd7c4d4cd8591827a669c8e223da60dbce35d4eee3fdbcc73bd
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
      && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
      GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %}
      {% if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
      && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
      CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
      %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
      {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
      {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
      config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
      endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
      is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
      %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
      endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
      is not none %}  --num-train-trials {{config.params.extra.num_train_trials}}
      {% endif %} {% if config.params.extra.data_path is not none %}  --data-path
      {{config.params.extra.data_path}} {% endif %} {% if config.params.extra.num_output_tokens
      is not none %}  --num-output-tokens {{config.params.extra.num_output_tokens}}
      {% endif %} {% if config.params.extra.subject is not none %}  --subject {{config.params.extra.subject}}
      {% endif %} {% if config.params.extra.condition is not none %}  --condition
      {{config.params.extra.condition}} {% endif %} {% if config.params.extra.max_length
      is not none %}  --max-length {{config.params.extra.max_length}} {% endif %}  -o
      {{config.output_dir}}  --local-path {{config.output_dir}}'
    framework_name: helm
    pkg_name: helm
    config:
      params:
        parallelism: 1
        task: medi_qa
        extra:
          data_path: null
          num_output_tokens: null
          subject: null
          condition: null
          max_length: null
          num_train_trials: null
          subset: null
          gpt_judge_api_key: GPT_JUDGE_API_KEY
          llama_judge_api_key: LLAMA_JUDGE_API_KEY
          claude_judge_api_key: CLAUDE_JUDGE_API_KEY
      supported_endpoint_types:
      - chat
      type: medi_qa
    target:
      api_endpoint: {}
- name: tooltalk
  description: ToolTalk task with default settings.
  harness: tooltalk
  container: nvcr.io/nvidia/eval-factory/tooltalk:25.11
  container_digest: sha256:008c039d34e6e9efebdca52efa3ff1b484a4c88da526b855a0c0773eaf2ba7d0
  defaults:
    command: '{% if target.api_endpoint.api_key is not none %}API_KEY=${{target.api_endpoint.api_key}}{%
      endif %} python -m tooltalk.evaluation.evaluate_{{''openai'' if ''azure'' in
      target.api_endpoint.url or ''api.openai'' in target.api_endpoint.url else ''nim''}}
      --dataset data/easy --database data/databases --model {{target.api_endpoint.model_id}}
      {% if config.params.max_new_tokens is not none %}--max_new_tokens {{config.params.max_new_tokens}}{%
      endif %} {% if config.params.temperature is not none %}--temperature {{config.params.temperature}}{%
      endif %} {% if config.params.top_p is not none %}--top_p {{config.params.top_p}}{%
      endif %} --api_mode all --output_dir {{config.output_dir}} --url {{target.api_endpoint.url}}
      {% if config.params.limit_samples is not none %}--first_n {{config.params.limit_samples}}{%
      endif %}'
    framework_name: tooltalk
    pkg_name: tooltalk
    config:
      params:
        task: tooltalk
        extra: {}
      supported_endpoint_types:
      - chat
      type: tooltalk
    target:
      api_endpoint: {}
