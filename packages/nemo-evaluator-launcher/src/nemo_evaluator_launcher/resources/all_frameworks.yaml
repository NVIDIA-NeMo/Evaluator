framework:
  name: lm-evaluation-harness
  pkg_name: lm_evaluation_harness
  full_name: Language Model Evaluation Harness
  description: This project provides a unified framework to test generative language
    models on a large number of different evaluation tasks.
  url: https://github.com/EleutherAI/lm-evaluation-harness
  source: https://gitlab-master.nvidia.com/swdl-nemollm-mlops/evals/lm-evaluation-harness
defaults:
  command: '{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{%
    endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot
    is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %} --model
    {% if target.api_endpoint.type == "completions" %}local-completions{% elif target.api_endpoint.type
    == "chat" %}local-chat-completions{% endif %} --model_args "base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests={{config.params.extra.tokenized_requests}},{%
    if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{%
    endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{
    config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{
    target.api_endpoint.stream }}" --log_samples --output_path {{config.output_dir}}
    --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples is
    not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type
    == "chat" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args
    is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature
    is not none or config.params.top_p is not none or config.params.max_new_tokens
    is not none %}--gen_kwargs="{% if config.params.temperature is not none %}temperature={{
    config.params.temperature }}{% endif %}{% if config.params.top_p is not none %},top_p={{
    config.params.top_p}}{% endif %}{% if config.params.max_new_tokens is not none
    %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}"{% endif %} {% if
    config.params.extra.downsampling_ratio is not none %}--downsampling_ratio {{ config.params.extra.downsampling_ratio
    }}{% endif %}'
  config:
    params:
      limit_samples: null
      max_new_tokens: null
      temperature: 1.0e-07
      top_p: 0.9999999
      parallelism: 10
      max_retries: 5
      request_timeout: 30
      extra:
        tokenizer: null
        tokenizer_backend: None
        downsampling_ratio: null
        tokenized_requests: false
  target:
    api_endpoint:
      stream: false
evaluations:
- name: MMLU
  description: The MMLU (Massive Multitask Language Understanding) benchmark is designed
    to measure the knowledge acquired during pretraining by evaluating models in zero-shot
    and few-shot settings. It covers 57 subjects across various fields, testing both
    world knowledge and problem-solving abilities.
  defaults:
    config:
      type: mmlu
      supported_endpoint_types:
      - completions
      params:
        task: mmlu_str
        extra:
          num_fewshot: 5
          args: --trust_remote_code
- name: MMLU-instruct
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark is
    designed to measure the knowledge acquired during pretraining by evaluating models
    in zero-shot and few-shot settings. It covers 57 subjects across various fields,
    testing both world knowledge and problem-solving abilities. - This variant defaults
    to zero-shot evaluation and instructs the model to produce a single letter response.'
  defaults:
    config:
      type: mmlu_instruct
      supported_endpoint_types:
      - chat
      - completions
      params:
        task: mmlu_str
        extra:
          num_fewshot: 0
          args: --trust_remote_code --add_instruction
- name: MMLU-tigerlab
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark is
    designed to measure the knowledge acquired during pretraining by evaluating models
    in zero-shot and few-shot settings. It covers 57 subjects across various fields,
    testing both world knowledge and problem-solving abilities. - This variant defaults
    to chain-of-thought zero-shot evaluation.'
  defaults:
    config:
      type: mmlu_cot_0_shot_chat
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_cot_0_shot_chat
        extra:
          args: --trust_remote_code
- name: IFEval
  description: IFEval is a dataset designed to test a model's ability to follow explicit
    instructions, such as "include keyword x" or "use format y." The focus is on the
    model's adherence to formatting instructions rather than the content generated,
    allowing for the use of strict and rigorous metrics.
  defaults:
    config:
      type: ifeval
      supported_endpoint_types:
      - chat
      params:
        task: ifeval
- name: MMLU-Pro
  description: MMLU-Pro is a refined version of the MMLU dataset, which has been a
    standard for multiple-choice knowledge assessment. Recent research identified
    issues with the original MMLU, such as noisy data (some unanswerable questions)
    and decreasing difficulty due to advances in model capabilities and increased
    data contamination. MMLU-Pro addresses these issues by presenting models with
    10 choices instead of 4, requiring reasoning on more questions, and undergoing
    expert review to reduce noise. As a result, MMLU-Pro is of higher quality and
    currently more challenging than the original.
  defaults:
    config:
      type: mmlu_pro
      supported_endpoint_types:
      - chat
      - completions
      params:
        task: mmlu_pro
        extra:
          num_fewshot: 5
- name: MMLU-Pro-instruct
  description: '- MMLU-Pro is a refined version of the MMLU dataset, which has been
    a standard for multiple-choice knowledge assessment. Recent research identified
    issues with the original MMLU, such as noisy data (some unanswerable questions)
    and decreasing difficulty due to advances in model capabilities and increased
    data contamination. MMLU-Pro addresses these issues by presenting models with
    10 choices instead of 4, requiring reasoning on more questions, and undergoing
    expert review to reduce noise. As a result, MMLU-Pro is of higher quality and
    currently more challenging than the original. - This variant applies a chat template
    and defaults to zero-shot evaluation.'
  defaults:
    config:
      type: mmlu_pro_instruct
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_pro
        max_new_tokens: 1024
        extra:
          num_fewshot: 0
- name: MMLU-Redux
  description: MMLU-Redux is a subset of 3,000 manually re-annotated questions across
    30 MMLU subjects.
  defaults:
    config:
      type: mmlu_redux
      supported_endpoint_types:
      - completions
      params:
        task: mmlu_redux
- name: MMLU-Redux-instruct
  description: '- MMLU-Redux is a subset of 3,000 manually re-annotated questions
    across 30 MMLU subjects. - This variant applies a chat template and defaults to
    zero-shot evaluation.'
  defaults:
    config:
      type: mmlu_redux_instruct
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_redux
        max_new_tokens: 8192
        extra:
          num_fewshot: 0
          args: --add_instruction
- name: indonesian_mmlu
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark translated
    to Indonesian. - This variant uses the Indonesian version of the MMLU tasks with
    string-based evaluation.'
  defaults:
    config:
      type: m_mmlu_id_str
      supported_endpoint_types:
      - chat
      - completions
      params:
        task: m_mmlu_id_str
        extra:
          num_fewshot: 0
          args: --trust_remote_code
- name: GSM8K
  description: The GSM8K benchmark evaluates the arithmetic reasoning of large language
    models using 1,319 grade school math word problems.
  defaults:
    config:
      type: gsm8k
      supported_endpoint_types:
      - completions
      params:
        task: gsm8k
- name: GSM8K-instruct
  description: '- The GSM8K benchmark evaluates the arithmetic reasoning of large
    language models using 1,319 grade school math word problems. - This variant defaults
    to chain-of-thought zero-shot evaluation with custom instructions.'
  defaults:
    config:
      type: gsm8k_cot_instruct
      supported_endpoint_types:
      - chat
      params:
        task: gsm8k_zeroshot_cot
        extra:
          args: --add_instruction
- name: GSM8K-cot-zeroshot
  description: '- The GSM8K benchmark evaluates the arithmetic reasoning of large
    language models using 1,319 grade school math word problems. - This variant defaults
    to chain-of-thought zero-shot evaluation.'
  defaults:
    config:
      type: gsm8k_cot_zeroshot
      supported_endpoint_types:
      - chat
      params:
        task: gsm8k_cot_zeroshot
        max_new_tokens: 1024
- name: GSM8K-cot-llama
  description: '- The GSM8K benchmark evaluates the arithmetic reasoning of large
    language models using 1,319 grade school math word problems. - This variant defaults
    to chain-of-thought evaluation - implementation taken from llama.'
  defaults:
    config:
      type: gsm8k_cot_llama
      supported_endpoint_types:
      - chat
      params:
        task: gsm8k_cot_llama
        max_new_tokens: 1024
- name: GSM8K-cot-zeroshot-llama
  description: '- The GSM8K benchmark evaluates the arithmetic reasoning of large
    language models using 1,319 grade school math word problems. - This variant defaults
    to chain-of-thought zero-shot evaluation - implementation taken from llama.'
  defaults:
    config:
      type: gsm8k_cot_zeroshot_llama
      supported_endpoint_types:
      - chat
      params:
        task: gsm8k_cot_llama
        max_new_tokens: 1024
        extra:
          num_fewshot: 0
- name: HumanEval-instruct
  description: '- The HumanEval benchmark measures functional correctness for synthesizing
    programs from docstrings. - Implementation taken from llama.'
  defaults:
    config:
      type: humaneval_instruct
      supported_endpoint_types:
      - chat
      params:
        task: humaneval_instruct
- name: MBPP EvalPlus
  description: MBPP EvalPlus is an extension of the MBPP benchmark that explores the
    limits of the current generation of large language models for program synthesis
    in general purpose programming languages.
  defaults:
    config:
      type: mbpp_plus
      supported_endpoint_types:
      - chat
      - completions
      params:
        task: mbpp_plus
- name: MGSM
  description: The Multilingual Grade School Math (MGSM) benchmark evaluates the reasoning
    abilities of large language models in multilingual settings. It consists of 250
    grade-school math problems from the GSM8K dataset, translated into ten diverse
    languages, and tests models using chain-of-thought prompting.
  defaults:
    config:
      type: mgsm
      supported_endpoint_types:
      - completions
      params:
        task: mgsm_direct
- name: MGSM-CoT
  description: The Multilingual Grade School Math (MGSM) benchmark evaluates the reasoning
    abilities of large language models in multilingual settings. It consists of 250
    grade-school math problems from the GSM8K dataset, translated into ten diverse
    languages, and tests models using chain-of-thought prompting.
  defaults:
    config:
      type: mgsm_cot
      supported_endpoint_types:
      - chat
      - completions
      params:
        task: mgsm_cot_native
        max_new_tokens: 1024
        extra:
          num_fewshot: 0
- name: WikiLingua
  description: The WikiLingua benchmark is a large-scale, multilingual dataset designed
    for evaluating cross-lingual abstractive summarization systems. It includes approximately
    770,000 article-summary pairs in 18 languages, extracted from WikiHow, with gold-standard
    alignments created by matching images used to describe each how-to step in an
    article.
  defaults:
    config:
      type: wikilingua
      supported_endpoint_types:
      - chat
      params:
        task: wikilingua
        extra:
          args: --trust_remote_code
- name: winogrande
  description: WinoGrande is a collection of 44k problems, inspired by Winograd Schema
    Challenge (Levesque, Davis, and Morgenstern 2011), but adjusted to improve the
    scale and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank
    task with binary options, the goal is to choose the right option for a given sentence
    which requires commonsense reasoning.
  defaults:
    config:
      type: winogrande
      supported_endpoint_types:
      - completions
      params:
        task: winogrande
        extra:
          num_fewshot: 5
- name: ARC Challenge
  description: The ARC dataset consists of 7,787 science exam questions drawn from
    a variety of sources, including science questions provided under license by a
    research partner affiliated with AI2. These are text-only, English language exam
    questions that span several grade levels as indicated in the files. Each question
    has a multiple choice structure (typically 4 answer options). The questions are
    sorted into a Challenge Set of 2,590 "hard" questions (those that both a retrieval
    and a co-occurrence method fail to answer correctly) and an Easy Set of 5,197
    questions.
  defaults:
    config:
      type: arc_challenge
      supported_endpoint_types:
      - completions
      params:
        task: arc_challenge
- name: ARC Challenge-instruct
  description: '- The ARC dataset consists of 7,787 science exam questions drawn from
    a variety of sources, including science questions provided under license by a
    research partner affiliated with AI2. These are text-only, English language exam
    questions that span several grade levels as indicated in the files. Each question
    has a multiple choice structure (typically 4 answer options). The questions are
    sorted into a Challenge Set of 2,590 "hard" questions (those that both a retrieval
    and a co-occurrence method fail to answer correctly) and an Easy Set of 5,197
    questions. - This variant applies a chat template and defaults to zero-shot evaluation.'
  defaults:
    config:
      type: arc_challenge_chat
      supported_endpoint_types:
      - chat
      params:
        task: arc_challenge_chat
        max_new_tokens: 1024
        extra:
          num_fewshot: 0
- name: HellaSwag
  description: The HellaSwag benchmark tests a language model's commonsense reasoning
    by having it choose the most logical ending for a given story.
  defaults:
    config:
      type: hellaswag
      supported_endpoint_types:
      - completions
      params:
        task: hellaswag
        extra:
          num_fewshot: 10
- name: Truthful QA
  description: The TruthfulQA benchmark measures the truthfulness of language models
    in generating answers to questions. It consists of 817 questions across 38 categories,
    such as health, law, finance, and politics, designed to test whether models can
    avoid generating false answers that mimic common human misconceptions.
  defaults:
    config:
      type: truthfulqa
      params:
        task: truthfulqa
- name: BIG-Bench Hard
  description: The BIG-Bench Hard (BBH) benchmark is a part of the BIG-Bench evaluation
    suite, focusing on 23 particularly difficult tasks that current language models
    struggle with. These tasks require complex, multi-step reasoning, and the benchmark
    evaluates models using few-shot learning and chain-of-thought prompting techniques.
  defaults:
    config:
      type: bbh
      supported_endpoint_types:
      - completions
      params:
        task: leaderboard_bbh
- name: BIG-Bench Hard-instruct
  description: The BIG-Bench Hard (BBH) benchmark is a part of the BIG-Bench evaluation
    suite, focusing on 23 particularly difficult tasks that current language models
    struggle with. These tasks require complex, multi-step reasoning, and the benchmark
    evaluates models using few-shot learning and chain-of-thought prompting techniques.
  defaults:
    config:
      type: bbh_instruct
      supported_endpoint_types:
      - chat
      params:
        task: bbh_zeroshot
- name: MuSR
  description: The MuSR (Multistep Soft Reasoning) benchmark evaluates the reasoning
    capabilities of large language models through complex, multistep tasks specified
    in natural language narratives. It introduces sophisticated natural language and
    complex reasoning challenges to test the limits of chain-of-thought prompting.
  defaults:
    config:
      type: musr
      supported_endpoint_types:
      - completions
      params:
        task: leaderboard_musr
- name: GPQA
  description: The GPQA (Graduate-Level Google-Proof Q&A) benchmark is a challenging
    dataset of 448 multiple-choice questions in biology, physics, and chemistry. It
    is designed to be extremely difficult for both humans and AI, ensuring that questions
    cannot be easily answered using web searches.
  defaults:
    config:
      type: gpqa
      supported_endpoint_types:
      - completions
      params:
        task: leaderboard_gpqa
- name: GPQA-Diamond-CoT
  description: The GPQA (Graduate-Level Google-Proof Q&A) benchmark is a challenging
    dataset of 448 multiple-choice questions in biology, physics, and chemistry. It
    is designed to be extremely difficult for both humans and AI, ensuring that questions
    cannot be easily answered using web searches.
  defaults:
    config:
      type: gpqa_diamond_cot
      supported_endpoint_types:
      - chat
      params:
        task: gpqa_diamond_cot_zeroshot
        max_new_tokens: 1024
- name: Frames Naive
  description: Frames Naive uses the prompt as input without additional context
  defaults:
    config:
      type: frames_naive
      supported_endpoint_types:
      - chat
      params:
        task: frames_naive
        max_new_tokens: 2048
        temperature: 0.0
- name: Frames Naive with Links
  description: Frames Naive with Links provides the prompt and relevant Wikipedia
    article links
  defaults:
    config:
      type: frames_naive_with_links
      supported_endpoint_types:
      - chat
      params:
        task: frames_naive_with_links
        max_new_tokens: 2048
        temperature: 0.0
- name: Frames Oracle
  description: Frames Oracle (long context) provides prompts and relevant text from
    curated and processed Wikipedia articles from "parasail-ai/frames-benchmark-wikipedia".
  defaults:
    config:
      type: frames_oracle
      supported_endpoint_types:
      - chat
      params:
        task: frames_oracle
        max_new_tokens: 2048
        temperature: 0.0
        max_retries: 5
        timeout: 1000
- name: CommonsenseQA
  description: '- CommonsenseQA is a multiple-choice question answering dataset that
    requires different types of commonsense knowledge to predict the correct answers.
    - It contains 12,102 questions with one correct answer and four distractor answers.'
  defaults:
    config:
      type: commonsense_qa
      supported_endpoint_types:
      - completions
      params:
        task: commonsense_qa
        extra:
          num_fewshot: 7
- name: OpenBookQA
  description: '- OpenBookQA is a question-answering dataset modeled after open book
    exams for assessing human understanding of a subject. - It consists of 5,957 multiple-choice
    elementary-level science questions (4,957 train, 500 dev, 500 test), which probe
    the understanding of a small "book" of 1,326 core science facts and the application
    of these facts to novel situations. - For training, the dataset includes a mapping
    from each question to the core science fact it was designed to probe. - Answering
    OpenBookQA questions requires additional broad common knowledge, not contained
    in the book. - The questions, by design, are answered incorrectly by both a retrieval-based
    algorithm and a word co-occurrence algorithm.'
  defaults:
    config:
      type: openbookqa
      supported_endpoint_types:
      - completions
      params:
        task: openbookqa
- name: MMLU-Logits
  description: '- The MMLU (Massive Multitask Language Understanding) benchmark is
    designed to measure the knowledge acquired during pretraining by evaluating models
    in zero-shot and few-shot settings. - It covers 57 subjects across various fields,
    testing both world knowledge and problem-solving abilities. - This variant uses
    the logits of the model to evaluate the accuracy.'
  defaults:
    config:
      type: mmlu_logits
      supported_endpoint_types:
      - completions
      params:
        task: mmlu
        extra:
          num_fewshot: 5
- name: PIQA
  description: "- Physical Interaction: Question Answering (PIQA) is a physical commonsense\n\
    \  reasoning and a corresponding benchmark dataset. PIQA was designed to investigate\n\
    \  the physical knowledge of existing models. To what extent are current approaches\n\
    \  actually learning about the world?"
  defaults:
    config:
      type: piqa
      supported_endpoint_types:
      - completions
      params:
        task: piqa
- name: Social IQA
  description: '- Social IQa contains 38,000 multiple choice questions for probing
    emotional and social intelligence in a variety of everyday situations (e.g., Q:
    "Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did
    Jordan do this?" A: "Make sure no one else could hear").'
  defaults:
    config:
      type: social_iqa
      supported_endpoint_types:
      - completions
      params:
        task: social_iqa
        extra:
          args: --trust_remote_code
- name: ADLR AGIEval-EN-CoT
  description: ADLR version of the AGIEval-EN-CoT benchmark used by NVIDIA Applied
    Deep Learning Research team (ADLR).
  defaults:
    config:
      type: adlr_agieval_en_cot
      supported_endpoint_types:
      - completions
      params:
        task: adlr_agieval_en_cot
        temperature: 0.0
        top_p: 1.0e-05
- name: ADLR MATH-500 Sampled
  description: MATH-500 Sampled version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  defaults:
    config:
      type: adlr_math_500_4_shot_sampled
      supported_endpoint_types:
      - completions
      params:
        task: adlr_math_500_4_shot_sampled
        temperature: 0.7
        top_p: 1.0
        extra:
          num_fewshot: 4
- name: ADLR RACE
  description: RACE version used by NVIDIA Applied Deep Learning Research team (ADLR).
  defaults:
    config:
      type: adlr_race
      supported_endpoint_types:
      - completions
      params:
        task: adlr_race
        temperature: 1.0
        top_p: 1.0
- name: ADLR TruthfulQA-MC2
  description: TruthfulQA-MC2 version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  defaults:
    config:
      type: adlr_truthfulqa_mc2
      supported_endpoint_types:
      - completions
      params:
        task: adlr_truthfulqa_mc2
        temperature: 1.0
        top_p: 1.0
- name: ADLR ARC-Challenge-Llama
  description: ARC-Challenge-Llama version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  defaults:
    config:
      type: adlr_arc_challenge_llama_25_shot
      supported_endpoint_types:
      - completions
      params:
        task: adlr_arc_challenge_llama
        temperature: 1.0
        top_p: 1.0
        extra:
          num_fewshot: 25
- name: ADLR GPQA-Diamond-CoT
  description: ADLR version of the GPQA-Diamond-CoT benchmark used by NVIDIA Applied
    Deep Learning Research team (ADLR).
  defaults:
    config:
      type: adlr_gpqa_diamond_cot_5_shot
      supported_endpoint_types:
      - completions
      params:
        task: adlr_gpqa_diamond_cot_5_shot
        temperature: 0.0
        top_p: 1.0e-05
        extra:
          num_fewshot: 5
- name: ADLR MMLU
  description: MMLU version used by NVIDIA Applied Deep Learning Research team (ADLR).
  defaults:
    config:
      type: adlr_mmlu
      supported_endpoint_types:
      - completions
      params:
        task: mmlu_str
        temperature: 0.0
        top_p: 1.0e-05
        extra:
          num_fewshot: 5
          args: --trust_remote_code
- name: ADLR MMLU-Pro
  description: MMLU-Pro 5-shot base version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  defaults:
    config:
      type: adlr_mmlu_pro_5_shot_base
      supported_endpoint_types:
      - completions
      params:
        task: adlr_mmlu_pro_5_shot_base
        temperature: 0.0
        top_p: 1.0e-05
        extra:
          num_fewshot: 5
- name: ADLR Minerva-Math
  description: Minerva-Math version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  defaults:
    config:
      type: adlr_minerva_math_nemo_4_shot
      supported_endpoint_types:
      - completions
      params:
        task: adlr_minerva_math_nemo
        temperature: 0.0
        top_p: 1.0e-05
        extra:
          num_fewshot: 4
- name: ADLR GSM8K-CoT
  description: GSM8K-CoT version used by NVIDIA Applied Deep Learning Research team
    (ADLR).
  defaults:
    config:
      type: adlr_gsm8k_cot_8_shot
      supported_endpoint_types:
      - completions
      params:
        task: adlr_gsm8k_fewshot_cot
        temperature: 0.0
        top_p: 1.0e-05
        extra:
          num_fewshot: 8
- name: ADLR HumanEval Greedy
  description: HumanEval Greedy version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  defaults:
    config:
      type: adlr_humaneval_greedy
      supported_endpoint_types:
      - completions
      params:
        task: adlr_humaneval_greedy
        temperature: 0.0
        top_p: 1.0e-05
- name: ADLR HumanEval Sampled
  description: HumanEval Sampled version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  defaults:
    config:
      type: adlr_humaneval_sampled
      supported_endpoint_types:
      - completions
      params:
        task: adlr_humaneval_sampled
        temperature: 0.6
        top_p: 0.95
- name: ADLR MBPP Greedy
  description: MBPP Greedy version used by NVIDIA Applied Deep Learning Research team
    (ADLR).
  defaults:
    config:
      type: adlr_mbpp_sanitized_3_shot_greedy
      supported_endpoint_types:
      - completions
      params:
        task: adlr_mbpp_sanitized_3_shot_greedy
        temperature: 0.0
        top_p: 1.0e-05
        extra:
          num_fewshot: 3
- name: ADLR MBPP Sampled
  description: MBPP Sampled version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  defaults:
    config:
      type: adlr_mbpp_sanitized_3_shot_sampled
      supported_endpoint_types:
      - completions
      params:
        task: adlr_mbpp_sanitized_3shot_sampled
        temperature: 0.6
        top_p: 0.95
        extra:
          num_fewshot: 3
- name: ADLR Global-MMLU
  description: Global-MMLU subset (8 languages - es, de, fr, zh, it, ja, pt, ko) used
    by NVIDIA Applied Deep Learning Research team (ADLR).
  defaults:
    config:
      type: adlr_global_mmlu_lite_5_shot
      supported_endpoint_types:
      - completions
      params:
        task: adlr_global_mmlu
        temperature: 1.0
        top_p: 1.0
        extra:
          num_fewshot: 5
- name: ADLR MGSM-CoT
  description: MGSM native CoT subset (6 languages - es, de, fr, zh, ja, ru) used
    by NVIDIA Applied Deep Learning Research team (ADLR).
  defaults:
    config:
      type: adlr_mgsm_native_cot_8_shot
      supported_endpoint_types:
      - completions
      params:
        task: adlr_mgsm_native_cot_8_shot
        temperature: 0.0
        top_p: 1.0e-05
        extra:
          num_fewshot: 8
- name: ADLR CommonsenseQA
  description: CommonsenseQA version used by NVIDIA Applied Deep Learning Research
    team (ADLR).
  defaults:
    config:
      type: adlr_commonsense_qa_7_shot
      supported_endpoint_types:
      - completions
      params:
        task: commonsense_qa
        temperature: 1.0
        top_p: 1.0
        extra:
          num_fewshot: 7
- name: ADLR Winogrande
  description: Winogrande version used by NVIDIA Applied Deep Learning Research team
    (ADLR).
  defaults:
    config:
      type: adlr_winogrande_5_shot
      supported_endpoint_types:
      - completions
      params:
        task: winogrande
        temperature: 1.0
        top_p: 1.0
        extra:
          num_fewshot: 5
- name: BBQ
  description: The BBQ (Bias Benchmark for QA) is a benchmark designed to measure
    social biases in question answering systems. It contains ambiguous questions spanning
    9 categories - disability, gender, nationality, physical appearance, race/ethnicity,
    religion, sexual orientation, socioeconomic status, and age.
  defaults:
    config:
      type: bbq
      supported_endpoint_types:
      - chat
      - completions
      params:
        task: bbq_generate
- name: ARC Multilingual
  description: The ARC dataset consists of 7,787 science exam questions drawn from
    a variety of sources, including science questions provided under license by a
    research partner affiliated with AI2. These are text-only, English language exam
    questions that span several grade levels as indicated in the files. Each question
    has a multiple choice structure (typically 4 answer options). The questions are
    sorted into a Challenge Set of 2,590 "hard" questions (those that both a retrieval
    and a co-occurrence method fail to answer correctly) and an Easy Set of 5,197
    questions.
  defaults:
    config:
      type: arc_multilingual
      supported_endpoint_types:
      - completions
      params:
        task: arc_multilingual
- name: HellaSwag Multilingual
  description: The HellaSwag benchmark tests a language model's commonsense reasoning
    by having it choose the most logical ending for a given story.
  defaults:
    config:
      type: hellaswag_multilingual
      supported_endpoint_types:
      - completions
      params:
        task: hellaswag_multilingual
        extra:
          num_fewshot: 10
- name: MMLU-ProX
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
  defaults:
    config:
      type: mmlu_prox
      supported_endpoint_types:
      - chat
      - completions
      params:
        task: mmlu_prox
- name: MMLU-ProX-French
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    (French dataset)
  defaults:
    config:
      type: mmlu_prox_fr
      supported_endpoint_types:
      - chat
      - completions
      params:
        task: mmlu_prox_fr
- name: MMLU-ProX-German
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    (German dataset)
  defaults:
    config:
      type: mmlu_prox_de
      supported_endpoint_types:
      - chat
      - completions
      params:
        task: mmlu_prox_de
- name: MMLU-ProX-Italian
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    (Italian dataset)
  defaults:
    config:
      type: mmlu_prox_it
      supported_endpoint_types:
      - chat
      - completions
      params:
        task: mmlu_prox_it
- name: MMLU-ProX-Japanese
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    (Japanese dataset)
  defaults:
    config:
      type: mmlu_prox_ja
      supported_endpoint_types:
      - chat
      - completions
      params:
        task: mmlu_prox_ja
- name: MMLU-ProX-Spanish
  description: A Multilingual Benchmark for Advanced Large Language Model Evaluation
    (Spanish dataset)
  defaults:
    config:
      type: mmlu_prox_es
      supported_endpoint_types:
      - chat
      - completions
      params:
        task: mmlu_prox_es
- name: Global-MMLU-Full
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full
- name: Global-MMLU-Full-AM
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_am
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_am
- name: Global-MMLU-Full-AR
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_ar
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_ar
- name: Global-MMLU-Full-BN
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_bn
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_bn
- name: Global-MMLU-Full-CS
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_cs
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_cs
- name: Global-MMLU-Full-DE
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_de
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_de
- name: Global-MMLU-Full-EL
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_el
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_el
- name: Global-MMLU-Full-EN
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_en
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_en
- name: Global-MMLU-Full-ES
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_es
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_es
- name: Global-MMLU-Full-FA
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_fa
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_fa
- name: Global-MMLU-Full-FIL
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_fil
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_fil
- name: Global-MMLU-Full-FR
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_fr
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_fr
- name: Global-MMLU-Full-HA
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_ha
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_ha
- name: Global-MMLU-Full-HE
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_he
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_he
- name: Global-MMLU-Full-HI
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_hi
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_hi
- name: Global-MMLU-Full-ID
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_id
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_id
- name: Global-MMLU-Full-IG
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_ig
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_ig
- name: Global-MMLU-Full-IT
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_it
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_it
- name: Global-MMLU-Full-JA
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_ja
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_ja
- name: Global-MMLU-Full-KO
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_ko
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_ko
- name: Global-MMLU-Full-KY
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_ky
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_ky
- name: Global-MMLU-Full-LT
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_lt
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_lt
- name: Global-MMLU-Full-MG
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_mg
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_mg
- name: Global-MMLU-Full-MS
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_ms
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_ms
- name: Global-MMLU-Full-NE
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_ne
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_ne
- name: Global-MMLU-Full-NL
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_nl
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_nl
- name: Global-MMLU-Full-NY
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_ny
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_ny
- name: Global-MMLU-Full-PL
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_pl
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_pl
- name: Global-MMLU-Full-PT
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_pt
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_pt
- name: Global-MMLU-Full-RO
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_ro
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_ro
- name: Global-MMLU-Full-RU
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_ru
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_ru
- name: Global-MMLU-Full-SI
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_si
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_si
- name: Global-MMLU-Full-SN
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_sn
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_sn
- name: Global-MMLU-Full-SO
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_so
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_so
- name: Global-MMLU-Full-SR
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_sr
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_sr
- name: Global-MMLU-Full-SV
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_sv
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_sv
- name: Global-MMLU-Full-SW
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_sw
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_sw
- name: Global-MMLU-Full-TE
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_te
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_te
- name: Global-MMLU-Full-TR
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_tr
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_tr
- name: Global-MMLU-Full-UK
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_uk
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_uk
- name: Global-MMLU-Full-VI
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_vi
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_vi
- name: Global-MMLU-Full-YO
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_yo
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_yo
- name: Global-MMLU-Full-ZH
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_full_zh
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_full_zh
- name: Global-MMLU
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu
- name: Global-MMLU-AR
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_ar
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_ar
- name: Global-MMLU-BN
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_bn
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_bn
- name: Global-MMLU-DE
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_de
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_de
- name: Global-MMLU-EN
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_en
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_en
- name: Global-MMLU-ES
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_es
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_es
- name: Global-MMLU-FR
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_fr
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_fr
- name: Global-MMLU-HI
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_hi
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_hi
- name: Global-MMLU-ID
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_id
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_id
- name: Global-MMLU-IT
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_it
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_it
- name: Global-MMLU-JA
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_ja
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_ja
- name: Global-MMLU-KO
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_ko
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_ko
- name: Global-MMLU-PT
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_pt
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_pt
- name: Global-MMLU-SW
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_sw
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_sw
- name: Global-MMLU-YO
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_yo
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_yo
- name: Global-MMLU-ZH
  description: Understanding and Addressing Cultural and Linguistic Biases in Multilingual
    Evaluation
  defaults:
    config:
      type: global_mmlu_zh
      supported_endpoint_types:
      - completions
      params:
        task: global_mmlu_zh
- name: AGIEval
  description: AGIEval - A Human-Centric Benchmark for Evaluating Foundation Models
  defaults:
    config:
      type: agieval
      supported_endpoint_types:
      - completions
      params:
        task: agieval
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/lm-evaluation-harness:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:1612de5cfdd7aeb6a9a380d0fcf4e2504ddd7dcee6da9b23f1167fe541dbed64
---
framework:
  name: mtbench
  pkg_name: mtbench
  full_name: MT-Bench
  description: MT-bench is designed to test multi-turn conversation and instruction-following
    ability, covering common use cases and focusing on challenging questions to differentiate
    models.
  url: https://github.com/lm-sys/FastChat
  source: https://gitlab-master.nvidia.com/mbien/mtbench-evaluator
defaults:
  command: 'mtbench-evaluator {% if target.api_endpoint.model_id is not none %} --model
    {{target.api_endpoint.model_id}}{% endif %} {% if target.api_endpoint.url is not
    none %} --url {{target.api_endpoint.url}}{% endif %} {% if target.api_endpoint.api_key
    is not none %} --api_key {{target.api_endpoint.api_key}}{% endif %} {% if config.params.request_timeout
    is not none %} --timeout {{config.params.request_timeout}}{% endif %} {% if config.params.max_retries
    is not none %} --max_retries {{config.params.max_retries}}{% endif %} {% if config.params.parallelism
    is not none %} --parallelism {{config.params.parallelism}}{% endif %} {% if config.params.max_new_tokens
    is not none %} --max_tokens {{config.params.max_new_tokens}}{% endif %} --workdir
    {{config.output_dir}} {% if config.params.temperature is not none %} --temperature
    {{config.params.temperature}}{% endif %} {% if config.params.top_p is not none
    %} --top_p {{config.params.top_p}}{% endif %} {% if config.params.extra.args is
    defined %} {{config.params.extra.args}} {% endif %} {% if config.params.limit_samples
    is not none %}--first_n {{config.params.limit_samples}}{% endif %} --generate
    --judge {% if config.params.extra.judge.url is not none %} --judge_url {{config.params.extra.judge.url}}{%
    endif %} {% if config.params.extra.judge.model_id is not none %} --judge_model
    {{config.params.extra.judge.model_id}}{% endif %} {% if config.params.extra.judge.api_key
    is not none %} --judge_api_key_name {{config.params.extra.judge.api_key}}{% endif
    %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
    {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
    is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
    endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
    {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
    is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %} {%
    if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens {{config.params.extra.judge.max_tokens}}{%
    endif %}     '
  config:
    params:
      max_new_tokens: 1024
      parallelism: 10
      max_retries: 5
      request_timeout: 30
      extra:
        judge:
          url: null
          model_id: gpt-4
          api_key: null
          request_timeout: 60
          max_retries: 16
          temperature: 0.0
          top_p: 0.0001
          max_tokens: 2048
  target:
    api_endpoint: {}
evaluations:
- name: mtbench
  description: Standard MT-Bench
  defaults:
    config:
      type: mtbench
      supported_endpoint_types:
      - chat
      params:
        task: mtbench
- name: mtbench-cor1
  description: Corrected MT-Bench
  defaults:
    config:
      type: mtbench-cor1
      supported_endpoint_types:
      - chat
      params:
        task: mtbench-cor1
        extra:
          args: --judge_reference_model gpt-4-0125-preview
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/mtbench:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:d460a64db8ed8944966946cdfb2b74c590f881a50e9b9af3a9fc48dd65262848
---
framework:
  name: ifbench
  pkg_name: ifbench
  full_name: Generalizing Verifiable Instruction Following
  description: This repo contains IFBench, which is a new, challenging benchmark for
    precise instruction following.
  url: https://github.com/allenai/IFBench
  source: https://github.com/allenai/IFBench
defaults:
  command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}  &&
    {% endif %} ifbench --model-url {{target.api_endpoint.url}} --model-name {{target.api_endpoint.model_id}}  --results-dir
    {{config.output_dir}} --inference-params max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}}
    --parallelism {{config.params.parallelism}} --retries {{config.params.max_retries}}
    {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}
    {% endif %}'
  config:
    supported_endpoint_types:
    - chat
    params:
      limit_samples: null
      max_new_tokens: 4096
      temperature: 0.01
      top_p: 0.95
      parallelism: 8
      max_retries: 5
  target:
    api_endpoint:
      stream: false
evaluations:
- name: ifbench
  description: ifbench
  defaults:
    config:
      type: ifbench
      params:
        task: ifbench
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/ifbench:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:38bb1ab96c495591b3cebbcf9bb87251a1a1a46abe36b2e067a8217d973fe6c5
---
framework:
  name: simple_evals
  pkg_name: simple_evals
  full_name: simple-evals
  description: simple-evals This repository contains a lightweight library for evaluating
    language models.
  url: https://github.com/openai/simple-evals
  source: https://gitlab-master.nvidia.com/dl/JoC/competitive_evaluation/simple-evals
defaults:
  command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
    && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
    is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
    | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
    "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml",
    "w"), default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
    --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
    {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
    --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
    --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
    --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
    is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {% if
    config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%
    endif %} {% if config.params.extra.add_system_prompt  %} --add_system_prompt {%
    endif %} {% if config.params.extra.downsampling_ratio is not none %} --downsampling_ratio
    {{config.params.extra.downsampling_ratio}}{% endif %} {% if config.params.extra.args
    is defined %} {{ config.params.extra.args }} {% endif %} {% if config.params.extra.judge.url
    is not none %} --judge_url {{config.params.extra.judge.url}}{% endif %} {% if
    config.params.extra.judge.model_id is not none %} --judge_model_id {{config.params.extra.judge.model_id}}{%
    endif %} {% if config.params.extra.judge.api_key is not none %} --judge_api_key_name
    {{config.params.extra.judge.api_key}}{% endif %} {% if config.params.extra.judge.backend
    is not none %} --judge_backend {{config.params.extra.judge.backend}}{% endif %}
    {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
    {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
    is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{%
    endif %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
    {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
    is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %} {%
    if config.params.extra.judge.max_tokens is not none %} --judge_max_tokens {{config.params.extra.judge.max_tokens}}{%
    endif %} {% if config.params.extra.judge.max_concurrent_requests is not none %}
    --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
    endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
    is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
    endif %}'
  config:
    params:
      limit_samples: null
      max_new_tokens: 4096
      temperature: 0
      top_p: 1.0e-05
      parallelism: 10
      max_retries: 5
      request_timeout: 60
      extra:
        downsampling_ratio: null
        add_system_prompt: false
        custom_config: null
        judge:
          url: null
          model_id: null
          api_key: null
          backend: openai
          request_timeout: 600
          max_retries: 16
          temperature: 0.0
          top_p: 0.0001
          max_tokens: 1024
          max_concurrent_requests: null
  target:
    api_endpoint: {}
evaluations:
- name: AIME_2025
  description: AIME 2025 questions, math
  defaults:
    config:
      type: AIME_2025
      supported_endpoint_types:
      - chat
      params:
        task: AIME_2025
        extra:
          n_samples: 10
          judge:
            api_key: JUDGE_API_KEY
- name: AIME_2024
  description: AIME 2024 questions, math
  defaults:
    config:
      type: AIME_2024
      supported_endpoint_types:
      - chat
      params:
        task: AIME_2024
- name: AA_AIME_2024
  description: AA AIME 2024 questions, math
  defaults:
    config:
      type: AA_AIME_2024
      supported_endpoint_types:
      - chat
      params:
        task: AA_AIME_2024
        extra:
          n_samples: 10
          judge:
            api_key: JUDGE_API_KEY
- name: AA_math_test_500
  description: AA Open Ai math test 500
  defaults:
    config:
      type: AA_math_test_500
      supported_endpoint_types:
      - chat
      params:
        task: AA_math_test_500
        extra:
          n_samples: 3
          judge:
            api_key: JUDGE_API_KEY
- name: math_test_500
  description: Open Ai math test 500
  defaults:
    config:
      type: math_test_500
      supported_endpoint_types:
      - chat
      params:
        task: math_test_500
- name: mgsm
  description: MGSM is a benchmark of grade-school math problems. The same 250 problems
    from GSM8K are each translated via human annotators in 10 languages.
  defaults:
    config:
      type: mgsm
      supported_endpoint_types:
      - chat
      params:
        task: mgsm
- name: humaneval
  description: HumanEval evaluates the performance in Python code generation tasks.
    It used to measure functional correctness for synthesizing programs from docstrings.
    It consists of 164 original programming problems, assessing language comprehension,
    algorithms, and simple mathematics, with some comparable to simple software interview
    questions.
  defaults:
    config:
      type: humaneval
      supported_endpoint_types:
      - chat
      params:
        task: humaneval
        n_repeats: 1
- name: humanevalplus
  description: HumanEvalPlus is a dataset of 164 programming problems, assessing language
    comprehension, algorithms, and simple mathematics, with some comparable to simple
    software interview questions.
  defaults:
    config:
      type: humanevalplus
      supported_endpoint_types:
      - chat
      params:
        task: humanevalplus
        n_repeats: 1
- name: mmlu_pro
  description: MMLU-Pro dataset is a more robust and challenging massive multi-task
    understanding dataset tailored to more rigorously benchmark large language models'
    capabilities. This dataset contains 12K complex questions across various disciplines.
  defaults:
    config:
      type: mmlu_pro
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_pro
- name: mmlu_am
  description: MMLU 0-shot CoT in Amharic (am)
  defaults:
    config:
      type: mmlu_am
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_am
- name: mmlu_ar
  description: MMLU 0-shot CoT in Arabic (ar)
  defaults:
    config:
      type: mmlu_ar
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ar
- name: mmlu_bn
  description: MMLU 0-shot CoT in Bengali (bn)
  defaults:
    config:
      type: mmlu_bn
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_bn
- name: mmlu_cs
  description: MMLU 0-shot CoT in Czech (cs)
  defaults:
    config:
      type: mmlu_cs
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_cs
- name: mmlu_de
  description: MMLU 0-shot CoT in German (de)
  defaults:
    config:
      type: mmlu_de
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_de
- name: mmlu_el
  description: MMLU 0-shot CoT in Greek (el)
  defaults:
    config:
      type: mmlu_el
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_el
- name: mmlu_en
  description: MMLU 0-shot CoT in English (en)
  defaults:
    config:
      type: mmlu_en
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_en
- name: mmlu_es
  description: MMLU 0-shot CoT in Spanish (es)
  defaults:
    config:
      type: mmlu_es
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_es
- name: mmlu_fa
  description: MMLU 0-shot CoT in Persian (fa)
  defaults:
    config:
      type: mmlu_fa
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_fa
- name: mmlu_fil
  description: MMLU 0-shot CoT in Filipino (fil)
  defaults:
    config:
      type: mmlu_fil
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_fil
- name: mmlu_fr
  description: MMLU 0-shot CoT in French (fr)
  defaults:
    config:
      type: mmlu_fr
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_fr
- name: mmlu_ha
  description: MMLU 0-shot CoT in Hausa (ha)
  defaults:
    config:
      type: mmlu_ha
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ha
- name: mmlu_he
  description: MMLU 0-shot CoT in Hebrew (he)
  defaults:
    config:
      type: mmlu_he
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_he
- name: mmlu_hi
  description: MMLU 0-shot CoT in Hindi (hi)
  defaults:
    config:
      type: mmlu_hi
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_hi
- name: mmlu_id
  description: MMLU 0-shot CoT in Indonesian (id)
  defaults:
    config:
      type: mmlu_id
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_id
- name: mmlu_ig
  description: MMLU 0-shot CoT in Igbo (ig)
  defaults:
    config:
      type: mmlu_ig
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ig
- name: mmlu_it
  description: MMLU 0-shot CoT in Italian (it)
  defaults:
    config:
      type: mmlu_it
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_it
- name: mmlu_ja
  description: MMLU 0-shot CoT in Japanese (ja)
  defaults:
    config:
      type: mmlu_ja
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ja
- name: mmlu_ko
  description: MMLU 0-shot CoT in Korean (ko)
  defaults:
    config:
      type: mmlu_ko
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ko
- name: mmlu_ky
  description: MMLU 0-shot CoT in Kyrgyz (ky)
  defaults:
    config:
      type: mmlu_ky
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ky
- name: mmlu_lt
  description: MMLU 0-shot CoT in Lithuanian (lt)
  defaults:
    config:
      type: mmlu_lt
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_lt
- name: mmlu_mg
  description: MMLU 0-shot CoT in Malagasy (mg)
  defaults:
    config:
      type: mmlu_mg
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_mg
- name: mmlu_ms
  description: MMLU 0-shot CoT in Malay (ms)
  defaults:
    config:
      type: mmlu_ms
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ms
- name: mmlu_ne
  description: MMLU 0-shot CoT in Nepali (ne)
  defaults:
    config:
      type: mmlu_ne
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ne
- name: mmlu_nl
  description: MMLU 0-shot CoT in Dutch (nl)
  defaults:
    config:
      type: mmlu_nl
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_nl
- name: mmlu_ny
  description: MMLU 0-shot CoT in Nyanja (ny)
  defaults:
    config:
      type: mmlu_ny
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ny
- name: mmlu_pl
  description: MMLU 0-shot CoT in Polish (pl)
  defaults:
    config:
      type: mmlu_pl
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_pl
- name: mmlu_pt
  description: MMLU 0-shot CoT in Portuguese (pt)
  defaults:
    config:
      type: mmlu_pt
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_pt
- name: mmlu_ro
  description: MMLU 0-shot CoT in Romanian (ro)
  defaults:
    config:
      type: mmlu_ro
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ro
- name: mmlu_ru
  description: MMLU 0-shot CoT in Russian (ru)
  defaults:
    config:
      type: mmlu_ru
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ru
- name: mmlu_si
  description: MMLU 0-shot CoT in Sinhala (si)
  defaults:
    config:
      type: mmlu_si
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_si
- name: mmlu_sn
  description: MMLU 0-shot CoT in Shona (sn)
  defaults:
    config:
      type: mmlu_sn
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_sn
- name: mmlu_so
  description: MMLU 0-shot CoT in Somali (so)
  defaults:
    config:
      type: mmlu_so
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_so
- name: mmlu_sr
  description: MMLU 0-shot CoT in Serbian (sr)
  defaults:
    config:
      type: mmlu_sr
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_sr
- name: mmlu_sv
  description: MMLU 0-shot CoT in Swedish (sv)
  defaults:
    config:
      type: mmlu_sv
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_sv
- name: mmlu_sw
  description: MMLU 0-shot CoT in Swahili (sw)
  defaults:
    config:
      type: mmlu_sw
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_sw
- name: mmlu_te
  description: MMLU 0-shot CoT in Telugu (te)
  defaults:
    config:
      type: mmlu_te
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_te
- name: mmlu_tr
  description: MMLU 0-shot CoT in Turkish (tr)
  defaults:
    config:
      type: mmlu_tr
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_tr
- name: mmlu_uk
  description: MMLU 0-shot CoT in Ukrainian (uk)
  defaults:
    config:
      type: mmlu_uk
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_uk
- name: mmlu_vi
  description: MMLU 0-shot CoT in Vietnamese (vi)
  defaults:
    config:
      type: mmlu_vi
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_vi
- name: mmlu_yo
  description: MMLU 0-shot CoT in Yoruba (yo)
  defaults:
    config:
      type: mmlu_yo
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_yo
- name: mmlu_ar-lite
  description: Lite version of the MMLU 0-shot CoT in Arabic (ar)
  defaults:
    config:
      type: mmlu_ar-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ar-lite
- name: mmlu_bn-lite
  description: Lite version of the MMLU 0-shot CoT in Bengali (bn)
  defaults:
    config:
      type: mmlu_bn-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_bn-lite
- name: mmlu_de-lite
  description: Lite version of the MMLU 0-shot CoT in German (de)
  defaults:
    config:
      type: mmlu_de-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_de-lite
- name: mmlu_en-lite
  description: Lite version of the MMLU 0-shot CoT in English (en)
  defaults:
    config:
      type: mmlu_en-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_en-lite
- name: mmlu_es-lite
  description: Lite version of the MMLU 0-shot CoT in Spanish (es)
  defaults:
    config:
      type: mmlu_es-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_es-lite
- name: mmlu_fr-lite
  description: Lite version of the MMLU 0-shot CoT in French (fr)
  defaults:
    config:
      type: mmlu_fr-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_fr-lite
- name: mmlu_hi-lite
  description: Lite version of the MMLU 0-shot CoT in Hindi (hi)
  defaults:
    config:
      type: mmlu_hi-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_hi-lite
- name: mmlu_id-lite
  description: Lite version of the MMLU 0-shot CoT in Indonesian (id)
  defaults:
    config:
      type: mmlu_id-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_id-lite
- name: mmlu_it-lite
  description: Lite version of the MMLU 0-shot CoT in Italian (it)
  defaults:
    config:
      type: mmlu_it-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_it-lite
- name: mmlu_ja-lite
  description: Lite version of the MMLU 0-shot CoT in Japanese (ja)
  defaults:
    config:
      type: mmlu_ja-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ja-lite
- name: mmlu_ko-lite
  description: Lite version of the MMLU 0-shot CoT in Korean (ko)
  defaults:
    config:
      type: mmlu_ko-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_ko-lite
- name: mmlu_my-lite
  description: Lite version of the MMLU 0-shot CoT in Malay (my)
  defaults:
    config:
      type: mmlu_my-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_my-lite
- name: mmlu_pt-lite
  description: Lite version of the MMLU 0-shot CoT in Portuguese (pt)
  defaults:
    config:
      type: mmlu_pt-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_pt-lite
- name: mmlu_sw-lite
  description: Lite version of the MMLU 0-shot CoT in Swahili (sw)
  defaults:
    config:
      type: mmlu_sw-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_sw-lite
- name: mmlu_yo-lite
  description: Lite version of the MMLU 0-shot CoT in Yoruba (yo)
  defaults:
    config:
      type: mmlu_yo-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_yo-lite
- name: mmlu_zh-lite
  description: Lite version of the MMLU 0-shot CoT in Chinese (Simplified) (zh)
  defaults:
    config:
      type: mmlu_zh-lite
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_zh-lite
- name: mmlu
  description: MMLU 0-shot CoT
  defaults:
    config:
      type: mmlu
      supported_endpoint_types:
      - chat
      params:
        task: mmlu
- name: gpqa_diamond
  description: gpqa_diamond 0-shot CoT
  defaults:
    config:
      type: gpqa_diamond
      supported_endpoint_types:
      - chat
      params:
        task: gpqa_diamond
- name: gpqa_extended
  description: gpqa_extended 0-shot CoT
  defaults:
    config:
      type: gpqa_extended
      supported_endpoint_types:
      - chat
      params:
        task: gpqa_extended
- name: gpqa_main
  description: gpqa_main 0-shot CoT
  defaults:
    config:
      type: gpqa_main
      supported_endpoint_types:
      - chat
      params:
        task: gpqa_main
- name: simpleqa
  description: A factuality benchmark called SimpleQA that measures the ability for
    language models to answer short, fact-seeking questions.
  defaults:
    config:
      type: simpleqa
      supported_endpoint_types:
      - chat
      params:
        task: simpleqa
- name: aime_2025_nemo
  description: AIME 2025 questions, math, using NeMo's alignment template
  defaults:
    config:
      type: aime_2025_nemo
      supported_endpoint_types:
      - chat
      params:
        task: aime_2025_nemo
        extra:
          n_samples: 10
- name: aime_2024_nemo
  description: AIME 2024 questions, math, using NeMo's alignment template
  defaults:
    config:
      type: aime_2024_nemo
      supported_endpoint_types:
      - chat
      params:
        task: aime_2024_nemo
        extra:
          n_samples: 10
- name: math_test_500_nemo
  description: math_test_500 questions, math, using NeMo's alignment template
  defaults:
    config:
      type: math_test_500_nemo
      supported_endpoint_types:
      - chat
      params:
        task: math_test_500_nemo
        extra:
          n_samples: 3
- name: gpqa_diamond_nemo
  description: gpqa_diamond questions, reasoning, using NeMo's alignment template
  defaults:
    config:
      type: gpqa_diamond_nemo
      supported_endpoint_types:
      - chat
      params:
        task: gpqa_diamond_nemo
        extra:
          n_samples: 5
- name: gpqa_diamond_aa_v2_llama_4
  description: gpqa_diamond questions with custom regex extraction patterns for Llama
    4
  defaults:
    config:
      type: gpqa_diamond_aa_v2_llama_4
      supported_endpoint_types:
      - chat
      params:
        task: gpqa_diamond
        extra:
          n_samples: 5
          custom_config:
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_colon_llama4
            - regex: (?i)(?:the )?best? answer is\s*[\*\_,{}\.]*([A-D])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_is_llama4
- name: gpqa_diamond_aa_v2
  description: gpqa_diamond questions with custom regex extraction patterns for AA
    v2
  defaults:
    config:
      type: gpqa_diamond_aa_v2
      supported_endpoint_types:
      - chat
      params:
        task: gpqa_diamond
        extra:
          n_samples: 5
          custom_config:
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: aa_v2_regex
- name: mmlu_llama_4
  description: MMLU questions with custom regex extraction patterns for Llama 4
  defaults:
    config:
      type: mmlu_llama_4
      supported_endpoint_types:
      - chat
      params:
        task: mmlu
        extra:
          custom_config:
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_colon_llama4
            - regex: (?i)(?:the )?best? answer is\s*[\*\_,{}\.]*([A-D])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_is_llama4
- name: mmlu_pro_llama_4
  description: MMLU-Pro questions with custom regex extraction patterns for Llama
    4
  defaults:
    config:
      type: mmlu_pro_llama_4
      supported_endpoint_types:
      - chat
      params:
        task: mmlu_pro
        extra:
          custom_config:
            extraction:
            - regex: (?i)[\*\_]{0,2}Answer[\*\_]{0,2}\s*:[\s\*\_]{0,2}\s*([A-Z])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_colon_llama4
            - regex: (?i)(?:the )?best? answer is\s*[\*\_,{}\.]*([A-D])(?![a-zA-Z0-9])
              match_group: 1
              name: answer_is_llama4
- name: healthbench
  description: HealthBench is an open-source benchmark measuring the performance and
    safety of large language models in healthcare.
  defaults:
    config:
      type: healthbench
      supported_endpoint_types:
      - chat
      params:
        task: healthbench
        extra:
          judge:
            api_key: JUDGE_API_KEY
- name: healthbench_consensus
  description: HealthBench is an open-source benchmark measuring the performance and
    safety of large language models in healthcare. The consensus subset measures 34
    particularly important aspects of model behavior and has been validated by the
    consensus of multiple physicians.
  defaults:
    config:
      type: healthbench_consensus
      supported_endpoint_types:
      - chat
      params:
        task: healthbench_consensus
        extra:
          judge:
            api_key: JUDGE_API_KEY
- name: healthbench_hard
  description: HealthBench is an open-source benchmark measuring the performance and
    safety of large language models in healthcare. The hard subset consists of 1000
    examples chosen because they are difficult for current frontier models.
  defaults:
    config:
      type: healthbench_hard
      supported_endpoint_types:
      - chat
      params:
        task: healthbench_hard
        extra:
          judge:
            api_key: JUDGE_API_KEY
- name: browsecomp
  description: BrowseComp is a benchmark for measuring the ability for agents to browse
    the web.
  defaults:
    config:
      type: browsecomp
      supported_endpoint_types:
      - chat
      params:
        task: browsecomp
        extra:
          judge:
            api_key: JUDGE_API_KEY
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/simple-evals:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:bb86e9fe35452679cacd362cfcc2b9485661108569a5d57565e3275e784f7b46
---
framework:
  name: bigcode-evaluation-harness
  pkg_name: bigcode_evaluation_harness
  full_name: Code Generation LM Evaluation Harness
  description: A framework for the evaluation of autoregressive code generation language
    models.
  url: https://github.com/bigcode-project/bigcode-evaluation-harness
  source: https://gitlab-master.nvidia.com/swdl-nemollm-mlops/evals/bigcode-evaluation-harness
defaults:
  command: '{% if target.api_endpoint.api_key is not none %}NVCF_TOKEN=${{target.api_endpoint.api_key}}{%
    endif %} bigcode-eval --model_type {% if target.api_endpoint.type == "completions"
    %}nim-base{% elif target.api_endpoint.type == "chat" %}nim-chat{% endif %} --url
    {{target.api_endpoint.url}} --model_kwargs ''{"model_name": "{{target.api_endpoint.model_id}}",
    "timeout": {{config.params.request_timeout}}, "connection_retries": {{config.params.max_retries}}}''
    --out_dir {{config.output_dir}} --task {{config.params.task}} --allow_code_execution
    --n_samples={{config.params.extra.n_samples}} {% if config.params.limit_samples
    is not none %}--limit {{config.params.limit_samples}}{% endif %} --max_new_tokens={{config.params.max_new_tokens}}
    --do_sample={{config.params.extra.do_sample}} --top_p {{config.params.top_p}}
    --temperature {{config.params.temperature}} --async_limit {{config.params.parallelism}}{%
    if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif
    %}'
  config:
    supported_endpoint_types:
    - completions
    params:
      limit_samples: null
      max_new_tokens: 512
      temperature: 1.0e-07
      top_p: 0.9999999
      parallelism: 10
      max_retries: 5
      request_timeout: 30
      extra:
        do_sample: true
        n_samples: 1
  target:
    api_endpoint: {}
evaluations:
- name: HumanEval
  description: HumanEval is used to measure functional correctness for synthesizing
    programs from docstrings. It consists of 164 original programming problems, assessing
    language comprehension, algorithms, and simple mathematics, with some comparable
    to simple software interview questions.
  defaults:
    config:
      type: humaneval
      supported_endpoint_types:
      - completions
      params:
        task: humaneval
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 20
- name: HumanEval-Instruct
  description: InstructHumanEval is a modified version of OpenAI HumanEval. For a
    given prompt, we extracted its signature, its docstring as well as its header
    to create a flexing setting which would allow to evaluation instruction-tuned
    LLM. The delimiters used in the instruction-tuning procedure can be use to build
    and instruction that would allow the model to elicit its best capabilities.
  defaults:
    config:
      type: humaneval_instruct
      supported_endpoint_types:
      - chat
      params:
        task: instruct-humaneval-nocontext-py
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 20
- name: HumanEval+
  description: HumanEvalPlus is a modified version of HumanEval containing 80x more
    test cases.
  defaults:
    config:
      type: humanevalplus
      supported_endpoint_types:
      - completions
      params:
        task: humanevalplus
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MBPP
  description: MBPP consists of Python programming problems, designed to be solvable
    by entry level programmers, covering programming fundamentals, standard library
    functionality, and so on. Each problem consists of a task description, code solution
    and 3 automated test cases.
  defaults:
    config:
      type: mbpp
      supported_endpoint_types:
      - completions
      - chat
      params:
        task: mbpp
        max_new_tokens: 2048
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 10
- name: MBPP+
  description: MBPP+ is a modified version of MBPP containing 35x more test cases.
  defaults:
    config:
      type: mbppplus
      supported_endpoint_types:
      - completions
      - chat
      params:
        task: mbppplus
        max_new_tokens: 2048
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MBPP+NeMo
  description: MBPP+NeMo is a modified version of MBPP+ that uses the NeMo alignment
    prompt template.
  defaults:
    config:
      type: mbppplus_nemo
      supported_endpoint_types:
      - chat
      params:
        task: mbppplus_nemo
        max_new_tokens: 2048
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-py
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-py
      supported_endpoint_types:
      - completions
      params:
        task: multiple-py
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-sh
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-sh
      supported_endpoint_types:
      - completions
      params:
        task: multiple-sh
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-cpp
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-cpp
      supported_endpoint_types:
      - completions
      params:
        task: multiple-cpp
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-cs
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-cs
      supported_endpoint_types:
      - completions
      params:
        task: multiple-cs
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-d
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-d
      supported_endpoint_types:
      - completions
      params:
        task: multiple-d
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-go
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-go
      supported_endpoint_types:
      - completions
      params:
        task: multiple-go
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-java
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-java
      supported_endpoint_types:
      - completions
      params:
        task: multiple-java
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-js
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-js
      supported_endpoint_types:
      - completions
      params:
        task: multiple-js
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-jl
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-jl
      supported_endpoint_types:
      - completions
      params:
        task: multiple-jl
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-lua
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-lua
      supported_endpoint_types:
      - completions
      params:
        task: multiple-lua
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-pl
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-pl
      supported_endpoint_types:
      - completions
      params:
        task: multiple-pl
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-php
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-php
      supported_endpoint_types:
      - completions
      params:
        task: multiple-php
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-r
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-r
      supported_endpoint_types:
      - completions
      params:
        task: multiple-r
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-rkt
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-rkt
      supported_endpoint_types:
      - completions
      params:
        task: multiple-rkt
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-rb
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-rb
      supported_endpoint_types:
      - completions
      params:
        task: multiple-rb
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-rs
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-rs
      supported_endpoint_types:
      - completions
      params:
        task: multiple-rs
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-scala
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-scala
      supported_endpoint_types:
      - completions
      params:
        task: multiple-scala
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-swift
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-swift
      supported_endpoint_types:
      - completions
      params:
        task: multiple-swift
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
- name: MultiPL-E-ts
  description: MultiPL-E is a suite of coding tasks for many programming languages
  defaults:
    config:
      type: multiple-ts
      supported_endpoint_types:
      - completions
      params:
        task: multiple-ts
        max_new_tokens: 1024
        temperature: 0.1
        top_p: 0.95
        extra:
          do_sample: true
          n_samples: 5
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/bigcode-evaluation-harness:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:bfe529f539895b0a1b1e5b4d13f208c8345e291e59220f54e2434a8c3467b3b5
---
framework:
  name: livecodebench
  pkg_name: livecodebench
  full_name: LiveCodeBench
  description: Holistic and Contamination Free Evaluation of Large Language Models
    for Code. Paper https://arxiv.org/pdf/2403.07974
  url: https://github.com/LiveCodeBench/LiveCodeBench
  source: https://gitlab-master.nvidia.com/dl/JoC/competitive_evaluation/LiveCodeBench
defaults:
  command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
    \ && \n{% endif %} livecodebench --model {{target.api_endpoint.model_id}} \\\n\
    \            --scenario {{config.params.task}} \\\n            --release_version\
    \ {{config.params.extra.release_version}} \\\n            --url {{target.api_endpoint.url}}\
    \ \\\n            --temperature {{config.params.temperature}} \\\n           \
    \ --top_p {{config.params.top_p}} \\\n            --evaluate \\\n            --codegen_n\
    \ {{config.params.extra.n_samples}} \\\n            --use_cache \\\n         \
    \   --cache_batch_size {{config.params.extra.cache_batch_size}} \\\n         \
    \   --num_process_evaluate {{config.params.extra.num_process_evaluate}} \\\n \
    \           --n {{config.params.extra.n_samples}} \\\n            --max_tokens\
    \ {{config.params.max_new_tokens}} \\\n            --out_dir {{config.output_dir}}\
    \ \\\n            --multiprocess {{config.params.parallelism}} \\\n          \
    \  --max_retries {{config.params.max_retries}} \\\n            --timeout {{config.params.request_timeout}}{%\
    \ if config.params.extra.start_date is not none %} --start_date {{config.params.extra.start_date}}\
    \ {% endif %} {% if config.params.extra.end_date is not none %} --end_date {{config.params.extra.end_date}}\
    \ {% endif %} {% if config.params.extra.support_system_role %} --support_system_role\
    \ {% endif %} {% if config.params.limit_samples is not none %} --first_n {{config.params.limit_samples}}{%\
    \ endif %}{% if config.params.extra.cot_code_execution == true %} --cot_code_execution\
    \ {% endif %}{% if config.params.extra.args is defined %} {{ config.params.extra.args\
    \ }} {% endif %}\n"
  config:
    params:
      limit_samples: null
      max_new_tokens: 4096
      temperature: 0.0
      top_p: 1.0e-05
      parallelism: 10
      max_retries: 5
      request_timeout: 60
      extra:
        n_samples: 10
        num_process_evaluate: 5
        cache_batch_size: 10
        support_system_role: false
        start_date: null
        end_date: null
        cot_code_execution: false
  target:
    api_endpoint: {}
evaluations:
- name: codegeneration_release_latest
  description: Code generation latest version
  defaults:
    config:
      type: codegeneration_release_latest
      supported_endpoint_types:
      - chat
      params:
        task: codegeneration
        extra:
          release_version: release_latest
- name: codegeneration_release_v1
  description: The initial release of the dataset with problems released between May
    2023 and Mar 2024 containing 400 problems.
  defaults:
    config:
      type: codegeneration_release_v1
      supported_endpoint_types:
      - chat
      params:
        task: codegeneration
        extra:
          release_version: release_v1
- name: codegeneration_release_v2
  description: The updated release of the dataset with problems released between May
    2023 and May 2024 containing 511 problems.
  defaults:
    config:
      type: codegeneration_release_v2
      supported_endpoint_types:
      - chat
      params:
        task: codegeneration
        extra:
          release_version: release_v2
- name: codegeneration_release_v3
  description: The updated release of the dataset with problems released between May
    2023 and Jul 2024 containing 612 problems.
  defaults:
    config:
      type: codegeneration_release_v3
      supported_endpoint_types:
      - chat
      params:
        task: codegeneration
        extra:
          release_version: release_v3
- name: codegeneration_release_v4
  description: The updated release of the dataset with problems released between May
    2023 and Sep 2024 containing 713 problems.
  defaults:
    config:
      type: codegeneration_release_v4
      supported_endpoint_types:
      - chat
      params:
        task: codegeneration
        extra:
          release_version: release_v4
- name: codegeneration_release_v5
  description: The updated release of the dataset with problems released between May
    2023 and Jan 2025 containing 880 problems.
  defaults:
    config:
      type: codegeneration_release_v5
      supported_endpoint_types:
      - chat
      params:
        task: codegeneration
        extra:
          release_version: release_v5
- name: codegeneration_release_v6
  description: The updated release of the dataset with problems released between May
    2023 and Apr 2025 containing 1055 problems.
  defaults:
    config:
      type: codegeneration_release_v6
      supported_endpoint_types:
      - chat
      params:
        task: codegeneration
        extra:
          release_version: release_v6
- name: codegeneration_notfast
  description: Not fast version of code generation (v2).
  defaults:
    config:
      type: codegeneration_notfast
      supported_endpoint_types:
      - chat
      params:
        task: codegeneration
        extra:
          args: --not_fast
- name: testoutputprediction
  description: Solve the natural language task on a specified input, evaluating the
    ability to generate testing outputs. The model is given the natural language problem
    description and an input, and the output should be the output for the problem.
  defaults:
    config:
      type: testoutputprediction
      supported_endpoint_types:
      - chat
      params:
        task: testoutputprediction
        extra:
          release_version: release_latest
- name: codeexecution_v2
  description: "\u201CExecute\u201D a program on an input, evaluating code comprehension\
    \ ability. The model is given a program and an input, and the output should be\
    \ the result."
  defaults:
    config:
      type: codeexecution_v2
      supported_endpoint_types:
      - chat
      params:
        task: codeexecution
        extra:
          release_version: release_v2
- name: codeexecution_v2_cot
  description: "\u201CCoT. Execute\u201D a program on an input, evaluating code comprehension\
    \ ability. The model is given a program and an input, and the output should be\
    \ the result."
  defaults:
    config:
      type: codeexecution_v2_cot
      supported_endpoint_types:
      - chat
      params:
        task: codeexecution
        extra:
          release_version: release_v2
          cot_code_execution: true
- name: AA_code_generation
  description: AA code generation evaluating code comprehension ability. The model
    is given a program and an input, and the output should be the result.
  defaults:
    config:
      type: AA_codegeneration
      supported_endpoint_types:
      - chat
      params:
        task: codegeneration
        extra:
          release_version: release_v5
          n_samples: 3
          start_date: 2024-07-01
          end_date: 2025-01-01
- name: livecodebench_0724_0125
  description: '- Code generation evaluating code comprehension ability. The model
    is given a program and an input, and the output should be the result. - The data
    period and sampling parameters used by Artificial Analaysis (https://artificialanalysis.ai/methodology/intelligence-benchmarking)'
  defaults:
    config:
      type: livecodebench_0724_0125
      supported_endpoint_types:
      - chat
      params:
        task: codegeneration
        temperature: 0.0
        max_new_tokens: 4096
        extra:
          release_version: release_v5
          n_samples: 3
          start_date: 2024-07-01
          end_date: 2025-01-01
- name: livecodebench_0824_0225
  description:
  - Code generation evaluating code comprehension ability. The model is given a program
    and an input, and the output should be the result.
  - The data period and sampling parameters used by NeMo Alignment team.
  defaults:
    config:
      type: livecodebench_0824_0225
      supported_endpoint_types:
      - chat
      params:
        task: codegeneration
        temperature: 0.0
        max_new_tokens: 4096
        extra:
          release_version: release_v5
          n_samples: 3
          start_date: 2024-08-01
          end_date: 2025-02-01
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/livecodebench:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:98a3aa0b4b09052e54f9ba4c689b75aab501230e4b3c9700ae244a457e960817
---
framework:
  name: scicode
  pkg_name: scicode
  full_name: SciCode Benchmark
  description: SciCode is a challenging benchmark designed to evaluate the capabilities
    of LLMs in generating code for solving realistic scientific research problems.
  url: https://github.com/scicode-bench/SciCode
  source: https://gitlab-master.nvidia.com/dl/JoC/competitive_evaluation/core_evals_frameworks/SciCode
defaults:
  command: '{% if target.api_endpoint.api_key is not none %}API_KEY=${{target.api_endpoint.api_key}}{%
    endif %} scicode_eval --model {{target.api_endpoint.model_id}} --url {{target.api_endpoint.url}}
    --output-dir {{config.output_dir}} --log-dir {{config.output_dir}}/logs {% if
    config.params.temperature is not none %}--temperature={{config.params.temperature}}{%
    endif %} {% if config.params.limit_samples is not none %}--limit-samples={{config.params.limit_samples}}{%
    endif %} --n-samples={{config.params.extra.n_samples}} --extra-params top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},max_tokens={{config.params.max_new_tokens}},max_retries={{config.params.max_retries}}
    {% if config.params.extra.with_background %}--with-background {% endif %} {% if
    config.params.extra.include_dev %}--include-dev{% endif %} {% if config.params.extra.eval_threads
    is not none %}--eval-threads={{config.params.extra.eval_threads}}{% endif %}'
  config:
    supported_endpoint_types:
    - chat
    params:
      limit_samples: null
      temperature: 0
      max_new_tokens: 2048
      top_p: 1.0e-05
      request_timeout: 60
      max_retries: 2
      extra:
        with_background: false
        include_dev: false
        n_samples: 1
        eval_threads: null
  target:
    api_endpoint:
      stream: false
evaluations:
- name: SciCode
  description: '- SciCode is a challenging benchmark designed to evaluate the capabilities
    of LLMs in generating code for solving realistic scientific research problems.
    - This variant does not include scientist-annotated background in the prompts.'
  defaults:
    config:
      type: scicode
- name: SciCode-Background
  description: '- SciCode is a challenging benchmark designed to evaluate the capabilities
    of LLMs in generating code for solving realistic scientific research problems.
    - This variant includes scientist-annotated background in the prompts.'
  defaults:
    config:
      type: scicode_background
      params:
        extra:
          with_background: true
- name: AA-SciCode
  description: '- SciCode is a challenging benchmark designed to evaluate the capabilities
    of LLMs in generating code for solving realistic scientific research problems.
    - This variant mimicks setup used by Artificial Analysis in their Intelligence
    Benchmark (v2). - It includes scientist-annotated background in the prompts and
    uses all available problems for evaluation (including "dev" set).'
  defaults:
    config:
      type: aa_scicode
      params:
        temperature: 0.0
        max_new_tokens: 4096
        extra:
          with_background: true
          include_dev: true
          n_samples: 3
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/scicode:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:1281e1da419d4841b6c8d6373116b7307974e0ab807d38118909b1314f292c4c
---
framework:
  name: hle
  pkg_name: hle
  full_name: Humanity's Last Exam
  description: Humanity's Last Exam (HLE) is a multi-modal benchmark at the frontier
    of human knowledge, designed to be the final closed-ended academic benchmark of
    its kind with broad subject coverage. Humanity's Last Exam consists of 3,000 questions
    across dozens of subjects, including mathematics, humanities, and the natural
    sciences. HLE is developed globally by subject-matter experts and consists of
    multiple-choice and short-answer questions suitable for automated grading.
  url: https://github.com/centerforaisafety/hle
  source: https://gitlab-master.nvidia.com/dl/JoC/competitive_evaluation/core_evals_frameworks/hle
defaults:
  command: hle_eval --dataset=cais/hle --model_name={{target.api_endpoint.model_id}}
    --model_url={{target.api_endpoint.url}}  --temperature={{config.params.temperature}}
    --top_p={{config.params.top_p}} --timeout={{config.params.request_timeout}}  {%
    if config.params.limit_samples is not none %}--limit {{config.params.limit_samples}}{%
    endif %} --output_dir={{config.output_dir}}  {% if target.api_endpoint.api_key
    is not none %}--api_key_name={{target.api_endpoint.api_key}}{% endif %} --max_retries={{config.params.max_retries}}
    --num_workers={{config.params.parallelism}}  --max_new_tokens={{config.params.max_new_tokens}}
    --text_only --generate --judge
  config:
    params:
      limit_samples: null
      temperature: 0.0
      top_p: 1.0
      request_timeout: 600.0
      max_new_tokens: 4096
      max_retries: 30
      parallelism: 100
  target:
    api_endpoint: {}
evaluations:
- name: hle
  description: hle
  defaults:
    config:
      type: hle
      supported_endpoint_types:
      - chat
      params:
        task: hle
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/hle:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:b1d4456951b751baeb7b8354e8d58f32be4e0f88f5dbeaabac34cb5c1832c78c
---
framework:
  name: bfcl
  pkg_name: bfcl
  full_name: Berkeley Function Calling Leaderboard
  description: The Berkeley Function Calling Leaderboard V3 (also called Berkeley
    Tool Calling Leaderboard V3) evaluates the LLM's ability to call functions (aka
    tools) accurately.
  url: https://gorilla.cs.berkeley.edu/leaderboard.html
  source: https://github.com/ShishirPatil/gorilla/tree/main/berkeley-function-call-leaderboard
defaults:
  command: "{%- if config.params.extra.custom_dataset.path is not none and config.params.extra.custom_dataset.format\
    \ is not none -%} echo \"Processing custom dataset...\" && export BFCL_DATA_DIR=$(core-evals-process-custom-dataset\
    \ \\\n  --dataset_format {{config.params.extra.custom_dataset.format}} \\\n  --dataset_path\
    \ {{config.params.extra.custom_dataset.path}} \\\n  --test_category {{config.params.task}}\
    \ \\\n  --processing_output_dir {{config.output_dir ~ \"/custom_dataset_processing\"\
    }} \\\n  {% if config.params.extra.custom_dataset.data_template_path %}--data_template_path\
    \ {{config.params.extra.custom_dataset.data_template_path}}{% endif %}) && \\\n\
    echo \"Using custom dataset at ${BFCL_DATA_DIR}\" && \\\n{% endif -%}\n{% if target.api_endpoint.api_key\
    \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{% endif %} bfcl\
    \ generate --model {{target.api_endpoint.model_id}} --test-category {{config.params.task}}\
    \ --model-mapping oai --result-dir {{config.output_dir}} --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\
    \  {% if config.params.limit_samples is not none %} --limit {{config.params.limit_samples}}{%\
    \ endif %} --num-threads  {{config.params.parallelism}} && \\\n{% if target.api_endpoint.api_key\
    \ is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{% endif %} bfcl\
    \ evaluate --model {{target.api_endpoint.model_id}} --test-category {{config.params.task}}\
    \ --model-mapping oai --result-dir {{config.output_dir}} --score-dir {{config.output_dir}}\
    \ --model-args base_url={{target.api_endpoint.url}},native_calling={{config.params.extra.native_calling}}\n"
  config:
    params:
      limit_samples: null
      parallelism: 10
      extra:
        native_calling: false
        custom_dataset:
          path: null
          format: null
          data_template_path: null
  target:
    api_endpoint: {}
evaluations:
- name: bfclv3
  description: Single-turn and Multi-turn, Live and Non-Live, AST and Exec evaluation
  defaults:
    config:
      type: bfclv3
      supported_endpoint_types:
      - chat
      - vlm
      params:
        task: all
- name: bfclv3_ast
  description: Single-turn and Multi-turn, Live and Non-Live, AST evaluation. Uses
    native function calling.
  defaults:
    config:
      type: bfclv3_ast
      supported_endpoint_types:
      - chat
      - vlm
      params:
        task: multi_turn,ast
        extra:
          native_calling: true
- name: bfclv3_ast_prompting
  description: Single-turn and Multi-turn, Live and Non-Live, AST evaluation. Not
    using native function calling.
  defaults:
    config:
      type: bfclv3_ast_prompting
      supported_endpoint_types:
      - chat
      - vlm
      params:
        task: multi_turn,ast
        extra:
          native_calling: false
- name: bfclv2
  description: Single-turn, Live and Non-Live, AST and Exec evaluation
  defaults:
    config:
      type: bfclv2
      supported_endpoint_types:
      - chat
      - vlm
      params:
        task: single_turn
- name: bfclv2_ast
  description: Single-turn, Live and Non-Live,  AST evaluation only. Uses native function
    calling.
  defaults:
    config:
      type: bfclv2_ast
      supported_endpoint_types:
      - chat
      - vlm
      params:
        task: ast
        extra:
          native_calling: true
- name: bfclv2_ast_prompting
  description: Single-turn, Live and Non-Live,  AST evaluation only. Not using native
    function calling.
  defaults:
    config:
      type: bfclv2_ast_prompting
      supported_endpoint_types:
      - chat
      - vlm
      params:
        task: ast
        extra:
          native_calling: false
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/bfcl:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:afd09c97a14d7a97e63cb3e765b8738a8c5d53ecc1771c953b6283f75f066dfa
---
framework:
  name: profbench
  pkg_name: profbench
  full_name: ProfBench
  description: Professional domain benchmark for evaluating LLMs on Physics PhD, Chemistry
    PhD, Finance MBA, and Consulting MBA tasks
  url: https://github.com/nvidia/profbench
  source: https://gitlab-master.nvidia.com/dl/JoC/competitive_evaluation/core_evals_frameworks/profbench
defaults:
  command: "{% if target.api_endpoint.api_key is not none %}\n  export API_KEY=${{target.api_endpoint.api_key}}\
    \ && \n{% endif %} {% if config.params.extra.run_generation %}\n  python -m profbench.run_report_generation\
    \ \\\n    --model {{target.api_endpoint.model_id}} \\\n    --library {{config.params.extra.library}}\
    \ \\\n    --timeout {{config.params.request_timeout}} \\\n    --parallel {{config.params.parallelism}}\
    \ \\\n    --retry-attempts {{config.params.max_retries}} \\\n    --folder {{config.output_dir}}{%\
    \ if target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
    \ endif %}{% if config.params.extra.version is not none %} --version {{config.params.extra.version}}{%\
    \ endif %}{% if config.params.extra.upload_documents %} --upload-documents{% endif\
    \ %}{% if config.params.extra.web_search %} --web-search{% endif %}{% if config.params.extra.reasoning\
    \ %} --reasoning{% endif %}{% if config.params.extra.reasoning_effort is not none\
    \ %} --reasoning-effort {{config.params.extra.reasoning_effort}}{% endif %}{%\
    \ if config.params.limit_samples is not none %} --limit-samples {{config.params.limit_samples}}{%\
    \ endif %}{% if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
    \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
    \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
    \ endif %} && \n  GENERATION_OUTPUT=$(ls -t {{config.output_dir}}/*.jsonl | head\
    \ -1) && \n{% endif %} {% if config.params.extra.run_judge_generated %}\n  python\
    \ -m profbench.run_best_llm_judge_on_generated_reports \\\n    --filename $GENERATION_OUTPUT\
    \ \\\n    --api-key $API_KEY \\\n    --model {{target.api_endpoint.model_id}}\
    \ \\\n    --library {{config.params.extra.library}} \\\n    --timeout {{config.params.request_timeout}}\
    \ \\\n    --parallel {{config.params.parallelism}} \\\n    --retry-attempts {{config.params.max_retries}}\
    \ \\\n    --output-folder {{config.output_dir}}/judgements{% if target.api_endpoint.url\
    \ is not none %} --base-url {{target.api_endpoint.url}}{% endif %}{% if config.params.limit_samples\
    \ is not none %} --limit-samples {{config.params.limit_samples}}{% endif %}{%\
    \ if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
    \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
    \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
    \ endif %} && \n  JUDGE_OUTPUT=$(ls -t {{config.output_dir}}/judgements/*.jsonl\
    \ | head -1) && \n  python -m profbench.score_report_generation $JUDGE_OUTPUT\n\
    {% endif %} {% if config.params.extra.run_judge_provided %}\n  python -m profbench.run_llm_judge_on_provided_reports\
    \ \\\n    --model {{target.api_endpoint.model_id}} \\\n    --library {{config.params.extra.library}}\
    \ \\\n    --timeout {{config.params.request_timeout}} \\\n    --parallel {{config.params.parallelism}}\
    \ \\\n    --retry-attempts {{config.params.max_retries}} \\\n    --folder {{config.output_dir}}{%\
    \ if target.api_endpoint.url is not none %} --base-url {{target.api_endpoint.url}}{%\
    \ endif %}{% if config.params.extra.reasoning %} --reasoning{% endif %}{% if config.params.extra.reasoning_effort\
    \ is not none %} --reasoning-effort {{config.params.extra.reasoning_effort}}{%\
    \ endif %}{% if config.params.extra.debug %} --debug{% endif %}{% if config.params.limit_samples\
    \ is not none %} --limit-samples {{config.params.limit_samples}}{% endif %}{%\
    \ if config.params.temperature is not none %} --temperature {{config.params.temperature}}{%\
    \ endif %}{% if config.params.top_p is not none %} --top-p {{config.params.top_p}}{%\
    \ endif %}{% if config.params.max_new_tokens is not none %} --max-tokens {{config.params.max_new_tokens}}{%\
    \ endif %} && \n  JUDGE_OUTPUT=$(ls -t {{config.output_dir}}/*.jsonl | head -1)\
    \ && \n  python -m profbench.score_llm_judge $JUDGE_OUTPUT\n{% endif %}\n"
  config:
    params:
      limit_samples: null
      max_new_tokens: 4096
      temperature: 0.0
      top_p: 1.0e-05
      parallelism: 10
      max_retries: 5
      request_timeout: 600
      extra:
        run_generation: false
        run_judge_generated: false
        run_judge_provided: false
        library: openai
        version: lite
        upload_documents: false
        web_search: false
        reasoning: false
        reasoning_effort: null
        debug: false
  target:
    api_endpoint: {}
evaluations:
- name: report_generation
  description: Generate professional reports and evaluate them (full pipeline)
  defaults:
    config:
      type: report_generation
      supported_endpoint_types:
      - chat
      params:
        extra:
          run_generation: true
          run_judge_generated: true
          run_judge_provided: false
- name: llm_judge
  description: Run LLM judge on provided ProfBench reports and score them
  defaults:
    config:
      type: llm_judge
      supported_endpoint_types:
      - chat
      params:
        extra:
          run_generation: false
          run_judge_generated: false
          run_judge_provided: true
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/profbench:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:251d1b47c52fbfed94837ef6ab5e0cca61ac218bb82d01b0e4314fb10264a9ef
---
framework:
  name: vlmevalkit
  pkg_name: vlmevalkit
  full_name: VLMEvalKit
  description: VLMEvalKit is an open-source evaluation toolkit of large vision-language
    models (LVLMs). It enables one-command evaluation of LVLMs on various benchmarks,
    without the heavy workload of data preparation under multiple repositories. In
    VLMEvalKit, we adopt generation-based evaluation for all LVLMs, and provide the
    evaluation results obtained with both exact matching and LLM-based answer extraction.
  url: https://github.com/open-compass/VLMEvalKit
  source: https://gitlab-master.nvidia.com/dl/JoC/competitive_evaluation/core_evals_frameworks/VLMEvalKit
defaults:
  command: "cat > {{config.output_dir}}/vlmeval_config.json << 'EOF'\n{\n  \"model\"\
    : {\n    \"{{target.api_endpoint.model_id.split('/')[-1]}}\": {\n      \"class\"\
    : \"CustomOAIEndpoint\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
    ,\n      \"api_base\": \"{{target.api_endpoint.url}}\",\n      \"api_key_var_name\"\
    : \"{{target.api_endpoint.api_key}}\",\n      \"max_tokens\": {{config.params.max_new_tokens}},\n\
    \      \"temperature\": {{config.params.temperature}},{% if config.params.top_p\
    \ is not none %}\n      \"top_p\": {{config.params.top_p}},{% endif %}\n     \
    \ \"retry\": {{config.params.max_retries}},\n      \"timeout\": {{config.params.request_timeout}}{%\
    \ if config.params.extra.wait is defined %},\n      \"wait\": {{config.params.extra.wait}}{%\
    \ endif %}{% if config.params.extra.img_size is defined %},\n      \"img_size\"\
    : {{config.params.extra.img_size}}{% endif %}{% if config.params.extra.img_detail\
    \ is defined %},\n      \"img_detail\": \"{{config.params.extra.img_detail}}\"\
    {% endif %}{% if config.params.extra.system_prompt is defined %},\n      \"system_prompt\"\
    : \"{{config.params.extra.system_prompt}}\"{% endif %}{% if config.params.extra.verbose\
    \ is defined %},\n      \"verbose\": {{config.params.extra.verbose}}{% endif %}\n\
    \    }\n  },\n  \"data\": {\n    \"{{config.params.extra.dataset.name}}\": {\n\
    \      \"class\": \"{{config.params.extra.dataset.class}}\",\n      \"dataset\"\
    : \"{{config.params.extra.dataset.name}}\",\n      \"model\": \"{{target.api_endpoint.model_id}}\"\
    \n    }\n  }\n}\nEOF\npython -m vlmeval.run \\\n  --config {{config.output_dir}}/vlmeval_config.json\
    \ \\\n  --work-dir {{config.output_dir}} \\\n  --api-nproc {{config.params.parallelism}}\
    \ \\\n  {%- if config.params.extra.judge is defined %}\n  --judge {{config.params.extra.judge.model}}\
    \ \\\n  --judge-args '{{config.params.extra.judge.args}}' \\\n  {%- endif %}\n\
    \  {% if config.params.limit_samples is not none %}--first-n {{config.params.limit_samples}}{%\
    \ endif %}\n"
  config:
    supported_endpoint_types:
    - vlm
    params:
      limit_samples: null
      max_new_tokens: 2048
      temperature: 0
      top_p: null
      parallelism: 4
      max_retries: 5
      request_timeout: 60
  target:
    api_endpoint: {}
evaluations:
- name: AI2D
  description: A benchmark for evaluating diagram understanding capabilities of large
    vision-language models.
  defaults:
    config:
      type: ai2d_judge
      params:
        extra:
          dataset:
            name: AI2D_TEST
            class: ImageMCQDataset
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
- name: ChartQA
  description: A Benchmark for Question Answering about Charts with Visual and Logical
    Reasoning
  defaults:
    config:
      type: chartqa
      params:
        extra:
          dataset:
            name: ChartQA_TEST
            class: ImageVQADataset
- name: MathVista-MINI
  description: Evaluating Math Reasoning in Visual Contexts
  defaults:
    config:
      type: mathvista-mini
      params:
        extra:
          dataset:
            name: MathVista_MINI
            class: MathVista
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
- name: MMMU
  description: A benchmark for evaluating multimodal models on massive multi-discipline
    tasks demanding college-level subject knowledge and deliberate reasoning.
  defaults:
    config:
      type: mmmu_judge
      params:
        extra:
          dataset:
            name: MMMU_DEV_VAL
            class: MMMUDataset
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
- name: OCRBench
  description: Comprehensive evaluation benchmark designed to assess the OCR capabilities
    of Large Multimodal Models
  defaults:
    config:
      type: ocrbench
      params:
        extra:
          dataset:
            name: OCRBench
            class: OCRBench
- name: OCR-Reasoning
  description: Comprehensive benchmark of 1,069 human-annotated examples designed
    to evaluate multimodal large language models on text-rich image reasoning tasks
    by assessing both final answers and the reasoning process across six core abilities
    and 18 practical tasks.
  defaults:
    config:
      type: ocr_reasoning
      params:
        extra:
          dataset:
            name: OCR_Reasoning
            class: OCR_Reasoning
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
- name: SlideVQA
  description: Evaluates ability to answer questions about slide decks by selecting
    relevant slides from multiple images
  defaults:
    config:
      type: slidevqa
      params:
        extra:
          dataset:
            name: SLIDEVQA
            class: SlideVQA
          judge:
            model: gpt-4o
            args: '{"use_azure": true}'
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-vlm/vlmevalkit:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:54f9dcd86d62d9897e035a424a7b304dc636052f267afccb019af15eae93a4e0
---
framework:
  name: garak
  pkg_name: garak
  full_name: Garak
  description: Garak is an LLM vulnerability scanner.
  url: https://github.com/NVIDIA/garak/tree/main
  source: https://gitlab-master.nvidia.com/dl/JoC/competitive_evaluation/core_evals_frameworks/garak
defaults:
  command: "cat > garak_config.yaml << 'EOF'\n{% if config.params.extra.seed is not\
    \ none %}run:\n  seed: {{config.params.extra.seed}}{% endif %}\nplugins:\n  {%\
    \ if config.params.extra.probes is not none %}probe_spec: {{config.params.extra.probes}}{%\
    \ endif %}\n  extended_detectors: true\n  model_type: {% if target.api_endpoint.type\
    \ == \"completions\" %}nim.NVOpenAICompletion{% elif target.api_endpoint.type\
    \ == \"chat\" %}nim.NVOpenAIChat{% endif %}\n  model_name: {{target.api_endpoint.model_id}}\n\
    \  generators:\n    nim:\n      uri: {{target.api_endpoint.url | replace('/chat/completions',\
    \ '') | replace('/completions', '')}}\n      {% if config.params.temperature is\
    \ not none %}temperature: {{config.params.temperature}}{% endif %}\n      {% if\
    \ config.params.top_p is not none %}top_p: {{config.params.top_p}}{% endif %}\n\
    \      {% if config.params.max_new_tokens is not none %}max_tokens: {{config.params.max_new_tokens}}{%\
    \ endif %}\nsystem:\n  parallel_attempts: {{config.params.parallelism}}\n  lite:\
    \ false\nEOF\n{% if target.api_endpoint.api_key is not none %}\nexport NIM_API_KEY=${{target.api_endpoint.api_key}}\
    \ &&\n{% else %}\nexport NIM_API_KEY=dummy &&\n{% endif %}\nexport XDG_DATA_HOME={{config.output_dir}}\
    \ &&\ngarak --config garak_config.yaml --report_prefix=results\n"
  config:
    supported_endpoint_types:
    - chat
    - completions
    params:
      max_new_tokens: 150
      temperature: 0.1
      top_p: 0.7
      parallelism: 32
      extra:
        probes: null
        seed: null
  target:
    api_endpoint: {}
evaluations:
- name: garak
  description: garak
  defaults:
    config:
      type: garak
      params:
        task: garak
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/garak:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:bf0eae3c50328f85a67f921bdf7f935aa3e8d045eaaa2c7c190c851b1d80252e
---
framework:
  name: safety_eval
  pkg_name: safety_eval
  full_name: Safety Harness
  description: Home for Safety evaluations
  url: null
  source: null
defaults:
  command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}  &&
    {% endif %} {% if config.params.extra.judge.api_key is not none %}export JUDGE_API_KEY=${{config.params.extra.judge.api_key}}
    && {% endif %} safety-eval  --model-name  {{target.api_endpoint.model_id}} --model-url
    {{target.api_endpoint.url}} --model-type {{target.api_endpoint.type}}  --judge-url  {{config.params.extra.judge.url}}   --results-dir
    {{config.output_dir}}   --eval {{config.params.task}}  --mut-inference-params
    max_tokens={{config.params.max_new_tokens}},temperature={{config.params.temperature}},top_p={{config.params.top_p}},timeout={{config.params.request_timeout}},concurrency={{config.params.parallelism}},retries={{config.params.max_retries}}
    --judge-inference-params concurrency={{config.params.extra.judge.parallelism}},retries={{config.params.max_retries}}  {%
    if config.params.extra.dataset is defined and config.params.extra.dataset %} --dataset
    {{config.params.extra.dataset}}{% endif %} {% if config.params.limit_samples is
    not none %} --limit {{config.params.limit_samples}} {% endif %} {% if config.params.extra.judge.model_id
    is not none %} --judge-model-name {{config.params.extra.judge.model_id}} {% endif
    %} {% if config.type == "aegis_v2_reasoning" %} {% if config.params.extra.evaluate_reasoning_traces  %}
    --evaluate-reasoning-traces {% endif %} {% endif %}'
  config:
    supported_endpoint_types:
    - chat
    - completions
    params:
      limit_samples: null
      max_new_tokens: 6144
      temperature: 0.6
      top_p: 0.95
      parallelism: 8
      max_retries: 5
      request_timeout: 30
      extra:
        judge:
          url: null
          model_id: null
          api_key: null
          parallelism: 32
          request_timeout: 60
          max_retries: 16
  target:
    api_endpoint:
      stream: false
evaluations:
- name: aegis_v2
  description: Aegis V2 without evaluating reasoning traces. This version is used
    by the NeMo Safety Toolkit.
  defaults:
    config:
      type: aegis_v2
      params:
        task: aegis_v2
        extra:
          evaluate_reasoning_traces: false
- name: aegis_v2_reasoning
  description: Aegis V2 with evaluating reasoning traces.
  defaults:
    config:
      type: aegis_v2_reasoning
      params:
        task: aegis_v2
        extra:
          evaluate_reasoning_traces: true
- name: wildguard
  description: Wildguard
  defaults:
    config:
      type: wildguard
      params:
        task: wildguard
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/safety-harness:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:c53b77274383e1b784a704b31db04e40618e55d40b57c5d2a3faf6bbdfacf509
---
framework:
  name: helm
  pkg_name: helm
  full_name: Holistic Evaluation of Language Models for Medical Applications
  description: A framework for evaluating large language models in medical applications
    across various healthcare tasks
  url: https://github.com/stanford-crfm/helm
  source: https://gitlab-master.nvidia.com/dl/JoC/competitive_evaluation/core_evals_frameworks/helm
defaults:
  command: '{% if target.api_endpoint.api_key is not none %}export OPENAI_API_KEY=${{target.api_endpoint.api_key}}
    && {% endif %} {% if config.params.extra.gpt_judge_api_key is not none %}export
    GPT_JUDGE_API_KEY=${{config.params.extra.gpt_judge_api_key}} && {% endif %} {%
    if config.params.extra.llama_judge_api_key is not none %}export LLAMA_JUDGE_API_KEY=${{config.params.extra.llama_judge_api_key}}
    && {% endif %} {% if config.params.extra.claude_judge_api_key is not none %}export
    CLAUDE_JUDGE_API_KEY=${{config.params.extra.claude_judge_api_key}} && {% endif
    %} helm-generate-dynamic-model-configs  --model-name {{target.api_endpoint.model_id}}  --base-url
    {{target.api_endpoint.url}}  --openai-model-name {{target.api_endpoint.model_id}}  --output-dir
    {{config.output_dir}} && helm-run  --run-entries {{config.params.task}}:{% if
    config.params.extra.subset is not none %}subset={{config.params.extra.subset}},{%
    endif %}model={{target.api_endpoint.model_id}}  {% if config.params.limit_samples
    is not none %} --max-eval-instances {{config.params.limit_samples}}  {% endif
    %} {% if config.params.parallelism is not none %} -n {{config.params.parallelism}}  {%
    endif %} --suite {{config.params.task}} {% if config.params.extra.num_train_trials
    is not none %}  --num-train-trials {{config.params.extra.num_train_trials}} {%
    endif %} {% if config.params.extra.data_path is not none %}  --data-path {{config.params.extra.data_path}}
    {% endif %} {% if config.params.extra.num_output_tokens is not none %}  --num-output-tokens
    {{config.params.extra.num_output_tokens}} {% endif %} {% if config.params.extra.subject
    is not none %}  --subject {{config.params.extra.subject}} {% endif %} {% if config.params.extra.condition
    is not none %}  --condition {{config.params.extra.condition}} {% endif %} {% if
    config.params.extra.max_length is not none %}  --max-length {{config.params.extra.max_length}}
    {% endif %}  -o {{config.output_dir}}  --local-path {{config.output_dir}}'
  config:
    params:
      limit_samples: null
      parallelism: 1
      extra:
        data_path: null
        num_output_tokens: null
        subject: null
        condition: null
        max_length: null
        num_train_trials: null
        subset: null
        gpt_judge_api_key: GPT_JUDGE_API_KEY
        llama_judge_api_key: LLAMA_JUDGE_API_KEY
        claude_judge_api_key: CLAUDE_JUDGE_API_KEY
  target:
    api_endpoint: {}
evaluations:
- name: medcalc_bench
  description: A dataset which consists of a patient note, a question requesting to
    compute a specific medical value, and a ground truth answer (Khandekar et al.,
    2024).
  defaults:
    config:
      type: medcalc_bench
      supported_endpoint_types:
      - chat
      params:
        task: medcalc_bench
- name: medec
  description: A dataset containing medical narratives with error detection and correction
    pairs (Abacha et al., 2025).
  defaults:
    config:
      type: medec
      supported_endpoint_types:
      - chat
      params:
        task: medec
- name: head_qa
  description: A collection of biomedical multiple-choice questions for testing medical
    knowledge (Vilares et al., 2019).
  defaults:
    config:
      type: head_qa
      supported_endpoint_types:
      - chat
      params:
        task: head_qa
- name: medbullets
  description: A USMLE-style medical question dataset with multiple-choice answers
    and explanations (MedBullets, 2025).
  defaults:
    config:
      type: medbullets
      supported_endpoint_types:
      - chat
      params:
        task: medbullets
- name: pubmed_qa
  description: A dataset that provides pubmed abstracts and asks associated questions
    yes/no/maybe questions.
  defaults:
    config:
      type: pubmed_qa
      supported_endpoint_types:
      - chat
      params:
        task: pubmed_qa
- name: ehr_sql
  description: Given a natural language instruction, generate an SQL query that would
    be used in clinical research.
  defaults:
    config:
      type: ehr_sql
      supported_endpoint_types:
      - chat
      params:
        task: ehr_sql
- name: race_based_med
  description: A collection of LLM outputs in response to medical questions with race-based
    biases, with the objective being to classify whether the output contains racially
    biased content.
  defaults:
    config:
      type: race_based_med
      supported_endpoint_types:
      - chat
      params:
        task: race_based_med
- name: medhallu
  description: A dataset of PubMed articles and associated questions, with the objective
    being to classify whether the answer is factual or hallucinated.
  defaults:
    config:
      type: medhallu
      supported_endpoint_types:
      - chat
      params:
        task: medhallu
- name: mtsamples_replicate
  description: Generate treatment plans based on clinical notes
  defaults:
    config:
      type: mtsamples_replicate
      supported_endpoint_types:
      - chat
      params:
        task: mtsamples_replicate
- name: aci_bench
  description: Extract and structure information from patient-doctor conversations
  defaults:
    config:
      type: aci_bench
      supported_endpoint_types:
      - chat
      params:
        task: aci_bench
- name: mtsamples_procedures
  description: Document and extract information about medical procedures
  defaults:
    config:
      type: mtsamples_procedures
      supported_endpoint_types:
      - chat
      params:
        task: mtsamples_procedures
- name: medication_qa
  description: Answer consumer medication-related questions
  defaults:
    config:
      type: medication_qa
      supported_endpoint_types:
      - chat
      params:
        task: medication_qa
- name: med_dialog_healthcaremagic
  description: Generate summaries of doctor-patient conversations, healthcaremagic
    version
  defaults:
    config:
      type: med_dialog_healthcaremagic
      supported_endpoint_types:
      - chat
      params:
        task: med_dialog
        extra:
          subset: healthcaremagic
- name: med_dialog_icliniq
  description: Generate summaries of doctor-patient conversations, icliniq version
  defaults:
    config:
      type: med_dialog_icliniq
      supported_endpoint_types:
      - chat
      params:
        task: med_dialog
        extra:
          subset: icliniq
- name: medi_qa
  description: Retrieve and rank answers based on medical question understanding
  defaults:
    config:
      type: medi_qa
      supported_endpoint_types:
      - chat
      params:
        task: medi_qa
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/helm:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:6d7b6151100405d700c97d55b9233d98f18a013de357338321ca1a0b14999496
---
framework:
  name: tooltalk
  pkg_name: tooltalk
  full_name: ToolTalk
  description: ToolTalk is designed to evaluate tool-augmented LLMs as a chatbot.
    ToolTalk contains a handcrafted dataset of 28 easy conversations and 50 hard conversations.
  url: https://github.com/microsoft/ToolTalk
  source: https://gitlab-master.nvidia.com/dl/JoC/competitive_evaluation/ToolTalk
defaults:
  command: '{% if target.api_endpoint.api_key is not none %}API_KEY=${{target.api_endpoint.api_key}}{%
    endif %} python -m tooltalk.evaluation.evaluate_{{''openai'' if ''azure'' in target.api_endpoint.url
    or ''api.openai'' in target.api_endpoint.url else ''nim''}} --dataset data/easy
    --database data/databases --model {{target.api_endpoint.model_id}} {% if config.params.max_new_tokens
    is not none %}--max_new_tokens {{config.params.max_new_tokens}}{% endif %} {%
    if config.params.temperature is not none %}--temperature {{config.params.temperature}}{%
    endif %} {% if config.params.top_p is not none %}--top_p {{config.params.top_p}}{%
    endif %} --api_mode all --output_dir {{config.output_dir}} --url {{target.api_endpoint.url}}
    {% if config.params.limit_samples is not none %}--first_n {{config.params.limit_samples}}{%
    endif %}'
  config:
    params:
      limit_samples: null
  target:
    api_endpoint: {}
evaluations:
- name: tooltalk
  description: tooltalk
  defaults:
    config:
      type: tooltalk
      supported_endpoint_types:
      - chat
      params:
        task: tooltalk
__container: gitlab-master.nvidia.com:5005/dl/joc/competitive_evaluation/nvidia-core-evals/ci-llm/tooltalk:dev-2025-11-10T13-29-9db0f7ca
__container_digest: sha256:d87254b0784060facdbe107f9a8b9768fe5f857e0e6575b6f536b6e5ccf5e48a

